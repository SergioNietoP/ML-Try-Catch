{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergioNietoP/machine-learning-datasheets/blob/main/Turf_Prediction_prueba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2qbeMFefL4Z"
      },
      "source": [
        "# **DataSheet Description**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "En este proyecto emplearemos un dataset que contiene información de los resultados de las carreras de el panorama nacional de carreras de caballos.\n",
        "\n",
        "- **Puesto** (Clase): Posición en la que terminan la carrera **int64**\n",
        "- **NombreCaballo**: Nombre de caballo  **object**\n",
        "- **Peso** : Peso de caballo  **object**\n",
        "- **Edad**: Edad de caballo **int64**\n",
        "- **DistanciaMeta**: Distancia respecto a ganador en llegada a meta **object** (DROPEAR)\n",
        "- **Mantilla**: Dorsal   **int64**\n",
        "- **Dividendo** : Pago por euro apostado por ganador de carrera **object** (DROPEAR)\n",
        "- **Propietario** : Cuadra Propietaria **object**\n",
        "- **Preparador**: Nombre Preparador **object**\n",
        "- **Jinete**: Jinete o Amazona que monta al caballo en esta carrera  **object**\n",
        "- **Problemas**: Problemas que posee en esta carrera el caballo  **object**\n",
        "- **UltimasActuaciones** : Posiciones en las que acaba el caballo de mas antigua a mas reciente (de izquierda a derecha)  **object** (¿COMO LA CALCULO?)\n",
        "- **Fecha**: Fecha Carrera   **object**\n",
        "- **Hora**: Hora carrera  **object**\n",
        "- **Terreno**: Estado del terreno en carrera **object**\n",
        "- **Distancia**: Longitud carrera  **float64**\n",
        "- **Tipo**: Tipo de terreno en carrera  **object**\n",
        "- **Categoría**: Categoria Caballos Participantes  **object**\n",
        "- **SentidoHipodromo**:  (0 -> Sentido Normal, 1 -> Inverso) **int64**\n",
        "- **Meteorología**: Previsión estado meteorología del día entero **object** (Tiempo3)\n",
        "- **Lluvia**: Previsión cantidad de lluvia del dia entero  **object**\n",
        "- **Viento**: Previsión velocidad de viento del dia entero  **int64**\n",
        "- **Temperatura**: Previsión de temp. del horario de la carrera  **int64**\n",
        "- **Hipodromo**: Nombre del hipódromo de la carrera   **object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh89cWwdfM-Q",
        "outputId": "ad93c891-3fb3-43d7-c3be-906ffa54fcd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Puesto                782\n",
              "NombreCaballo         782\n",
              "Peso                  782\n",
              "Edad                  782\n",
              "Mantilla              782\n",
              "Propietario           782\n",
              "Preparador            782\n",
              "Jinete                782\n",
              "Problemas             425\n",
              "UltimasActuaciones    782\n",
              "Fecha                 782\n",
              "Hora                  782\n",
              "Terreno               782\n",
              "Distancia             782\n",
              "Tipo                  781\n",
              "Categoría             782\n",
              "SentidoHipodromo      782\n",
              "Meteorología          782\n",
              "LLuvia                782\n",
              "Viento                782\n",
              "Temperatura           782\n",
              "Hipodromo             782\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import sklearn.compose\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/SergioNietoP/machine-learning-datasheets/main/Learning%20Datasheets/Turf%20Prediction/Data%20Turf%20Races%20Train%20-%206.csv'\n",
        "\n",
        "url1 = 'https://raw.githubusercontent.com/SergioNietoP/machine-learning-datasheets/main/Learning%20Datasheets/Turf%20Prediction/Data%20Turf%20Races%20Test%20-%202.csv'\n",
        "test = pd.read_csv(url1, sep=',')\n",
        "\n",
        "\n",
        "train = pd.read_csv(url, sep=',')\n",
        "\n",
        "train = train.drop('DistanciaMeta', axis=1)\n",
        "train = train.drop('Dividendo', axis=1)\n",
        "\n",
        "ogData = train\n",
        "train.head()\n",
        "train.count()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def getDebutantes(arrayLast):\n",
        "#   aux = []\n",
        "#   index = 0\n",
        "#   for element in arrayLast:\n",
        "#     element = element.replace(',', '')\n",
        "#     element = element.replace('[', '')\n",
        "#     element = element[:len(element)-1]\n",
        "#     if 'Debutante' in element:\n",
        "#       aux.append(index)\n",
        "#     index += 1\n",
        "#   return aux\n",
        "\n",
        "# #Desarrollar crear columnas en \n",
        "\n",
        "# def getNombres(array):\n",
        "#   aux = []\n",
        "#   for element in array:\n",
        "#     nombre = train.at[element, 'NombreCaballo']\n",
        "#     nombre = nombre.partition('(')[0]\n",
        "#     aux.append(nombre)\n",
        "#   return aux\n",
        "\n",
        "# def createColumnas(array):\n",
        "#   for element in array:\n",
        "#     train[element] = 0\n",
        "    \n",
        "# t_array = test[\"UltimasActuaciones\"]\n",
        "# variable = getDebutantes(t_array)\n",
        "# columnas = getNombres(variable)\n",
        "# createColumnas(columnas)"
      ],
      "metadata": {
        "id": "rSZqsxMXe5sc"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBAEU6JSkbnA"
      },
      "source": [
        "# Preprocesamiento\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "VAIAe2irmQV4",
        "outputId": "6fb31a2b-607d-4ad8-c4c7-0d4d54ebdfb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Puesto        NombreCaballo Peso  Edad  Mantilla          Propietario  \\\n",
              "0       1               SAFAGA   57     2         6  ASOC.LA TOLEDANA...   \n",
              "1       2            ROCK MOON   57     2         5              MEDREAL   \n",
              "2       3               ISOLDA   57     2         3           EL HERROJO   \n",
              "3       4          LADY CLUNIA   57     2         4           BRAZACORTA   \n",
              "4       5  DE FIESTA (IRE) (a)   57     2         2         REZA PAZOOKI   \n",
              "5       6          ALERTA ROJA   57     2         1  LAC INTERNACIONA...   \n",
              "6       1               BAILEN   57     2         3              LEONESA   \n",
              "7       2        ARAKA LA KANA   57     2         2               ODISEA   \n",
              "8       3               WITIZA   57     2         9  LAC INTERNACIONA...   \n",
              "9       4             PERILLAN   57     2         7      E. PEREZ GUZMAN   \n",
              "\n",
              "      Preparador       Jinete Problemas UltimasActuaciones  \\\n",
              "0     CH.DELCHER  G.GUEDJ-GAY       NaN         [02,04,01]   \n",
              "1  G.ARIZKORRETA   V. JANACEK       NaN            [04,06]   \n",
              "2     J.M.OSORIO   J.GELABERT       NaN               [01]   \n",
              "3         B.RAMA     B. FAYOS        -8            [08,15]   \n",
              "4        O.ANAYA    R.N.VALLE        -8            [09,11]   \n",
              "5         A.SOTO     C. CADEL        -3            [07,05]   \n",
              "6        J.LOPEZ   J.GELABERT       NaN            [01,08]   \n",
              "7        J.LOPEZ      C.PEREZ        -3               [02]   \n",
              "8        J.LOPEZ     B. FAYOS        -3         [05,04,04]   \n",
              "9  J.A.RODRIGUEZ    R.N.VALLE        -8   [07,14,05,01,02]   \n",
              "\n",
              "                     Fecha   Hora     Terreno  Distancia  Tipo Categoría  \\\n",
              "0  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "1  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "2  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "3  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "4  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "5  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "6  13 de noviembre de 2022  12:20  H - Blando       1500  Liso         C   \n",
              "7  13 de noviembre de 2022  12:20  H - Blando       1500  Liso         C   \n",
              "8  13 de noviembre de 2022  12:20  H - Blando       1500  Liso         C   \n",
              "9  13 de noviembre de 2022  12:20  H - Blando       1500  Liso         C   \n",
              "\n",
              "   SentidoHipodromo          Meteorología  LLuvia  Viento  Temperatura  \\\n",
              "0                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "1                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "2                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "3                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "4                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "5                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "6                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "7                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "8                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "9                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "\n",
              "     Hipodromo  \n",
              "0  La Zarzuela  \n",
              "1  La Zarzuela  \n",
              "2  La Zarzuela  \n",
              "3  La Zarzuela  \n",
              "4  La Zarzuela  \n",
              "5  La Zarzuela  \n",
              "6  La Zarzuela  \n",
              "7  La Zarzuela  \n",
              "8  La Zarzuela  \n",
              "9  La Zarzuela  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac44eb74-5a10-4d99-a9e0-0828288f2756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Puesto</th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>SAFAGA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>ASOC.LA TOLEDANA...</td>\n",
              "      <td>CH.DELCHER</td>\n",
              "      <td>G.GUEDJ-GAY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[02,04,01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ROCK MOON</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>MEDREAL</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[04,06]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ISOLDA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>EL HERROJO</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>LADY CLUNIA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>BRAZACORTA</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[08,15]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>DE FIESTA (IRE) (a)</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>REZA PAZOOKI</td>\n",
              "      <td>O.ANAYA</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>-8</td>\n",
              "      <td>[09,11]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>ALERTA ROJA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>LAC INTERNACIONA...</td>\n",
              "      <td>A.SOTO</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>-3</td>\n",
              "      <td>[07,05]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>BAILEN</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>LEONESA</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[01,08]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>ARAKA LA KANA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ODISEA</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>C.PEREZ</td>\n",
              "      <td>-3</td>\n",
              "      <td>[02]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>WITIZA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>LAC INTERNACIONA...</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-3</td>\n",
              "      <td>[05,04,04]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>PERILLAN</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>E. PEREZ GUZMAN</td>\n",
              "      <td>J.A.RODRIGUEZ</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>-8</td>\n",
              "      <td>[07,14,05,01,02]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac44eb74-5a10-4d99-a9e0-0828288f2756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac44eb74-5a10-4d99-a9e0-0828288f2756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac44eb74-5a10-4d99-a9e0-0828288f2756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "train.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkuLFwodhPi"
      },
      "source": [
        "## **Tratamiento (Puesto)(Train)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "fiCZavCsFC0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cae47f-b517-493c-da89-8f9f1739bafe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Puesto                757\n",
              "NombreCaballo         757\n",
              "Peso                  757\n",
              "Edad                  757\n",
              "Mantilla              757\n",
              "Propietario           757\n",
              "Preparador            757\n",
              "Jinete                757\n",
              "Problemas             414\n",
              "UltimasActuaciones    757\n",
              "Fecha                 757\n",
              "Hora                  757\n",
              "Terreno               757\n",
              "Distancia             757\n",
              "Tipo                  756\n",
              "Categoría             757\n",
              "SentidoHipodromo      757\n",
              "Meteorología          757\n",
              "LLuvia                757\n",
              "Viento                757\n",
              "Temperatura           757\n",
              "Hipodromo             757\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "train = train.drop(train[train.Puesto >= 90].index)\n",
        "train = train.reset_index(drop=True)\n",
        "train.count()\n",
        "# train['Puesto'] = train['Puesto'].replace(95, 10) # Penalización Parado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train['Puesto'] = train['Puesto'].replace(94, 10) # Penalización Parado"
      ],
      "metadata": {
        "id": "FMid_fSOk7H6"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "GZ0pl92Bdi13"
      },
      "outputs": [],
      "source": [
        "# train['Puesto'] = train['Puesto'].replace(97, 13) # Penalización Retirado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQUBnHP2c_B_"
      },
      "source": [
        "## **Tratamiento (Fecha)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "CP0UP3Vsektt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f06de9-3307-4f3a-b996-a61006c81d60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     2022-11-13\n",
              "1     2022-11-13\n",
              "2     2022-11-13\n",
              "3     2022-11-13\n",
              "4     2022-11-13\n",
              "5     2022-11-13\n",
              "6     2022-11-13\n",
              "7     2022-11-13\n",
              "8     2022-11-13\n",
              "9     2022-11-13\n",
              "10    2022-11-13\n",
              "11    2022-11-13\n",
              "12    2022-11-13\n",
              "13    2022-11-13\n",
              "14    2022-11-13\n",
              "15    2022-11-13\n",
              "16    2022-11-13\n",
              "17    2022-11-13\n",
              "18    2022-11-13\n",
              "19    2022-11-13\n",
              "20    2022-11-13\n",
              "21    2022-11-13\n",
              "22    2022-11-13\n",
              "23    2022-11-13\n",
              "24    2022-11-13\n",
              "25    2022-11-13\n",
              "26    2022-11-13\n",
              "27    2022-11-13\n",
              "28    2022-11-13\n",
              "29    2022-11-13\n",
              "30    2022-11-13\n",
              "31    2022-11-13\n",
              "32    2022-11-13\n",
              "33    2022-11-13\n",
              "34    2022-11-13\n",
              "35    2022-11-13\n",
              "36    2022-11-13\n",
              "37    2022-11-13\n",
              "38    2022-11-13\n",
              "39    2022-11-13\n",
              "40    2022-11-13\n",
              "41    2022-11-13\n",
              "42    2022-11-13\n",
              "43    2022-11-13\n",
              "44    2022-11-13\n",
              "45    2022-11-13\n",
              "46    2022-11-13\n",
              "47    2022-11-13\n",
              "48    2022-11-13\n",
              "49    2022-11-13\n",
              "50    2022-11-13\n",
              "51    2022-11-13\n",
              "52    2022-11-13\n",
              "53    2022-11-13\n",
              "54    2022-11-13\n",
              "55    2022-11-13\n",
              "56    2022-11-13\n",
              "57    2022-11-13\n",
              "58    2022-11-13\n",
              "59    2022-11-13\n",
              "60    2022-11-13\n",
              "61    2022-11-13\n",
              "62    2022-11-13\n",
              "63    2022-11-13\n",
              "64    2022-11-13\n",
              "65    2022-11-13\n",
              "66    2022-11-06\n",
              "67    2022-11-06\n",
              "68    2022-11-06\n",
              "69    2022-11-06\n",
              "70    2022-11-06\n",
              "71    2022-11-06\n",
              "72    2022-11-06\n",
              "73    2022-11-06\n",
              "74    2022-11-06\n",
              "75    2022-11-06\n",
              "76    2022-11-06\n",
              "77    2022-11-06\n",
              "78    2022-11-06\n",
              "79    2022-11-06\n",
              "80    2022-11-06\n",
              "81    2022-11-06\n",
              "82    2022-11-06\n",
              "83    2022-11-06\n",
              "84    2022-11-06\n",
              "85    2022-11-06\n",
              "86    2022-11-06\n",
              "87    2022-11-06\n",
              "88    2022-11-06\n",
              "89    2022-11-06\n",
              "90    2022-11-06\n",
              "91    2022-11-06\n",
              "92    2022-11-06\n",
              "93    2022-11-06\n",
              "94    2022-11-06\n",
              "95    2022-11-06\n",
              "96    2022-11-06\n",
              "97    2022-11-06\n",
              "98    2022-11-06\n",
              "99    2022-11-06\n",
              "100   2022-11-06\n",
              "101   2022-11-06\n",
              "102   2022-11-06\n",
              "103   2022-11-06\n",
              "104   2022-11-06\n",
              "105   2022-11-06\n",
              "106   2022-11-06\n",
              "107   2022-11-06\n",
              "108   2022-11-06\n",
              "109   2022-11-06\n",
              "110   2022-11-06\n",
              "111   2022-11-06\n",
              "112   2022-11-06\n",
              "113   2022-11-06\n",
              "114   2022-11-06\n",
              "115   2022-11-06\n",
              "116   2022-11-06\n",
              "117   2022-11-06\n",
              "118   2022-11-06\n",
              "119   2022-11-06\n",
              "Name: FechaAux, dtype: datetime64[ns]"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "import datetime as dt\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "def switchMonth(month):\n",
        "  if month == 'ENERO':  \n",
        "   return '01'\n",
        "  elif month == 'FEBRERO':\n",
        "    return '02'            \n",
        "  elif month == 'MARZO':\n",
        "   return '03'\n",
        "  elif month == 'ABRIL':\n",
        "    return '04' \n",
        "  elif month == 'MAYO':\n",
        "    return '05'                  \n",
        "  elif month == 'JUNIO':\n",
        "    return '06'\n",
        "  elif month == 'JULIO':\n",
        "    return '07'\n",
        "  elif month == 'AGOSTO':\n",
        "    return '08'\n",
        "  elif month == 'SEPTIEMBRE':\n",
        "    return '09'\n",
        "  elif month == 'OCTUBRE':\n",
        "     return '10'\n",
        "  elif month == 'NOVIEMBRE':\n",
        "    return '11'\n",
        "  elif month == 'DICIEMBRE':\n",
        "    return '12'\n",
        "\n",
        "def convertDate(fechas):\n",
        "  index = 0\n",
        "  for element in fechas:\n",
        "    sum = 0\n",
        "    year = element[-4:]\n",
        "    dateAux = year\n",
        "    day = element[:2]\n",
        "    if(int(day) < 10):\n",
        "      day = \"0\" + day[:1]\n",
        "    month = element[5:]\n",
        "    month = ''.join(month.split())[:-6].upper()\n",
        "    month = switchMonth(month)\n",
        "    dateAux = dateAux + \"-\" + month + \"-\" + day\n",
        "\n",
        "    today = date.today()\n",
        "    yearToday = int(str(today)[:4])\n",
        "    monthToday = int(str(today)[5:7])\n",
        "    dayToday = int(str(today)[8:10])\n",
        "    if(yearToday > int(year)):\n",
        "      sum += (yearToday - int(year))*365\n",
        "    sum += (monthToday - int(month))*30\n",
        "    sum += (dayToday - int(day))\n",
        "    datetimeAux = dt.datetime(int(year), int(month), int(day))\n",
        "    train.at[index, 'FechaAux'] = datetimeAux\n",
        "\n",
        "    train.at[index, 'year'] = int(year)\n",
        "    train.at[index, 'month'] = int(month)\n",
        "    train.at[index, 'day'] = int(day)\n",
        "    calculateSeason(int(month), index)\n",
        "\n",
        "    train.at[index, 'DiasDesdeCarrera'] = sum\n",
        "    index += 1\n",
        "   \n",
        "def calculateSeason(month, index):\n",
        "  # train['Invierno'] = 0\n",
        "  # train['Primavera'] = 0\n",
        "  # train['Verano'] = 0\n",
        "  train['Otoño'] = 0\n",
        "  # if(month >= 12 and month <=2):\n",
        "  #    train.at[index, 'Invierno'] = month\n",
        "  # if(month >= 3 and month <=5):\n",
        "  #    train.at[index, 'Primavera'] = month\n",
        "  # if(month >= 6 and month <=8):\n",
        "  #    train.at[index, 'Verano'] = month\n",
        "  if(month >= 9 and month <=11):\n",
        "     train.at[index, 'Otoño'] = month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "t_array = train[\"Fecha\"] \n",
        "convertDate(t_array)\n",
        "train['FechaAux'].head(120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "wVxaeGrr8y7t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "d3d179de-8380-412b-ebcc-7db30b451823"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Puesto        NombreCaballo Peso  Edad  Mantilla          Propietario  \\\n",
              "0       1               SAFAGA   57     2         6  ASOC.LA TOLEDANA...   \n",
              "1       2            ROCK MOON   57     2         5              MEDREAL   \n",
              "2       3               ISOLDA   57     2         3           EL HERROJO   \n",
              "3       4          LADY CLUNIA   57     2         4           BRAZACORTA   \n",
              "4       5  DE FIESTA (IRE) (a)   57     2         2         REZA PAZOOKI   \n",
              "\n",
              "      Preparador       Jinete Problemas UltimasActuaciones  \\\n",
              "0     CH.DELCHER  G.GUEDJ-GAY       NaN         [02,04,01]   \n",
              "1  G.ARIZKORRETA   V. JANACEK       NaN            [04,06]   \n",
              "2     J.M.OSORIO   J.GELABERT       NaN               [01]   \n",
              "3         B.RAMA     B. FAYOS        -8            [08,15]   \n",
              "4        O.ANAYA    R.N.VALLE        -8            [09,11]   \n",
              "\n",
              "                     Fecha   Hora     Terreno  Distancia  Tipo Categoría  \\\n",
              "0  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "1  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "2  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "3  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "4  13 de noviembre de 2022  11:45  H - Blando       1500  Liso         C   \n",
              "\n",
              "   SentidoHipodromo          Meteorología  LLuvia  Viento  Temperatura  \\\n",
              "0                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "1                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "2                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "3                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "4                 0  Parcialmente nublado     0.0       6         15.0   \n",
              "\n",
              "     Hipodromo   FechaAux    year  month   day  Otoño  DiasDesdeCarrera  \n",
              "0  La Zarzuela 2022-11-13  2022.0   11.0  13.0      0              37.0  \n",
              "1  La Zarzuela 2022-11-13  2022.0   11.0  13.0      0              37.0  \n",
              "2  La Zarzuela 2022-11-13  2022.0   11.0  13.0      0              37.0  \n",
              "3  La Zarzuela 2022-11-13  2022.0   11.0  13.0      0              37.0  \n",
              "4  La Zarzuela 2022-11-13  2022.0   11.0  13.0      0              37.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85a19269-6b8a-4819-9dc7-6a8281053fc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Puesto</th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>SAFAGA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>ASOC.LA TOLEDANA...</td>\n",
              "      <td>CH.DELCHER</td>\n",
              "      <td>G.GUEDJ-GAY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[02,04,01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>ROCK MOON</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>MEDREAL</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[04,06]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>ISOLDA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>EL HERROJO</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>LADY CLUNIA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>BRAZACORTA</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[08,15]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>DE FIESTA (IRE) (a)</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>REZA PAZOOKI</td>\n",
              "      <td>O.ANAYA</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>-8</td>\n",
              "      <td>[09,11]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85a19269-6b8a-4819-9dc7-6a8281053fc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85a19269-6b8a-4819-9dc7-6a8281053fc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85a19269-6b8a-4819-9dc7-6a8281053fc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "22J42KX73vcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed6b6c0-6590-419f-cd32-33defd943f98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     35.0\n",
              "1     45.0\n",
              "2     14.0\n",
              "3     35.0\n",
              "4     45.0\n",
              "5     32.0\n",
              "6     32.0\n",
              "7     32.0\n",
              "8     35.0\n",
              "9     21.0\n",
              "10    32.0\n",
              "11     7.0\n",
              "12    35.0\n",
              "13    32.0\n",
              "14    35.0\n",
              "15    63.0\n",
              "16    45.0\n",
              "17    14.0\n",
              "18    45.0\n",
              "19    35.0\n",
              "20    35.0\n",
              "21    35.0\n",
              "22    63.0\n",
              "23    35.0\n",
              "24    14.0\n",
              "25    14.0\n",
              "26    14.0\n",
              "27    42.0\n",
              "28    21.0\n",
              "29    21.0\n",
              "30    21.0\n",
              "31    42.0\n",
              "32    45.0\n",
              "33     7.0\n",
              "34    28.0\n",
              "Name: DaysSincePreviousRace, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "def daysSincePreviousRace(nombres):\n",
        "  index = 0\n",
        "  for caballo in nombres:\n",
        "    indexNombre = 0\n",
        "    nombresAparece = []\n",
        "    for otroCaballo in nombres:\n",
        "      if(otroCaballo == caballo):\n",
        "        nombresAparece.append(indexNombre)\n",
        "      indexNombre += 1\n",
        "\n",
        "    day_actual = train.at[index, 'day']\n",
        "    month_actual = train.at[index, 'month']\n",
        "    year_actual = train.at[index, 'year']\n",
        "\n",
        "    fechaCaballoActual = dt.datetime(int(year_actual), int(month_actual), int(day_actual))\n",
        "    fechaMasReciente = dt.datetime(2020, 1, 1)\n",
        "    indexAux2 = 0\n",
        "    fechaCambiada = False\n",
        "    for aparicion in nombresAparece:\n",
        "      if(train.at[aparicion, 'FechaAux'] < fechaCaballoActual): \n",
        "        if(indexAux2 == 0):\n",
        "          fechaMasReciente = train.at[aparicion, 'FechaAux']\n",
        "          fechaCambiada = True\n",
        "          indexAux2 += 1\n",
        "        elif(train.at[aparicion, 'FechaAux'] > fechaMasReciente):\n",
        "          fechaMasReciente =  train.at[aparicion, 'FechaAux']\n",
        "    if(fechaCambiada):\n",
        "      diferenciaDias = (fechaCaballoActual - fechaMasReciente).days\n",
        "    else:\n",
        "      diferenciaDias = 45 # Penalización si no aparece en carrera anterior\n",
        "    train.at[index, 'DaysSincePreviousRace'] = diferenciaDias\n",
        "    index += 1\n",
        "\n",
        "\n",
        "eventos = train['NombreCaballo']\n",
        "daysSincePreviousRace(eventos)\n",
        "train['DaysSincePreviousRace'].head(35)\n",
        "# Recorro los nombres de caballos y guardo en un array los index de las columnas ligadas a un nombre de Caballo y despues recorrer las fechas y cuando coincida el array comprobar si ese dato\n",
        "# es mas antiguo que la fecha de la fila actual y en el caso de serlo compruebo si es la más antigua de entre las anteriores participaciones\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tratamiento (NombreCaballo)(Train)**"
      ],
      "metadata": {
        "id": "pRrKU5JlLmug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eraseBlank(list):\n",
        "  aux = []\n",
        "  for element in list:\n",
        "    element = element.strip()\n",
        "    aux.append(element)\n",
        "  return aux\n",
        "\n",
        "test['NombreCaballo'] = test['NombreCaballo'].str.partition('(')[0]\n",
        "listaNombreCaballo = test['NombreCaballo'].unique().tolist()\n",
        "\n",
        "listTestHorse= eraseBlank(listaNombreCaballo)\n",
        "\n",
        "print(listTestHorse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X9s-JwZLuv4",
        "outputId": "87b4aaad-7b7f-4db5-829f-7605fa862ca2"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PHILIPPO', 'PONCE DE LEON', 'EL PATER', 'BRAGNOSERA', 'BERTIZ', 'SANDRO', 'GRAZALEMA', 'NICOLASA', 'LA PERLA NEGRA', 'WHITY', 'AFRODITA', 'VADALEW', 'HEBE', 'WENDY', 'MADRE MIA', 'LADY RAZALMA', 'FAITH ROSE', \"WARRIOR'S REVENGE\", 'SHELBY', 'WHITE WINE', 'VITA BARELIERE', 'XILADO', 'WINTON', 'AMERICANO', 'SUPERIOR BEAUTY', 'STARSHADOW', 'FUOCO GRECO', 'SANCTI PETRI', 'ROBLON', 'WALKING TO GLORY', 'FORTUNATO', 'AUSTRALIA CAPE', 'MAITRE YODA', 'ASTURIAS', 'SEVERUS', 'ROBAYERA', 'VIKING CITY', 'KANE ORE', 'EMBAT', 'MONTERREDONDO', 'HADES', 'USI DE U', 'MEDICEAN BLUE', 'EL CANEY', 'FINELY TUNED', 'THE GAME', 'ORBAYO', 'ASTRAL', 'TRES DE TREBOL', 'SOGALINDA', 'CHUSQUEZ', 'SOFUNNY', 'LA MAL AMADA', 'ATLANTICO', 'CAROLINA WEST', 'JERY SMAIH', 'RUMBERA', 'SANS ATTENDRE', 'HIGHLAND MARKET', 'PIU BIRCH', 'ARETHA', 'IZAMAL', 'UPSDAWN', 'UPSILON']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listTestHorse.sort()\n",
        "print(listTestHorse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-nJbwAdR7Ci",
        "outputId": "8fff15a6-7002-4fcb-db56-27c22e1ea8a1"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AFRODITA', 'AMERICANO', 'ARETHA', 'ASTRAL', 'ASTURIAS', 'ATLANTICO', 'AUSTRALIA CAPE', 'BERTIZ', 'BRAGNOSERA', 'CAROLINA WEST', 'CHUSQUEZ', 'EL CANEY', 'EL PATER', 'EMBAT', 'FAITH ROSE', 'FINELY TUNED', 'FORTUNATO', 'FUOCO GRECO', 'GRAZALEMA', 'HADES', 'HEBE', 'HIGHLAND MARKET', 'IZAMAL', 'JERY SMAIH', 'KANE ORE', 'LA MAL AMADA', 'LA PERLA NEGRA', 'LADY RAZALMA', 'MADRE MIA', 'MAITRE YODA', 'MEDICEAN BLUE', 'MONTERREDONDO', 'NICOLASA', 'ORBAYO', 'PHILIPPO', 'PIU BIRCH', 'PONCE DE LEON', 'ROBAYERA', 'ROBLON', 'RUMBERA', 'SANCTI PETRI', 'SANDRO', 'SANS ATTENDRE', 'SEVERUS', 'SHELBY', 'SOFUNNY', 'SOGALINDA', 'STARSHADOW', 'SUPERIOR BEAUTY', 'THE GAME', 'TRES DE TREBOL', 'UPSDAWN', 'UPSILON', 'USI DE U', 'VADALEW', 'VIKING CITY', 'VITA BARELIERE', 'WALKING TO GLORY', \"WARRIOR'S REVENGE\", 'WENDY', 'WHITE WINE', 'WHITY', 'WINTON', 'XILADO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['Jinete'] = test['Jinete'].str.partition('(')[0]\n",
        "lista = test['Jinete'].unique().tolist()\n",
        "\n",
        "listTestJockey = eraseBlank(lista)\n",
        "\n",
        "test['Preparador'] = test['Preparador'].str.partition('(')[0]\n",
        "lista = test['Preparador'].unique().tolist()\n",
        "\n",
        "listTestTrainer = eraseBlank(lista)\n",
        "\n",
        "test['Propietario'] = test['Propietario'].str.partition('(')[0]\n",
        "lista = test['Propietario'].unique().tolist()\n",
        "\n",
        "listTestOwner = eraseBlank(lista)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZSeprKNlOoSA"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOxbWh3uid7V",
        "outputId": "6888014e-e337-469b-800b-00922645363e"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 757 entries, 0 to 756\n",
            "Data columns (total 29 columns):\n",
            " #   Column                 Non-Null Count  Dtype         \n",
            "---  ------                 --------------  -----         \n",
            " 0   Puesto                 757 non-null    int64         \n",
            " 1   NombreCaballo          757 non-null    object        \n",
            " 2   Peso                   757 non-null    object        \n",
            " 3   Edad                   757 non-null    int64         \n",
            " 4   Mantilla               757 non-null    int64         \n",
            " 5   Propietario            757 non-null    object        \n",
            " 6   Preparador             757 non-null    object        \n",
            " 7   Jinete                 757 non-null    object        \n",
            " 8   Problemas              414 non-null    object        \n",
            " 9   UltimasActuaciones     757 non-null    object        \n",
            " 10  Fecha                  757 non-null    object        \n",
            " 11  Hora                   757 non-null    object        \n",
            " 12  Terreno                757 non-null    object        \n",
            " 13  Distancia              757 non-null    int64         \n",
            " 14  Tipo                   756 non-null    object        \n",
            " 15  Categoría              757 non-null    object        \n",
            " 16  SentidoHipodromo       757 non-null    int64         \n",
            " 17  Meteorología           757 non-null    object        \n",
            " 18  LLuvia                 757 non-null    float64       \n",
            " 19  Viento                 757 non-null    int64         \n",
            " 20  Temperatura            757 non-null    float64       \n",
            " 21  Hipodromo              757 non-null    object        \n",
            " 22  FechaAux               757 non-null    datetime64[ns]\n",
            " 23  year                   757 non-null    float64       \n",
            " 24  month                  757 non-null    float64       \n",
            " 25  day                    757 non-null    float64       \n",
            " 26  Otoño                  757 non-null    int64         \n",
            " 27  DiasDesdeCarrera       757 non-null    float64       \n",
            " 28  DaysSincePreviousRace  757 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(7), int64(7), object(14)\n",
            "memory usage: 171.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def changeNotFoundInListHorse(listTrain, listTest):\n",
        "  index = 0\n",
        "  listaApariciones = []\n",
        "  for element in listTrain:\n",
        "    if(element not in listTest):\n",
        "      print(\"Encontrado:\", element)\n",
        "      train.at[index, 'NombreCaballo'] = 'OtherHorse'\n",
        "    else:\n",
        "      print(\"NOO Encontrado:\", element)\n",
        "      listaApariciones.append(element)\n",
        "    index += 1\n",
        "  return listaApariciones\n",
        "\n",
        "\n",
        "train['NombreCaballo'] = train['NombreCaballo'].str.partition('(')[0]\n",
        "listTrainHorse = train['NombreCaballo'].tolist()\n",
        "listTrainHorse= eraseBlank(listTrainHorse)"
      ],
      "metadata": {
        "id": "6atesVdLq4jl"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculo Contricantes\n",
        "---"
      ],
      "metadata": {
        "id": "gAhENIVQgUq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculoContrincantes(listaF, listaH, longitud):\n",
        "  for fecha in listaF:\n",
        "    for horario in listaH:\n",
        "      index = 0\n",
        "      listaIndex = []\n",
        "      sum = 0\n",
        "      while(index < longitud):\n",
        "        if((train.at[index, 'Fecha'] == fecha ) and (train.at[index, 'Hora'] == horario )):\n",
        "          listaIndex.append(index)\n",
        "          sum += 1\n",
        "        index += 1\n",
        "      for indice in listaIndex:\n",
        "        train.at[indice, 'Contrincantes'] = sum\n",
        "\n",
        "\n",
        "listaFechas = train['Fecha'].unique().tolist()\n",
        "listaHorarios = train['Hora'].unique().tolist()\n",
        "longitud = len(train.index)\n",
        "\n",
        "calculoContrincantes(listaFechas, listaHorarios, longitud)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m10TP9xcdekh"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculo Raiz Puesto \n",
        "---"
      ],
      "metadata": {
        "id": "FNHPqSxQ1-Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculoRaizPuesto(index):\n",
        "  i = 0\n",
        "  while(i < index):\n",
        "    train.at[i, 'Puesto'] = (train.at[i, 'Puesto'] / (train.at[i, 'Contrincantes']**(1. / 3.)))\n",
        "    i += 1\n",
        "  \n",
        "calculoRaizPuesto(len(train.index))\n"
      ],
      "metadata": {
        "id": "CVa8uN062CSG"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Busqueda Caballos Ganadores en ciertas condiciones**\n",
        "---"
      ],
      "metadata": {
        "id": "3tLWw1W6pSYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Busqueda Distancia\n",
        "---"
      ],
      "metadata": {
        "id": "QjeeiBjzpr_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def comprobacionTipoDistancia(indice):\n",
        "  tipo = 0   # 0, 1, 2 / Corta, Media, Larga\n",
        "  val = train.at[indice, 'Distancia']\n",
        "  if(val >= 3200):\n",
        "    tipo = 2\n",
        "  if((val >= 1600) & (val <= 2400)):\n",
        "    tipo = 1\n",
        "  return tipo;\n",
        "\n",
        "def destrezaDistancia(indices):                      \n",
        "  for i in range(indices):\n",
        "    print(i)\n",
        "    tipo = comprobacionTipoDistancia(i)\n",
        "    nombre = train.at[i, 'NombreCaballo']\n",
        "    fecha = train.at[i, 'FechaAux']\n",
        "    media = 0 # Media carreras\n",
        "    sum = 0 # Numero Carreras\n",
        "    mediaAux = 0 \n",
        "    sumAux = 0\n",
        "    for j in range(indices):\n",
        "      fechaAux = train.at[j, 'FechaAux']\n",
        "      tipoAux = comprobacionTipoDistancia(j)\n",
        "      nombreAux= train.at[j, 'NombreCaballo']\n",
        "      if(nombreAux == nombre):\n",
        "        if((fechaAux < fecha) & (tipoAux == tipo)):\n",
        "          media += (train.at[j, 'Puesto'] )\n",
        "          sum += 1\n",
        "        elif((fechaAux < fecha) & (tipoAux != tipo)):\n",
        "          mediaAux += (train.at[j, 'Puesto'] )\n",
        "          sumAux += 1\n",
        "    if(sum != 0):\n",
        "      media /= sum\n",
        "    if(sumAux != 0):\n",
        "      mediaAux /= sumAux\n",
        "    print(i, \"-->\", nombre, \" - Media->\", media, \"/MediaAux-->\", mediaAux, \"/Numero Destreza(\", sum, sumAux, \"):Numero Otros/\") # Si hay mas de 2 participaciones (en esas condiciones y resto ) y con una media al menos un 33% mas baja , es valido\n",
        "    if((sum >= 2) & (sumAux >= 2) & (((1.1* media) < mediaAux))): # Si hay mas de 2 participaciones (en esas condiciones y resto ) y con una media al menos un 33% mas baja , es valido\n",
        "      train.at[i, 'DestrezaDistancia'] = 1\n",
        "    else:\n",
        "      train.at[i, 'DestrezaDistancia'] = 0\n",
        "        \n",
        "\n",
        "indices = len(train.index)\n",
        "fechas = train['FechaAux'].tolist()\n",
        "trainOg = train\n",
        "destrezaDistancia(indices)\n",
        "\n",
        "\n",
        "#Len Intervalos\n",
        "print(len(train[(train['Distancia'] < 1600).tolist()]))\n",
        "print(len(train[((train['Distancia'] >= 1600) & (train['Distancia'] <= 2400) ).tolist()]))\n",
        "print(len(train[(train['Distancia'] >= 3200).tolist()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKE7dYDjp2JO",
        "outputId": "0a10435d-dea1-4523-b647-0c4d553465c8"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0 --> SAFAGA  - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "1\n",
            "1 --> ROCK MOON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "2\n",
            "2 --> ISOLDA  - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "3\n",
            "3 --> LADY CLUNIA  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "4\n",
            "4 --> DE FIESTA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "5\n",
            "5 --> ALERTA ROJA  - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "6\n",
            "6 --> BAILEN  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "7\n",
            "7 --> ARAKA LA KANA  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "8\n",
            "8 --> WITIZA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "9\n",
            "9 --> PERILLAN  - Media-> 3.3333333333333335 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "10\n",
            "10 --> MOM CHERIE   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "11\n",
            "11 --> BLUE MOON EYES  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "12\n",
            "12 --> CLIFFORD  - Media-> 1.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "13\n",
            "13 --> TRAPIO  - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "14\n",
            "14 --> CANARION   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "15\n",
            "15 --> MADERAS   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "16\n",
            "16 --> SIXTEEN TONS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "17\n",
            "17 --> ATLANTIC NORTH   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "18\n",
            "18 --> GLOBALIZATION   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "19\n",
            "19 --> FREE JAZZ   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "20\n",
            "20 --> URABA   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "21\n",
            "21 --> QUISQUILLA   - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "22\n",
            "22 --> SIDNEY  - Media-> 6.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "23\n",
            "23 --> BRAVE TOWN  - Media-> 6.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "24\n",
            "24 --> VALE   - Media-> 1.0 /MediaAux--> 2.0 /Numero Destreza( 2 2 ):Numero Otros/\n",
            "25\n",
            "25 --> BIG MACK  - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "26\n",
            "26 --> AARASH  - Media-> 3.5 /MediaAux--> 3.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "27\n",
            "27 --> ELAMIRR   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "28\n",
            "28 --> GRAN BELGA   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "29\n",
            "29 --> DUTCH KIKI   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "30\n",
            "30 --> ARENAL   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "31\n",
            "31 --> MAX'S THUNDER   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "32\n",
            "32 --> MONTERREDONDO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "33\n",
            "33 --> TIHANNA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "34\n",
            "34 --> SPANISH COLT   - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "35\n",
            "35 --> RAMIRIQUI   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "36\n",
            "36 --> RODABALLO   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "37\n",
            "37 --> RAIKU   - Media-> 0.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "38\n",
            "38 --> RESACON   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "39\n",
            "39 --> SPEAK IN COLOURS   - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "40\n",
            "40 --> SMASH HIT   - Media-> 0 /MediaAux--> 1.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "41\n",
            "41 --> NATURAL PATH   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "42\n",
            "42 --> UPA LOLA  - Media-> 0 /MediaAux--> 0.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "43\n",
            "43 --> KENDAYA   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "44\n",
            "44 --> POWERFUL SOLE   - Media-> 0 /MediaAux--> 2.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "45\n",
            "45 --> MAGIC WARRIOR   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "46\n",
            "46 --> SIMPLY STRIKING   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "47\n",
            "47 --> GUERREIRA   - Media-> 0 /MediaAux--> 2.3333333333333335 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "48\n",
            "48 --> SOGALINDA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "49\n",
            "49 --> DROVER   - Media-> 1.3333333333333333 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "50\n",
            "50 --> BELLETTI   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "51\n",
            "51 --> SPANISH CAMELOT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "52\n",
            "52 --> ATLANTICO   - Media-> 3.0 /MediaAux--> 1.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "53\n",
            "53 --> EINAR   - Media-> 2.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "54\n",
            "54 --> PIRLO  - Media-> 1.0 /MediaAux--> 6.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "55\n",
            "55 --> LA MAL AMADA  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "56\n",
            "56 --> NAYMA  - Media-> 3.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "57\n",
            "57 --> ROBLE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "58\n",
            "58 --> PADERNO   - Media-> 3.0 /MediaAux--> 4.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "59\n",
            "59 --> NAYADE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "60\n",
            "60 --> CHAM'S DREAM   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "61\n",
            "61 --> FIRST CROWD   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "62\n",
            "62 --> PIU BIRCH   - Media-> 3.4 /MediaAux--> 0 /Numero Destreza( 5 0 ):Numero Otros/\n",
            "63\n",
            "63 --> IZASKUN   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "64\n",
            "64 --> BRIAREO   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "65\n",
            "65 --> KWA HERI  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "66\n",
            "66 --> CASANDRA   - Media-> 1.3333333333333333 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "67\n",
            "67 --> HELHEIM   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "68\n",
            "68 --> AL BARIN   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "69\n",
            "69 --> PAPYRUS ROAD   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "70\n",
            "70 --> BLUE MOON EYES  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "71\n",
            "71 --> HARVAC  - Media-> 3.0 /MediaAux--> 5.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "72\n",
            "72 --> MARY JOE   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "73\n",
            "73 --> NOOZHAH SUREÑA  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "74\n",
            "74 --> ASTUTO   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "75\n",
            "75 --> ATLANTA   - Media-> 1.0 /MediaAux--> 2.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "76\n",
            "76 --> PEKEN   - Media-> 1.0 /MediaAux--> 0.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "77\n",
            "77 --> VEGADEO  - Media-> 3.0 /MediaAux--> 1.6666666666666667 /Numero Destreza( 1 3 ):Numero Otros/\n",
            "78\n",
            "78 --> BORONDON   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "79\n",
            "79 --> COOLMEEN VEGA   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "80\n",
            "80 --> CAT HACLYSME   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "81\n",
            "81 --> FAMILY TRIP   - Media-> 1.0 /MediaAux--> 3.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "82\n",
            "82 --> LADY MCQUEEN   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "83\n",
            "83 --> COLOSAL   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "84\n",
            "84 --> INCREDIT   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "85\n",
            "85 --> PREMIERE RHAPSODIE   - Media-> 2.6666666666666665 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "86\n",
            "86 --> PERFECT SUNRISE   - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "87\n",
            "87 --> BARBADO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "88\n",
            "88 --> SOMETHING  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "89\n",
            "89 --> DERDAS   - Media-> 1.75 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "90\n",
            "90 --> DAIQUIRYA  - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "91\n",
            "91 --> KOHOOF   - Media-> 1.25 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "92\n",
            "92 --> ALBORAN   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "93\n",
            "93 --> BRAVO  - Media-> 1.5 /MediaAux--> 3.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "94\n",
            "94 --> NEW JACK SWING   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "95\n",
            "95 --> TUNANTE  - Media-> 2.6666666666666665 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "96\n",
            "96 --> FINTAS   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "97\n",
            "97 --> ALNASHERAT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "98\n",
            "98 --> ASTIMEGOESBY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "99\n",
            "99 --> RUGBY INDUS   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "100\n",
            "100 --> TESSICO   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "101\n",
            "101 --> SMASH HIT   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "102\n",
            "102 --> ADAAYLIGHT DANCER   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "103\n",
            "103 --> WHAT'S UP   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "104\n",
            "104 --> BAADIRR   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "105\n",
            "105 --> SALLAB   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "106\n",
            "106 --> VETONA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "107\n",
            "107 --> TIHANNA   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "108\n",
            "108 --> SINCELEJO   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "109\n",
            "109 --> CORPS DES PAGES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "110\n",
            "110 --> GLORYTOF   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "111\n",
            "111 --> IL DECAMERONE   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "112\n",
            "112 --> MEDIA STORM   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "113\n",
            "113 --> MARACAY  - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "114\n",
            "114 --> SIR ROQUE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "115\n",
            "115 --> TARANTELA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "116\n",
            "116 --> ALABAMA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "117\n",
            "117 --> BABY GROOM  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "118\n",
            "118 --> COLIBRI   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "119\n",
            "119 --> ASTURIAS  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "120\n",
            "120 --> UPSILON   - Media-> 6.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "121\n",
            "121 --> SANBLASS  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "122\n",
            "122 --> ANAZ   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "123\n",
            "123 --> TE KOOP   - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "124\n",
            "124 --> ISOLDA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "125\n",
            "125 --> PRIAPO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "126\n",
            "126 --> SANCTI PETRI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "127\n",
            "127 --> ATLANTIC NORTH   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "128\n",
            "128 --> BRIGADOON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "129\n",
            "129 --> MORE LATE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "130\n",
            "130 --> BEL EVENT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "131\n",
            "131 --> CLASSIC RILEY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "132\n",
            "132 --> BIG MACK  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "133\n",
            "133 --> DABESPIR   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "134\n",
            "134 --> ATLANTICO   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "135\n",
            "135 --> AMIL   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "136\n",
            "136 --> LIGHTNINGSUN  - Media-> 2.0 /MediaAux--> 2.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "137\n",
            "137 --> VELETA  - Media-> 5.0 /MediaAux--> 2.5 /Numero Destreza( 1 2 ):Numero Otros/\n",
            "138\n",
            "138 --> NATURAL TALENT  - Media-> 0 /MediaAux--> 1.75 /Numero Destreza( 0 4 ):Numero Otros/\n",
            "139\n",
            "139 --> VINCERO   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "140\n",
            "140 --> VEGADEO  - Media-> 0 /MediaAux--> 1.6666666666666667 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "141\n",
            "141 --> CONSPIRACY   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "142\n",
            "142 --> MACHU PICCHU  - Media-> 3.0 /MediaAux--> 1.0 /Numero Destreza( 1 2 ):Numero Otros/\n",
            "143\n",
            "143 --> BRIAREO   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "144\n",
            "144 --> HALAGADA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "145\n",
            "145 --> UPSDAWN   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "146\n",
            "146 --> MANIOKA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "147\n",
            "147 --> HANNA  - Media-> 2.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "148\n",
            "148 --> LADY RAZALMA   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "149\n",
            "149 --> FAITH ROSE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "150\n",
            "150 --> ROBLON   - Media-> 3.0 /MediaAux--> 0.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "151\n",
            "151 --> PINSAPO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "152\n",
            "152 --> STARSHADOW   - Media-> 2.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "153\n",
            "153 --> SHELBY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "154\n",
            "154 --> WINTON   - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "155\n",
            "155 --> XILADO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "156\n",
            "156 --> UMBRA   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "157\n",
            "157 --> WARRIOR'S REVENGE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "158\n",
            "158 --> AURORITA   - Media-> 0 /MediaAux--> 5.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "159\n",
            "159 --> ATREVIDA   - Media-> 6.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "160\n",
            "160 --> WALKING TO GLORY   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "161\n",
            "161 --> FORTUNATO   - Media-> 0 /MediaAux--> 1.3333333333333333 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "162\n",
            "162 --> CHARMING LOOK   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "163\n",
            "163 --> COSTA ESMERALDA   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "164\n",
            "164 --> FIJI GOLD   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "165\n",
            "165 --> BABA KARAM  - Media-> 0 /MediaAux--> 1.6666666666666667 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "166\n",
            "166 --> TALES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "167\n",
            "167 --> VALE   - Media-> 2.0 /MediaAux--> 1.0 /Numero Destreza( 1 2 ):Numero Otros/\n",
            "168\n",
            "168 --> EINAR   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "169\n",
            "169 --> PEAKY BLINDERS  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "170\n",
            "170 --> AARASH  - Media-> 0 /MediaAux--> 3.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "171\n",
            "171 --> PADERNO   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "172\n",
            "172 --> BALLET STAR  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "173\n",
            "173 --> BIDUL  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "174\n",
            "174 --> SOMMERSUN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "175\n",
            "175 --> PETRA   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "176\n",
            "176 --> WAR OF DANCE   - Media-> 0 /MediaAux--> 0.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "177\n",
            "177 --> KOWALSKY   - Media-> 0.0 /MediaAux--> 0.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "178\n",
            "178 --> PAMPLONA  - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "179\n",
            "179 --> MONCAYO   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "180\n",
            "180 --> MAHATMA   - Media-> 0 /MediaAux--> 0.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "181\n",
            "181 --> ARQUETU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "182\n",
            "182 --> ASTRAL  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "183\n",
            "183 --> ORBAYO   - Media-> 0 /MediaAux--> 1.3333333333333333 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "184\n",
            "184 --> TRES DE TREBOL   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "185\n",
            "185 --> SIX RIVERS   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "186\n",
            "186 --> TRESEFES  - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "187\n",
            "187 --> COBRECES   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "188\n",
            "188 --> FINELY TUNED   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "189\n",
            "189 --> UROGALLO  - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "190\n",
            "190 --> RESUMPTION   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "191\n",
            "191 --> PENRHEAD   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "192\n",
            "192 --> VOLCAN   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "193\n",
            "193 --> AL BARIN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "194\n",
            "194 --> KENMYA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "195\n",
            "195 --> BLUE MOON EYES  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "196\n",
            "196 --> HELHEIM   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "197\n",
            "197 --> PERILLAN  - Media-> 3.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "198\n",
            "198 --> GRACIOSA CANARIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "199\n",
            "199 --> QUILLA MOON   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "200\n",
            "200 --> CAPOEIRA  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "201\n",
            "201 --> GLORYTOF   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "202\n",
            "202 --> GALILODGE   - Media-> 0 /MediaAux--> 5.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "203\n",
            "203 --> GRAN BELGA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "204\n",
            "204 --> ATLANTA   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "205\n",
            "205 --> COOLMEEN VEGA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "206\n",
            "206 --> CORVERA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "207\n",
            "207 --> NASAB   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "208\n",
            "208 --> IL DECAMERONE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "209\n",
            "209 --> FURIOSO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "210\n",
            "210 --> MARACAY  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "211\n",
            "211 --> ORZOWEI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "212\n",
            "212 --> RUGBY INDUS   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "213\n",
            "213 --> ARENAL   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "214\n",
            "214 --> SANSEVERO   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "215\n",
            "215 --> KOHOOF   - Media-> 1.6666666666666667 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "216\n",
            "216 --> DAIQUIRYA  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "217\n",
            "217 --> INCREDIT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "218\n",
            "218 --> DERDAS   - Media-> 1.6666666666666667 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "219\n",
            "219 --> BATTLE OF WATERLOO   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "220\n",
            "220 --> TUNANTE  - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "221\n",
            "221 --> MARIANCE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "222\n",
            "222 --> CORRECAMINOS  - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "223\n",
            "223 --> UPA LOLA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "224\n",
            "224 --> DAJLA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "225\n",
            "225 --> SUPER TRIP   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "226\n",
            "226 --> GUERREIRA   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "227\n",
            "227 --> DIVA VICTORIA   - Media-> 0.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "228\n",
            "228 --> CASILDA  - Media-> 0.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "229\n",
            "229 --> KENNOCHA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "230\n",
            "230 --> DEFINED   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "231\n",
            "231 --> ASTURIAS  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "232\n",
            "232 --> UROGALLO  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "233\n",
            "233 --> SEVERUS   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "234\n",
            "234 --> DUTCH KIKI   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "235\n",
            "235 --> MAITRE YODA   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "236\n",
            "236 --> EMBAT   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "237\n",
            "237 --> TESSICO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "238\n",
            "238 --> PEKEN   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "239\n",
            "239 --> PUELL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "240\n",
            "240 --> CAT HACLYSME   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "241\n",
            "241 --> VINCERO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "242\n",
            "242 --> DEMOSTHENE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "243\n",
            "243 --> BELLUNEZA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "244\n",
            "244 --> CARISMA  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "245\n",
            "245 --> AVE MUNDI  - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "246\n",
            "246 --> YESWECAN BARELIERE   - Media-> 3.0 /MediaAux--> 4.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "247\n",
            "247 --> PINTURERA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "248\n",
            "248 --> AIFOS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "249\n",
            "249 --> RUSHDY   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "250\n",
            "250 --> DABESPIR   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "251\n",
            "251 --> PIRLO  - Media-> 0 /MediaAux--> 6.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "252\n",
            "252 --> MACHU PICCHU  - Media-> 1.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "253\n",
            "253 --> MELODY FRANCE   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "254\n",
            "254 --> LIGHTNINGSUN  - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "255\n",
            "255 --> FAMILY TRIP   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "256\n",
            "256 --> HANNA  - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "257\n",
            "257 --> VELETA  - Media-> 1.0 /MediaAux--> 5.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "258\n",
            "258 --> BALLET STAR  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "259\n",
            "259 --> RAIKU   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "260\n",
            "260 --> SPEAK IN COLOURS   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "261\n",
            "261 --> KENDAYA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "262\n",
            "262 --> EL BOSNIA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "263\n",
            "263 --> FINTAS   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "264\n",
            "264 --> STOWEMAN   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "265\n",
            "265 --> EL SOKHNA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "266\n",
            "266 --> ADAAYLIGHT DANCER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "267\n",
            "267 --> TIHANNA   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "268\n",
            "268 --> SPANISH COLT   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "269\n",
            "269 --> VALE   - Media-> 1.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "270\n",
            "270 --> COSTA ESMERALDA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "271\n",
            "271 --> UNAMUNO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "272\n",
            "272 --> ROBLE   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "273\n",
            "273 --> AARASH  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "274\n",
            "274 --> BOSTON BRUIN   - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "275\n",
            "275 --> MUGUETAJARRA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "276\n",
            "276 --> PORTOFINO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "277\n",
            "277 --> THE GAME   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "278\n",
            "278 --> MEDIA STORM   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "279\n",
            "279 --> THE WAY OF BONNIE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "280\n",
            "280 --> PHILAU   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "281\n",
            "281 --> WHITE KING   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "282\n",
            "282 --> GO WITH THE WIND   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "283\n",
            "283 --> TARANTELA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "284\n",
            "284 --> SAY GOOD BUY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "285\n",
            "285 --> PAPER TROPHY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "286\n",
            "286 --> FINELY TUNED   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "287\n",
            "287 --> JASPEROID   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "288\n",
            "288 --> LORDOFTHEHORIZON   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "289\n",
            "289 --> JO PICKETT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "290\n",
            "290 --> GAHERIS   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "291\n",
            "291 --> DAYSHANN   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "292\n",
            "292 --> EL CANEY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "293\n",
            "293 --> CHEVALIER CATHARE   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "294\n",
            "294 --> EL INGRATO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "295\n",
            "295 --> BABY GROOM  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "296\n",
            "296 --> COLIBRI   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "297\n",
            "297 --> BABA KARAM  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "298\n",
            "298 --> EINAR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "299\n",
            "299 --> ASTRAL  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "300\n",
            "300 --> PEAKY BLINDERS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "301\n",
            "301 --> HEADHUNTER   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "302\n",
            "302 --> NAVIA   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "303\n",
            "303 --> KANE ORE   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "304\n",
            "304 --> KATARA  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "305\n",
            "305 --> IDOLE ROYALE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "306\n",
            "306 --> FIRST CROWD   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "307\n",
            "307 --> BAILEN  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "308\n",
            "308 --> ARAKA LA KANA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "309\n",
            "309 --> MOM CHERIE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "310\n",
            "310 --> COSI COSI  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "311\n",
            "311 --> URBIETA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "312\n",
            "312 --> LADY ANGIE  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "313\n",
            "313 --> ALERTA ROJA  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "314\n",
            "314 --> URRACA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "315\n",
            "315 --> AMAZING COOL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "316\n",
            "316 --> HIGHDARK BLUE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "317\n",
            "317 --> PICCOLISIMA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "318\n",
            "318 --> TRAPIO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "319\n",
            "319 --> VIKING CITY   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "320\n",
            "320 --> SOFUNNY   - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "321\n",
            "321 --> SOGALINDA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "322\n",
            "322 --> GOLD BEACH   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "323\n",
            "323 --> DROVER   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "324\n",
            "324 --> CHAM'S DREAM   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "325\n",
            "325 --> RUMBERA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "326\n",
            "326 --> DIMANCHE DE MAI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "327\n",
            "327 --> PIU BIRCH   - Media-> 3.25 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "328\n",
            "328 --> CAROLINA WEST  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "329\n",
            "329 --> IL DIVO  - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "330\n",
            "330 --> AMANDINE BARELIERE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "331\n",
            "331 --> ROBAYERA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "332\n",
            "332 --> SIX RIVERS   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "333\n",
            "333 --> SANS ATTENDRE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "334\n",
            "334 --> HIGHLAND MARKET   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "335\n",
            "335 --> ALPHABETIC   - Media-> 3.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "336\n",
            "336 --> VEREMOS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "337\n",
            "337 --> WINTER'S TALE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "338\n",
            "338 --> IZAMAL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "339\n",
            "339 --> PAIS DE GALES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "340\n",
            "340 --> SILVER DUST   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "341\n",
            "341 --> ORBAYO   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "342\n",
            "342 --> ASTURIAS  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "343\n",
            "343 --> COBRECES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "344\n",
            "344 --> ARETHA  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "345\n",
            "345 --> MONZALVOS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "346\n",
            "346 --> TRESEFES  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "347\n",
            "347 --> NATURAL TALENT  - Media-> 1.3333333333333333 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "348\n",
            "348 --> CHAM'S PRIDE   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "349\n",
            "349 --> SINCELEJO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "350\n",
            "350 --> LA MAL AMADA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "351\n",
            "351 --> TE KOOP   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "352\n",
            "352 --> SAMEDI RIEN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "353\n",
            "353 --> RODABALLO   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "354\n",
            "354 --> SIMPLY STRIKING   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "355\n",
            "355 --> RESACON   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "356\n",
            "356 --> POWERFUL SOLE   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "357\n",
            "357 --> STAR OF BENGAL   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "358\n",
            "358 --> BRAVO  - Media-> 0 /MediaAux--> 1.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "359\n",
            "359 --> PRINCE HAMLET   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "360\n",
            "360 --> BAADIRR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "361\n",
            "361 --> WHAT'S UP   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "362\n",
            "362 --> MR HOBBS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "363\n",
            "363 --> DANCE JUPITER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "364\n",
            "364 --> GIPSY BLUE EYES   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "365\n",
            "365 --> CAPOEIRA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "366\n",
            "366 --> CORVERA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "367\n",
            "367 --> GRAN BELGA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "368\n",
            "368 --> NASAB   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "369\n",
            "369 --> ALABAMA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "370\n",
            "370 --> PADERNO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "371\n",
            "371 --> TURCO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "372\n",
            "372 --> KOWALSKY   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "373\n",
            "373 --> WAR OF DANCE   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "374\n",
            "374 --> CANARION   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "375\n",
            "375 --> SWIFTWAY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "376\n",
            "376 --> AMERICANO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "377\n",
            "377 --> CASANDRA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "378\n",
            "378 --> PENRHEAD   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "379\n",
            "379 --> URABA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "380\n",
            "380 --> KOMOTU   - Media-> 3.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "381\n",
            "381 --> FREE JAZZ   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "382\n",
            "382 --> CONCEPCION LINE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "383\n",
            "383 --> WINTON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "384\n",
            "384 --> QUISQUILLA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "385\n",
            "385 --> RATIONALEXUBERANCE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "386\n",
            "386 --> BRAVE TOWN  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "387\n",
            "387 --> LADY MOON  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "388\n",
            "388 --> SAFAGA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "389\n",
            "389 --> PAMPLONA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "390\n",
            "390 --> CLIFFORD  - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "391\n",
            "391 --> WITIZA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "392\n",
            "392 --> EL COMANDANTE  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "393\n",
            "393 --> STARSHADOW   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "394\n",
            "394 --> LADY CLUNIA  - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "395\n",
            "395 --> BLUE MOON EYES  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "396\n",
            "396 --> HARVAC  - Media-> 0 /MediaAux--> 5.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "397\n",
            "397 --> TEYMUR  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "398\n",
            "398 --> NOOZHAH SUREÑA  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "399\n",
            "399 --> KATALINA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "400\n",
            "400 --> PERILLAN  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "401\n",
            "401 --> WAYNA  - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "402\n",
            "402 --> AURORITA   - Media-> 5.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "403\n",
            "403 --> CONSPIRACY   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "404\n",
            "404 --> BIDUL  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "405\n",
            "405 --> PRETTY EMERALD   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "406\n",
            "406 --> ATLANTA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "407\n",
            "407 --> LADY MCQUEEN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "408\n",
            "408 --> UPSDAWN   - Media-> 6.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "409\n",
            "409 --> MARIANCE   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "410\n",
            "410 --> COLOSAL   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "411\n",
            "411 --> LINDA   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "412\n",
            "412 --> PREMIERE RHAPSODIE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "413\n",
            "413 --> PANTXINETA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "414\n",
            "414 --> ARENAS DE SANPEDRO  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "415\n",
            "415 --> GOYESCA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "416\n",
            "416 --> TRITON  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "417\n",
            "417 --> CALCAS  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "418\n",
            "418 --> VEGADEO  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "419\n",
            "419 --> TINTIN  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "420\n",
            "420 --> FINTAS   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "421\n",
            "421 --> PERFECT SUNRISE   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "422\n",
            "422 --> TUNANTE  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "423\n",
            "423 --> BORONDON   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "424\n",
            "424 --> KOHOOF   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "425\n",
            "425 --> EYE OF HEAVEN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "426\n",
            "426 --> CARISMA  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "427\n",
            "427 --> YESWECAN BARELIERE   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "428\n",
            "428 --> FARNESIO   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "429\n",
            "429 --> MAHATMA   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "430\n",
            "430 --> RED HOT ACTION   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "431\n",
            "431 --> UMBRA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "432\n",
            "432 --> ALERTA ROJA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "433\n",
            "433 --> ADHARHA   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "434\n",
            "434 --> SOGALINDA  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "435\n",
            "435 --> DABESPIR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "436\n",
            "436 --> NATURAL TALENT  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "437\n",
            "437 --> ROBLE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "438\n",
            "438 --> DROVER   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "439\n",
            "439 --> RUSHDY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "440\n",
            "440 --> IZASKUN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "441\n",
            "441 --> ATLANTICO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "442\n",
            "442 --> JACINTHE BERE   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "443\n",
            "443 --> SANBLASS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "444\n",
            "444 --> PIU BIRCH   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "445\n",
            "445 --> BELLETTI   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "446\n",
            "446 --> NAYMA  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "447\n",
            "447 --> COSSIO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "448\n",
            "448 --> IL DIVO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "449\n",
            "449 --> MELAMPO  - Media-> 0.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "450\n",
            "450 --> UPSILON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "451\n",
            "451 --> BERTIE FROM BEAU   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "452\n",
            "452 --> DISILLUSION   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "453\n",
            "453 --> QUEEN OF GLORY   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "454\n",
            "454 --> FORTUNATO   - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "455\n",
            "455 --> QUILLA MOON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "456\n",
            "456 --> HELHEIM   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "457\n",
            "457 --> MAITRE YODA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "458\n",
            "458 --> SPANISH COLT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "459\n",
            "459 --> DUTCH KIKI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "460\n",
            "460 --> MAX'S THUNDER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "461\n",
            "461 --> RAMIRIQUI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "462\n",
            "462 --> ARENAL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "463\n",
            "463 --> BABA KARAM  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "464\n",
            "464 --> ELAMIRR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "465\n",
            "465 --> AARASH  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "466\n",
            "466 --> CHARMING LOOK   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "467\n",
            "467 --> COSMIC HORIZON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "468\n",
            "468 --> CASILDA  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "469\n",
            "469 --> DIVA VICTORIA   - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "470\n",
            "470 --> UPA LOLA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "471\n",
            "471 --> AUNT AGATHA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "472\n",
            "472 --> PORTALMA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "473\n",
            "473 --> DAJLA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "474\n",
            "474 --> FIJI GOLD   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "475\n",
            "475 --> TIHANNA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "476\n",
            "476 --> GUERREIRA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "477\n",
            "477 --> STAR OF BENGAL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "478\n",
            "478 --> VETONA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "479\n",
            "479 --> RUGBY INDUS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "480\n",
            "480 --> SALLAB   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "481\n",
            "481 --> RESUMPTION   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "482\n",
            "482 --> YESWECAN BARELIERE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "483\n",
            "483 --> ALL IRON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "484\n",
            "484 --> KOWALSKY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "485\n",
            "485 --> MONCAYO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "486\n",
            "486 --> VOLCAN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "487\n",
            "487 --> SHELBY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "488\n",
            "488 --> CLIFFORD  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "489\n",
            "489 --> STARSHADOW   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "490\n",
            "490 --> AKBAHARI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "491\n",
            "491 --> ROBLON   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "492\n",
            "492 --> KOMOTU   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "493\n",
            "493 --> WAMBA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "494\n",
            "494 --> WAYNA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "495\n",
            "495 --> HARVAC  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "496\n",
            "496 --> PEYRASSOL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "497\n",
            "497 --> SPANISH RULER   - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "498\n",
            "498 --> ATREVIDA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "499\n",
            "499 --> FAMILY TRIP   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "500\n",
            "500 --> MELODY FRANCE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "501\n",
            "501 --> PEKEN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "502\n",
            "502 --> LIGHTNINGSUN  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "503\n",
            "503 --> BATTLE OF WATERLOO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "504\n",
            "504 --> KOHOOF   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "505\n",
            "505 --> HANNA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "506\n",
            "506 --> MACHU PICCHU  - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "507\n",
            "507 --> BLACK VOICE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "508\n",
            "508 --> IDEAL  - Media-> 2.3333333333333335 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "509\n",
            "509 --> CONSPIRACY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "510\n",
            "510 --> BODAK YELLOW   - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "511\n",
            "511 --> VELETA  - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "512\n",
            "512 --> OCEANIC BLUE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "513\n",
            "513 --> GRISOL  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "514\n",
            "514 --> PIRLO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "515\n",
            "515 --> LINDA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "516\n",
            "516 --> BIDUL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "517\n",
            "517 --> ATLANTA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "518\n",
            "518 --> PRETTY EMERALD   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "519\n",
            "519 --> ALBORAN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "520\n",
            "520 --> VALE   - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "521\n",
            "521 --> DIMAX   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "522\n",
            "522 --> CUPPACOFFEE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "523\n",
            "523 --> NEW JACK SWING   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "524\n",
            "524 --> DERDAS   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "525\n",
            "525 --> TIERMES  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "526\n",
            "526 --> BOSTON BRUIN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "527\n",
            "527 --> WEERT   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "528\n",
            "528 --> GALILODGE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "529\n",
            "529 --> TAZONES  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "530\n",
            "530 --> UPSDAWN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "531\n",
            "531 --> MARACAY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "532\n",
            "532 --> DAYSHANN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "533\n",
            "533 --> THE GAME   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "534\n",
            "534 --> FINELY TUNED   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "535\n",
            "535 --> GO WITH THE WIND   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "536\n",
            "536 --> LORDOFTHEHORIZON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "537\n",
            "537 --> CHEVALIER CATHARE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "538\n",
            "538 --> GAHERIS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "539\n",
            "539 --> PHILAU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "540\n",
            "540 --> GLORYTOF   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "541\n",
            "541 --> QATAR RIVER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "542\n",
            "542 --> WHITE KING   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "543\n",
            "543 --> LISICLES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "544\n",
            "544 --> AUSTRALIA CAPE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "545\n",
            "545 --> SIX RIVERS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "546\n",
            "546 --> BEL OUEST   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "547\n",
            "547 --> SEVERUS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "548\n",
            "548 --> HIGHLAND MARKET   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "549\n",
            "549 --> CHAMALE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "550\n",
            "550 --> CAMARINES   - Media-> 0 /MediaAux--> 0.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "551\n",
            "551 --> SPEAK IN COLOURS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "552\n",
            "552 --> EL BOSNIA  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "553\n",
            "553 --> STOWEMAN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "554\n",
            "554 --> KENDAYA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "555\n",
            "555 --> BRAVO  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "556\n",
            "556 --> FINTAS   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "557\n",
            "557 --> CORRECAMINOS  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "558\n",
            "558 --> TRITON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "559\n",
            "559 --> CALCAS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "560\n",
            "560 --> MACE WINDU  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "561\n",
            "561 --> TINTIN  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "562\n",
            "562 --> KATARA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "563\n",
            "563 --> BRIBON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "564\n",
            "564 --> ARENAS DE SANPEDRO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "565\n",
            "565 --> MAESTRANZA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "566\n",
            "566 --> MAHATMA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "567\n",
            "567 --> LADY MOON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "568\n",
            "568 --> WAR OF DANCE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "569\n",
            "569 --> WITIZA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "570\n",
            "570 --> PERILLAN  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "571\n",
            "571 --> URABA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "572\n",
            "572 --> COSI COSI  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "573\n",
            "573 --> FORTUNATO   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "574\n",
            "574 --> TEYMUR  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "575\n",
            "575 --> PETRA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "576\n",
            "576 --> ZHARINA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "577\n",
            "577 --> ARIANE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "578\n",
            "578 --> SAFAGA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "579\n",
            "579 --> ENORINA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "580\n",
            "580 --> QUEEN OF GLORY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "581\n",
            "581 --> WALKING TO GLORY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "582\n",
            "582 --> LADY RAZALMA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "583\n",
            "583 --> ADHARHA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "584\n",
            "584 --> CAROLINA WEST  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "585\n",
            "585 --> SOFUNNY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "586\n",
            "586 --> VIKING CITY   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "587\n",
            "587 --> VINCI LISA   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "588\n",
            "588 --> NATURAL TALENT  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "589\n",
            "589 --> HEY JUDE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "590\n",
            "590 --> SHE IS FIERCE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "591\n",
            "591 --> ARETHA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "592\n",
            "592 --> ALPHABETIC   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "593\n",
            "593 --> PIU BIRCH   - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "594\n",
            "594 --> RODABALLO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "595\n",
            "595 --> RESACON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "596\n",
            "596 --> GUERREIRA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "597\n",
            "597 --> RAIKU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "598\n",
            "598 --> SMASH HIT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "599\n",
            "599 --> POWERFUL SOLE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "600\n",
            "600 --> TRES DE TREBOL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "601\n",
            "601 --> EMBAT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "602\n",
            "602 --> ORBAYO   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "603\n",
            "603 --> ASTURIAS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "604\n",
            "604 --> COLIBRI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "605\n",
            "605 --> MEDICEAN BLUE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "606\n",
            "606 --> ASTRAL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "607\n",
            "607 --> BELLETTI   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "608\n",
            "608 --> DISILLUSION   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "609\n",
            "609 --> CASANDRA   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "610\n",
            "610 --> BLUE MOON EYES  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "611\n",
            "611 --> EL COMANDANTE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "612\n",
            "612 --> NOOZHAH SUREÑA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "613\n",
            "613 --> CONCEPCION LINE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "614\n",
            "614 --> MARY JOE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "615\n",
            "615 --> KOMOTU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "616\n",
            "616 --> MAX MAGICAL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "617\n",
            "617 --> WANDA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "618\n",
            "618 --> FARNESIO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "619\n",
            "619 --> ROBLON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "620\n",
            "620 --> PAMPLONA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "621\n",
            "621 --> MADERAS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "622\n",
            "622 --> SWIFTWAY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "623\n",
            "623 --> HELHEIM   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "624\n",
            "624 --> MOM CHERIE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "625\n",
            "625 --> BAILEN  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "626\n",
            "626 --> WILD HAWK   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "627\n",
            "627 --> WHITE WINE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "628\n",
            "628 --> VITA BARELIERE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "629\n",
            "629 --> SPANISH RULER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "630\n",
            "630 --> FLAMING GLASS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "631\n",
            "631 --> AURORITA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "632\n",
            "632 --> LADY CLUNIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "633\n",
            "633 --> SIDNEY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "634\n",
            "634 --> MAGIC WARRIOR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "635\n",
            "635 --> MAAMUR   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "636\n",
            "636 --> SOMETHING  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "637\n",
            "637 --> BRAVO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "638\n",
            "638 --> COLOSAL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "639\n",
            "639 --> DIVA VICTORIA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "640\n",
            "640 --> CASILDA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "641\n",
            "641 --> CARISMA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "642\n",
            "642 --> AVE MUNDI  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "643\n",
            "643 --> AZUL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "644\n",
            "644 --> VINCENZO  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "645\n",
            "645 --> EL BOSNIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "646\n",
            "646 --> DERDAS   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "647\n",
            "647 --> MARIANCE   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "648\n",
            "648 --> CORRECAMINOS  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "649\n",
            "649 --> FINTAS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "650\n",
            "650 --> BLACK VOICE   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "651\n",
            "651 --> TUNANTE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "652\n",
            "652 --> PREMIERE RHAPSODIE   - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "653\n",
            "653 --> NEW JACK SWING   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "654\n",
            "654 --> INES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "655\n",
            "655 --> SUPER TRIP   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "656\n",
            "656 --> HARDPIA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "657\n",
            "657 --> TARANTELA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "658\n",
            "658 --> KENNOCHA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "659\n",
            "659 --> FIRST CROWD   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "660\n",
            "660 --> NAVIA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "661\n",
            "661 --> KANE ORE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "662\n",
            "662 --> AMIL   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "663\n",
            "663 --> DAIQUIRYA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "664\n",
            "664 --> KOHOOF   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "665\n",
            "665 --> CONSPIRACY   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "666\n",
            "666 --> FAMILY TRIP   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "667\n",
            "667 --> SKIBO CASTLE   - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "668\n",
            "668 --> BORONDON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "669\n",
            "669 --> KRYPTON   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "670\n",
            "670 --> CHUSQUEZ   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "671\n",
            "671 --> IDEAL  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "672\n",
            "672 --> BEE IN YOUR BONNET   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "673\n",
            "673 --> SANSEVERO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "674\n",
            "674 --> IDEAL  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "675\n",
            "675 --> DERDAS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "676\n",
            "676 --> BRIAREO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "677\n",
            "677 --> MACADAMIA  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "678\n",
            "678 --> VEGADEO  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "679\n",
            "679 --> PANTXINETA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "680\n",
            "680 --> PIU BIRCH   - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "681\n",
            "681 --> SAORI  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "682\n",
            "682 --> GRISOL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "683\n",
            "683 --> CAMARINES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "684\n",
            "684 --> VINCI LISA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "685\n",
            "685 --> SOFUNNY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "686\n",
            "686 --> CHAM'S DREAM   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "687\n",
            "687 --> PEDRO EL GRANDE   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "688\n",
            "688 --> NAYMA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "689\n",
            "689 --> IRUÑIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "690\n",
            "690 --> CHAM'S PRIDE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "691\n",
            "691 --> MELAMPO  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "692\n",
            "692 --> DROVER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "693\n",
            "693 --> KARLUVY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "694\n",
            "694 --> SOGALINDA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "695\n",
            "695 --> SHE IS FIERCE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "696\n",
            "696 --> AMERICANISM   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "697\n",
            "697 --> HAVANA MAGIC   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "698\n",
            "698 --> SINDAWER  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "699\n",
            "699 --> ROSICRUCIEN   - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "700\n",
            "700 --> ROCHET ROUGE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "701\n",
            "701 --> BELADOR  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "702\n",
            "702 --> NATURAL TALENT  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "703\n",
            "703 --> RUSHDY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "704\n",
            "704 --> BALLET STAR  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "705\n",
            "705 --> CRIMSON MYSTERY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "706\n",
            "706 --> HEADHUNTER   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "707\n",
            "707 --> ORBAYO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "708\n",
            "708 --> AS DE OROS   - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "709\n",
            "709 --> BABA KARAM  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "710\n",
            "710 --> UROGALLO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "711\n",
            "711 --> AS DE OROS   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "712\n",
            "712 --> MAUNA LOA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "713\n",
            "713 --> PREMIERE RHAPSODIE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "714\n",
            "714 --> BLACK VOICE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "715\n",
            "715 --> CONSPIRACY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "716\n",
            "716 --> IDEAL  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "717\n",
            "717 --> MELODY FRANCE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "718\n",
            "718 --> CHAPMAN BILLY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "719\n",
            "719 --> LIGHTNINGSUN  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "720\n",
            "720 --> SKIBO CASTLE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "721\n",
            "721 --> PERFECT SUNRISE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "722\n",
            "722 --> TRASGU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "723\n",
            "723 --> MARIANCE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "724\n",
            "724 --> NEW JACK SWING   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "725\n",
            "725 --> VINCENZO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "726\n",
            "726 --> WEERT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "727\n",
            "727 --> MELAMPO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "728\n",
            "728 --> MACHU PICCHU  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "729\n",
            "729 --> VIOLETTA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "730\n",
            "730 --> GIPSY BLUE EYES   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "731\n",
            "731 --> FORTUNATO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "732\n",
            "732 --> ASTUTO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "733\n",
            "733 --> CANARION   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "734\n",
            "734 --> CASANDRA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "735\n",
            "735 --> LADY ANGIE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "736\n",
            "736 --> HADDAF   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "737\n",
            "737 --> VELETA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "738\n",
            "738 --> VALE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "739\n",
            "739 --> DARK PROFIT   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "740\n",
            "740 --> CHARMING LOOK   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "741\n",
            "741 --> BODAK YELLOW   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "742\n",
            "742 --> BERTIE FROM BEAU   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "743\n",
            "743 --> JACINTHE BERE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "744\n",
            "744 --> CAT HACLYSME   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "745\n",
            "745 --> FITERO   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "746\n",
            "746 --> ROSICRUCIEN   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "747\n",
            "747 --> ALPHABETIC   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "748\n",
            "748 --> VALL DE RUDA   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "749\n",
            "749 --> PURITAINE SPRING  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "750\n",
            "750 --> MACADAMIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "751\n",
            "751 --> FIRST CROWD   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "752\n",
            "752 --> VEGADEO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "753\n",
            "753 --> PEDRO EL GRANDE   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "754\n",
            "754 --> VIKING CITY   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "755\n",
            "755 --> PIU BIRCH   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "756\n",
            "756 --> SASLONG   - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "330\n",
            "409\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "train[['NombreCaballo','DestrezaDistancia']].head(501)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s_5eCJA6mSkm",
        "outputId": "426e1786-4bc5-4ed3-b2b9-6e544fbd0ff2"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           NombreCaballo  DestrezaDistancia\n",
              "0                 SAFAGA                0.0\n",
              "1              ROCK MOON                0.0\n",
              "2                 ISOLDA                0.0\n",
              "3            LADY CLUNIA                0.0\n",
              "4             DE FIESTA                 0.0\n",
              "5            ALERTA ROJA                0.0\n",
              "6                 BAILEN                0.0\n",
              "7          ARAKA LA KANA                0.0\n",
              "8                 WITIZA                0.0\n",
              "9               PERILLAN                0.0\n",
              "10           MOM CHERIE                 0.0\n",
              "11        BLUE MOON EYES                0.0\n",
              "12              CLIFFORD                0.0\n",
              "13                TRAPIO                0.0\n",
              "14             CANARION                 0.0\n",
              "15              MADERAS                 0.0\n",
              "16         SIXTEEN TONS                 0.0\n",
              "17       ATLANTIC NORTH                 0.0\n",
              "18        GLOBALIZATION                 0.0\n",
              "19            FREE JAZZ                 0.0\n",
              "20                URABA                 0.0\n",
              "21           QUISQUILLA                 0.0\n",
              "22                SIDNEY                0.0\n",
              "23            BRAVE TOWN                0.0\n",
              "24                 VALE                 1.0\n",
              "25              BIG MACK                0.0\n",
              "26                AARASH                0.0\n",
              "27              ELAMIRR                 0.0\n",
              "28           GRAN BELGA                 0.0\n",
              "29           DUTCH KIKI                 0.0\n",
              "30               ARENAL                 0.0\n",
              "31        MAX'S THUNDER                 0.0\n",
              "32        MONTERREDONDO                 0.0\n",
              "33              TIHANNA                 0.0\n",
              "34         SPANISH COLT                 0.0\n",
              "35            RAMIRIQUI                 0.0\n",
              "36            RODABALLO                 0.0\n",
              "37                RAIKU                 0.0\n",
              "38              RESACON                 0.0\n",
              "39     SPEAK IN COLOURS                 0.0\n",
              "40            SMASH HIT                 0.0\n",
              "41         NATURAL PATH                 0.0\n",
              "42              UPA LOLA                0.0\n",
              "43              KENDAYA                 0.0\n",
              "44        POWERFUL SOLE                 0.0\n",
              "45        MAGIC WARRIOR                 0.0\n",
              "46      SIMPLY STRIKING                 0.0\n",
              "47            GUERREIRA                 0.0\n",
              "48             SOGALINDA                0.0\n",
              "49               DROVER                 0.0\n",
              "50             BELLETTI                 0.0\n",
              "51      SPANISH CAMELOT                 0.0\n",
              "52            ATLANTICO                 0.0\n",
              "53                EINAR                 0.0\n",
              "54                 PIRLO                0.0\n",
              "55          LA MAL AMADA                0.0\n",
              "56                 NAYMA                0.0\n",
              "57                ROBLE                 0.0\n",
              "58              PADERNO                 0.0\n",
              "59               NAYADE                 0.0\n",
              "60         CHAM'S DREAM                 0.0\n",
              "61          FIRST CROWD                 0.0\n",
              "62            PIU BIRCH                 0.0\n",
              "63              IZASKUN                 0.0\n",
              "64              BRIAREO                 0.0\n",
              "65              KWA HERI                0.0\n",
              "66             CASANDRA                 0.0\n",
              "67              HELHEIM                 0.0\n",
              "68             AL BARIN                 0.0\n",
              "69         PAPYRUS ROAD                 0.0\n",
              "70        BLUE MOON EYES                0.0\n",
              "71                HARVAC                0.0\n",
              "72             MARY JOE                 0.0\n",
              "73        NOOZHAH SUREÑA                0.0\n",
              "74               ASTUTO                 0.0\n",
              "75              ATLANTA                 0.0\n",
              "76                PEKEN                 0.0\n",
              "77               VEGADEO                0.0\n",
              "78             BORONDON                 0.0\n",
              "79        COOLMEEN VEGA                 0.0\n",
              "80         CAT HACLYSME                 0.0\n",
              "81          FAMILY TRIP                 0.0\n",
              "82         LADY MCQUEEN                 0.0\n",
              "83              COLOSAL                 0.0\n",
              "84             INCREDIT                 0.0\n",
              "85   PREMIERE RHAPSODIE                 0.0\n",
              "86      PERFECT SUNRISE                 0.0\n",
              "87               BARBADO                0.0\n",
              "88             SOMETHING                0.0\n",
              "89               DERDAS                 0.0\n",
              "90             DAIQUIRYA                0.0\n",
              "91               KOHOOF                 0.0\n",
              "92              ALBORAN                 0.0\n",
              "93                 BRAVO                0.0\n",
              "94       NEW JACK SWING                 0.0\n",
              "95               TUNANTE                0.0\n",
              "96               FINTAS                 0.0\n",
              "97           ALNASHERAT                 0.0\n",
              "98         ASTIMEGOESBY                 0.0\n",
              "99          RUGBY INDUS                 0.0\n",
              "100             TESSICO                 0.0\n",
              "101           SMASH HIT                 0.0\n",
              "102   ADAAYLIGHT DANCER                 0.0\n",
              "103           WHAT'S UP                 0.0\n",
              "104             BAADIRR                 0.0\n",
              "105              SALLAB                 0.0\n",
              "106              VETONA                 0.0\n",
              "107             TIHANNA                 0.0\n",
              "108           SINCELEJO                 0.0\n",
              "109     CORPS DES PAGES                 0.0\n",
              "110            GLORYTOF                 0.0\n",
              "111       IL DECAMERONE                 0.0\n",
              "112         MEDIA STORM                 0.0\n",
              "113              MARACAY                0.0\n",
              "114           SIR ROQUE                 0.0\n",
              "115           TARANTELA                 0.0\n",
              "116             ALABAMA                 0.0\n",
              "117           BABY GROOM                0.0\n",
              "118             COLIBRI                 0.0\n",
              "119             ASTURIAS                0.0\n",
              "120             UPSILON                 0.0\n",
              "121             SANBLASS                0.0\n",
              "122                ANAZ                 0.0\n",
              "123             TE KOOP                 0.0\n",
              "124               ISOLDA                0.0\n",
              "125              PRIAPO                 0.0\n",
              "126        SANCTI PETRI                 0.0\n",
              "127      ATLANTIC NORTH                 0.0\n",
              "128           BRIGADOON                 0.0\n",
              "129            MORE LATE                0.0\n",
              "130           BEL EVENT                 0.0\n",
              "131       CLASSIC RILEY                 0.0\n",
              "132             BIG MACK                0.0\n",
              "133            DABESPIR                 0.0\n",
              "134           ATLANTICO                 0.0\n",
              "135                AMIL                 0.0\n",
              "136         LIGHTNINGSUN                0.0\n",
              "137               VELETA                0.0\n",
              "138       NATURAL TALENT                0.0\n",
              "139             VINCERO                 0.0\n",
              "140              VEGADEO                0.0\n",
              "141          CONSPIRACY                 0.0\n",
              "142         MACHU PICCHU                0.0\n",
              "143             BRIAREO                 0.0\n",
              "144             HALAGADA                0.0\n",
              "145             UPSDAWN                 0.0\n",
              "146             MANIOKA                 0.0\n",
              "147                HANNA                0.0\n",
              "148        LADY RAZALMA                 0.0\n",
              "149           FAITH ROSE                0.0\n",
              "150              ROBLON                 0.0\n",
              "151             PINSAPO                 0.0\n",
              "152          STARSHADOW                 0.0\n",
              "153              SHELBY                 0.0\n",
              "154              WINTON                 0.0\n",
              "155              XILADO                 0.0\n",
              "156               UMBRA                 0.0\n",
              "157   WARRIOR'S REVENGE                 0.0\n",
              "158            AURORITA                 0.0\n",
              "159            ATREVIDA                 0.0\n",
              "160    WALKING TO GLORY                 0.0\n",
              "161           FORTUNATO                 0.0\n",
              "162       CHARMING LOOK                 0.0\n",
              "163     COSTA ESMERALDA                 0.0\n",
              "164           FIJI GOLD                 0.0\n",
              "165           BABA KARAM                0.0\n",
              "166               TALES                 0.0\n",
              "167                VALE                 0.0\n",
              "168               EINAR                 0.0\n",
              "169       PEAKY BLINDERS                0.0\n",
              "170               AARASH                0.0\n",
              "171             PADERNO                 0.0\n",
              "172          BALLET STAR                0.0\n",
              "173                BIDUL                0.0\n",
              "174           SOMMERSUN                 0.0\n",
              "175               PETRA                 0.0\n",
              "176        WAR OF DANCE                 0.0\n",
              "177            KOWALSKY                 0.0\n",
              "178             PAMPLONA                0.0\n",
              "179             MONCAYO                 0.0\n",
              "180             MAHATMA                 0.0\n",
              "181             ARQUETU                 0.0\n",
              "182               ASTRAL                0.0\n",
              "183              ORBAYO                 0.0\n",
              "184      TRES DE TREBOL                 0.0\n",
              "185          SIX RIVERS                 0.0\n",
              "186             TRESEFES                0.0\n",
              "187            COBRECES                 0.0\n",
              "188        FINELY TUNED                 0.0\n",
              "189             UROGALLO                0.0\n",
              "190          RESUMPTION                 0.0\n",
              "191            PENRHEAD                 0.0\n",
              "192              VOLCAN                 0.0\n",
              "193            AL BARIN                 0.0\n",
              "194              KENMYA                 0.0\n",
              "195       BLUE MOON EYES                0.0\n",
              "196             HELHEIM                 0.0\n",
              "197             PERILLAN                0.0\n",
              "198     GRACIOSA CANARIA                0.0\n",
              "199         QUILLA MOON                 0.0\n",
              "200             CAPOEIRA                0.0\n",
              "201            GLORYTOF                 0.0\n",
              "202           GALILODGE                 0.0\n",
              "203          GRAN BELGA                 0.0\n",
              "204             ATLANTA                 0.0\n",
              "205       COOLMEEN VEGA                 0.0\n",
              "206             CORVERA                 0.0\n",
              "207               NASAB                 0.0\n",
              "208       IL DECAMERONE                 0.0\n",
              "209              FURIOSO                0.0\n",
              "210              MARACAY                0.0\n",
              "211             ORZOWEI                 0.0\n",
              "212         RUGBY INDUS                 0.0\n",
              "213              ARENAL                 0.0\n",
              "214           SANSEVERO                 0.0\n",
              "215              KOHOOF                 0.0\n",
              "216            DAIQUIRYA                0.0\n",
              "217            INCREDIT                 0.0\n",
              "218              DERDAS                 0.0\n",
              "219  BATTLE OF WATERLOO                 0.0\n",
              "220              TUNANTE                0.0\n",
              "221            MARIANCE                 0.0\n",
              "222         CORRECAMINOS                0.0\n",
              "223             UPA LOLA                0.0\n",
              "224               DAJLA                 0.0\n",
              "225          SUPER TRIP                 0.0\n",
              "226           GUERREIRA                 0.0\n",
              "227       DIVA VICTORIA                 0.0\n",
              "228              CASILDA                0.0\n",
              "229            KENNOCHA                 0.0\n",
              "230             DEFINED                 0.0\n",
              "231             ASTURIAS                0.0\n",
              "232             UROGALLO                0.0\n",
              "233             SEVERUS                 0.0\n",
              "234          DUTCH KIKI                 0.0\n",
              "235         MAITRE YODA                 0.0\n",
              "236               EMBAT                 0.0\n",
              "237             TESSICO                 0.0\n",
              "238               PEKEN                 0.0\n",
              "239               PUELL                 0.0\n",
              "240        CAT HACLYSME                 0.0\n",
              "241             VINCERO                 0.0\n",
              "242          DEMOSTHENE                 0.0\n",
              "243           BELLUNEZA                 0.0\n",
              "244              CARISMA                0.0\n",
              "245            AVE MUNDI                0.0\n",
              "246  YESWECAN BARELIERE                 0.0\n",
              "247            PINTURERA                0.0\n",
              "248                AIFOS                0.0\n",
              "249              RUSHDY                 0.0\n",
              "250            DABESPIR                 0.0\n",
              "251                PIRLO                0.0\n",
              "252         MACHU PICCHU                0.0\n",
              "253       MELODY FRANCE                 0.0\n",
              "254         LIGHTNINGSUN                0.0\n",
              "255         FAMILY TRIP                 0.0\n",
              "256                HANNA                0.0\n",
              "257               VELETA                0.0\n",
              "258          BALLET STAR                0.0\n",
              "259               RAIKU                 0.0\n",
              "260    SPEAK IN COLOURS                 0.0\n",
              "261             KENDAYA                 0.0\n",
              "262            EL BOSNIA                0.0\n",
              "263              FINTAS                 0.0\n",
              "264            STOWEMAN                 0.0\n",
              "265           EL SOKHNA                 0.0\n",
              "266   ADAAYLIGHT DANCER                 0.0\n",
              "267             TIHANNA                 0.0\n",
              "268        SPANISH COLT                 0.0\n",
              "269                VALE                 0.0\n",
              "270     COSTA ESMERALDA                 0.0\n",
              "271             UNAMUNO                 0.0\n",
              "272               ROBLE                 0.0\n",
              "273               AARASH                0.0\n",
              "274        BOSTON BRUIN                 0.0\n",
              "275         MUGUETAJARRA                0.0\n",
              "276           PORTOFINO                 0.0\n",
              "277            THE GAME                 0.0\n",
              "278         MEDIA STORM                 0.0\n",
              "279   THE WAY OF BONNIE                 0.0\n",
              "280              PHILAU                 0.0\n",
              "281          WHITE KING                 0.0\n",
              "282    GO WITH THE WIND                 0.0\n",
              "283           TARANTELA                 0.0\n",
              "284        SAY GOOD BUY                 0.0\n",
              "285        PAPER TROPHY                 0.0\n",
              "286        FINELY TUNED                 0.0\n",
              "287           JASPEROID                 0.0\n",
              "288    LORDOFTHEHORIZON                 0.0\n",
              "289          JO PICKETT                 0.0\n",
              "290             GAHERIS                 0.0\n",
              "291            DAYSHANN                 0.0\n",
              "292            EL CANEY                 0.0\n",
              "293   CHEVALIER CATHARE                 0.0\n",
              "294          EL INGRATO                 0.0\n",
              "295           BABY GROOM                0.0\n",
              "296             COLIBRI                 0.0\n",
              "297           BABA KARAM                0.0\n",
              "298               EINAR                 0.0\n",
              "299               ASTRAL                0.0\n",
              "300       PEAKY BLINDERS                0.0\n",
              "301          HEADHUNTER                 0.0\n",
              "302               NAVIA                 0.0\n",
              "303            KANE ORE                 0.0\n",
              "304               KATARA                0.0\n",
              "305        IDOLE ROYALE                 0.0\n",
              "306         FIRST CROWD                 0.0\n",
              "307               BAILEN                0.0\n",
              "308        ARAKA LA KANA                0.0\n",
              "309          MOM CHERIE                 0.0\n",
              "310            COSI COSI                0.0\n",
              "311              URBIETA                0.0\n",
              "312           LADY ANGIE                0.0\n",
              "313          ALERTA ROJA                0.0\n",
              "314               URRACA                0.0\n",
              "315         AMAZING COOL                0.0\n",
              "316        HIGHDARK BLUE                0.0\n",
              "317         PICCOLISIMA                 0.0\n",
              "318               TRAPIO                0.0\n",
              "319         VIKING CITY                 0.0\n",
              "320             SOFUNNY                 0.0\n",
              "321            SOGALINDA                0.0\n",
              "322          GOLD BEACH                 0.0\n",
              "323              DROVER                 0.0\n",
              "324        CHAM'S DREAM                 0.0\n",
              "325             RUMBERA                 0.0\n",
              "326     DIMANCHE DE MAI                 0.0\n",
              "327           PIU BIRCH                 0.0\n",
              "328        CAROLINA WEST                0.0\n",
              "329              IL DIVO                0.0\n",
              "330  AMANDINE BARELIERE                 0.0\n",
              "331            ROBAYERA                 0.0\n",
              "332          SIX RIVERS                 0.0\n",
              "333       SANS ATTENDRE                 0.0\n",
              "334     HIGHLAND MARKET                 0.0\n",
              "335          ALPHABETIC                 0.0\n",
              "336             VEREMOS                 0.0\n",
              "337       WINTER'S TALE                 0.0\n",
              "338               IZAMAL                0.0\n",
              "339       PAIS DE GALES                 0.0\n",
              "340         SILVER DUST                 0.0\n",
              "341              ORBAYO                 0.0\n",
              "342             ASTURIAS                0.0\n",
              "343            COBRECES                 0.0\n",
              "344               ARETHA                0.0\n",
              "345            MONZALVOS                0.0\n",
              "346             TRESEFES                0.0\n",
              "347       NATURAL TALENT                0.0\n",
              "348        CHAM'S PRIDE                 0.0\n",
              "349           SINCELEJO                 0.0\n",
              "350         LA MAL AMADA                0.0\n",
              "351             TE KOOP                 0.0\n",
              "352         SAMEDI RIEN                 0.0\n",
              "353           RODABALLO                 0.0\n",
              "354     SIMPLY STRIKING                 0.0\n",
              "355             RESACON                 0.0\n",
              "356       POWERFUL SOLE                 0.0\n",
              "357      STAR OF BENGAL                 0.0\n",
              "358                BRAVO                0.0\n",
              "359       PRINCE HAMLET                 0.0\n",
              "360             BAADIRR                 0.0\n",
              "361           WHAT'S UP                 0.0\n",
              "362            MR HOBBS                 0.0\n",
              "363       DANCE JUPITER                 0.0\n",
              "364     GIPSY BLUE EYES                 0.0\n",
              "365             CAPOEIRA                0.0\n",
              "366             CORVERA                 0.0\n",
              "367          GRAN BELGA                 0.0\n",
              "368               NASAB                 0.0\n",
              "369             ALABAMA                 0.0\n",
              "370             PADERNO                 0.0\n",
              "371                TURCO                0.0\n",
              "372            KOWALSKY                 0.0\n",
              "373        WAR OF DANCE                 0.0\n",
              "374            CANARION                 0.0\n",
              "375            SWIFTWAY                 0.0\n",
              "376           AMERICANO                 0.0\n",
              "377            CASANDRA                 0.0\n",
              "378            PENRHEAD                 0.0\n",
              "379               URABA                 0.0\n",
              "380              KOMOTU                 0.0\n",
              "381           FREE JAZZ                 0.0\n",
              "382     CONCEPCION LINE                 0.0\n",
              "383              WINTON                 0.0\n",
              "384          QUISQUILLA                 0.0\n",
              "385  RATIONALEXUBERANCE                 0.0\n",
              "386           BRAVE TOWN                0.0\n",
              "387            LADY MOON                0.0\n",
              "388               SAFAGA                0.0\n",
              "389             PAMPLONA                0.0\n",
              "390             CLIFFORD                0.0\n",
              "391               WITIZA                0.0\n",
              "392        EL COMANDANTE                0.0\n",
              "393          STARSHADOW                 0.0\n",
              "394          LADY CLUNIA                0.0\n",
              "395       BLUE MOON EYES                0.0\n",
              "396               HARVAC                0.0\n",
              "397               TEYMUR                0.0\n",
              "398       NOOZHAH SUREÑA                0.0\n",
              "399             KATALINA                0.0\n",
              "400             PERILLAN                0.0\n",
              "401                WAYNA                0.0\n",
              "402            AURORITA                 0.0\n",
              "403          CONSPIRACY                 0.0\n",
              "404                BIDUL                0.0\n",
              "405      PRETTY EMERALD                 0.0\n",
              "406             ATLANTA                 0.0\n",
              "407        LADY MCQUEEN                 0.0\n",
              "408             UPSDAWN                 0.0\n",
              "409            MARIANCE                 0.0\n",
              "410             COLOSAL                 0.0\n",
              "411               LINDA                 0.0\n",
              "412  PREMIERE RHAPSODIE                 0.0\n",
              "413           PANTXINETA                0.0\n",
              "414   ARENAS DE SANPEDRO                0.0\n",
              "415              GOYESCA                0.0\n",
              "416               TRITON                0.0\n",
              "417               CALCAS                0.0\n",
              "418              VEGADEO                0.0\n",
              "419               TINTIN                0.0\n",
              "420              FINTAS                 0.0\n",
              "421     PERFECT SUNRISE                 0.0\n",
              "422              TUNANTE                0.0\n",
              "423            BORONDON                 0.0\n",
              "424              KOHOOF                 0.0\n",
              "425       EYE OF HEAVEN                 0.0\n",
              "426              CARISMA                0.0\n",
              "427  YESWECAN BARELIERE                 0.0\n",
              "428            FARNESIO                 0.0\n",
              "429             MAHATMA                 0.0\n",
              "430      RED HOT ACTION                 0.0\n",
              "431               UMBRA                 0.0\n",
              "432          ALERTA ROJA                0.0\n",
              "433             ADHARHA                 0.0\n",
              "434            SOGALINDA                0.0\n",
              "435            DABESPIR                 0.0\n",
              "436       NATURAL TALENT                0.0\n",
              "437               ROBLE                 0.0\n",
              "438              DROVER                 0.0\n",
              "439              RUSHDY                 0.0\n",
              "440             IZASKUN                 0.0\n",
              "441           ATLANTICO                 0.0\n",
              "442       JACINTHE BERE                 0.0\n",
              "443             SANBLASS                0.0\n",
              "444           PIU BIRCH                 0.0\n",
              "445            BELLETTI                 0.0\n",
              "446                NAYMA                0.0\n",
              "447               COSSIO                0.0\n",
              "448              IL DIVO                0.0\n",
              "449              MELAMPO                0.0\n",
              "450             UPSILON                 0.0\n",
              "451    BERTIE FROM BEAU                 0.0\n",
              "452         DISILLUSION                 0.0\n",
              "453      QUEEN OF GLORY                 0.0\n",
              "454           FORTUNATO                 0.0\n",
              "455         QUILLA MOON                 0.0\n",
              "456             HELHEIM                 0.0\n",
              "457         MAITRE YODA                 0.0\n",
              "458        SPANISH COLT                 0.0\n",
              "459          DUTCH KIKI                 0.0\n",
              "460       MAX'S THUNDER                 0.0\n",
              "461           RAMIRIQUI                 0.0\n",
              "462              ARENAL                 0.0\n",
              "463           BABA KARAM                0.0\n",
              "464             ELAMIRR                 0.0\n",
              "465               AARASH                0.0\n",
              "466       CHARMING LOOK                 0.0\n",
              "467      COSMIC HORIZON                 0.0\n",
              "468              CASILDA                0.0\n",
              "469       DIVA VICTORIA                 0.0\n",
              "470             UPA LOLA                0.0\n",
              "471         AUNT AGATHA                 0.0\n",
              "472            PORTALMA                 0.0\n",
              "473               DAJLA                 0.0\n",
              "474           FIJI GOLD                 0.0\n",
              "475             TIHANNA                 0.0\n",
              "476           GUERREIRA                 0.0\n",
              "477      STAR OF BENGAL                 0.0\n",
              "478              VETONA                 0.0\n",
              "479         RUGBY INDUS                 0.0\n",
              "480              SALLAB                 0.0\n",
              "481          RESUMPTION                 0.0\n",
              "482  YESWECAN BARELIERE                 0.0\n",
              "483             ALL IRON                0.0\n",
              "484            KOWALSKY                 0.0\n",
              "485             MONCAYO                 0.0\n",
              "486              VOLCAN                 0.0\n",
              "487              SHELBY                 0.0\n",
              "488             CLIFFORD                0.0\n",
              "489          STARSHADOW                 0.0\n",
              "490            AKBAHARI                 0.0\n",
              "491              ROBLON                 0.0\n",
              "492              KOMOTU                 0.0\n",
              "493                WAMBA                0.0\n",
              "494                WAYNA                0.0\n",
              "495               HARVAC                0.0\n",
              "496           PEYRASSOL                 0.0\n",
              "497       SPANISH RULER                 0.0\n",
              "498            ATREVIDA                 0.0\n",
              "499         FAMILY TRIP                 0.0\n",
              "500       MELODY FRANCE                 0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94df8e41-5a99-4adb-ba5f-f14b58656032\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>DestrezaDistancia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SAFAGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ROCK MOON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISOLDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LADY CLUNIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DE FIESTA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ALERTA ROJA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>BAILEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ARAKA LA KANA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WITIZA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PERILLAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>MOM CHERIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>BLUE MOON EYES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CLIFFORD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>TRAPIO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>CANARION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>MADERAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SIXTEEN TONS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ATLANTIC NORTH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>GLOBALIZATION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>FREE JAZZ</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>URABA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>QUISQUILLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SIDNEY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>BRAVE TOWN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>VALE</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>BIG MACK</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AARASH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>ELAMIRR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>GRAN BELGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>DUTCH KIKI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>ARENAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>MAX'S THUNDER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>MONTERREDONDO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>TIHANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>SPANISH COLT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>RAMIRIQUI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>RODABALLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>RAIKU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>RESACON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>SPEAK IN COLOURS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>SMASH HIT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>NATURAL PATH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>UPA LOLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>KENDAYA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>POWERFUL SOLE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>MAGIC WARRIOR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>SIMPLY STRIKING</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>GUERREIRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>DROVER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>BELLETTI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>SPANISH CAMELOT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>EINAR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>PIRLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>NAYMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>ROBLE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>PADERNO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>NAYADE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>CHAM'S DREAM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>FIRST CROWD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>IZASKUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>BRIAREO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>KWA HERI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>CASANDRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>HELHEIM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>AL BARIN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>PAPYRUS ROAD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>BLUE MOON EYES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>HARVAC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>MARY JOE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>NOOZHAH SUREÑA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>ASTUTO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>ATLANTA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>PEKEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>VEGADEO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>BORONDON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>COOLMEEN VEGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>CAT HACLYSME</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>FAMILY TRIP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>LADY MCQUEEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>COLOSAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>INCREDIT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>PREMIERE RHAPSODIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>PERFECT SUNRISE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>BARBADO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>SOMETHING</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>DERDAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>DAIQUIRYA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>KOHOOF</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>ALBORAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>BRAVO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>NEW JACK SWING</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>TUNANTE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>FINTAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>ALNASHERAT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>ASTIMEGOESBY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>RUGBY INDUS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>TESSICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>SMASH HIT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>ADAAYLIGHT DANCER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>WHAT'S UP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>BAADIRR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>SALLAB</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>VETONA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>TIHANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>SINCELEJO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>CORPS DES PAGES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>GLORYTOF</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>IL DECAMERONE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>MEDIA STORM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>MARACAY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>SIR ROQUE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>TARANTELA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>ALABAMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>BABY GROOM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>COLIBRI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>UPSILON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>SANBLASS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>ANAZ</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>TE KOOP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>ISOLDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>PRIAPO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>SANCTI PETRI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>ATLANTIC NORTH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>BRIGADOON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>MORE LATE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>BEL EVENT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>CLASSIC RILEY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>BIG MACK</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>DABESPIR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>AMIL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>LIGHTNINGSUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>VELETA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>NATURAL TALENT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>VINCERO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>VEGADEO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>CONSPIRACY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>MACHU PICCHU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>BRIAREO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>HALAGADA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>UPSDAWN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>MANIOKA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>HANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>LADY RAZALMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>FAITH ROSE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>ROBLON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>PINSAPO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>SHELBY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>WINTON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>XILADO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>UMBRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>WARRIOR'S REVENGE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>AURORITA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>ATREVIDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>WALKING TO GLORY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>FORTUNATO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>CHARMING LOOK</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>COSTA ESMERALDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>FIJI GOLD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>BABA KARAM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>TALES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>VALE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>EINAR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>PEAKY BLINDERS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>AARASH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>PADERNO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>BALLET STAR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>BIDUL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>SOMMERSUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>PETRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>WAR OF DANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>KOWALSKY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>PAMPLONA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>MONCAYO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>MAHATMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>ARQUETU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>ASTRAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>ORBAYO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>TRES DE TREBOL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>SIX RIVERS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>TRESEFES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>COBRECES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>FINELY TUNED</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>UROGALLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>RESUMPTION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>PENRHEAD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>VOLCAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>AL BARIN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>KENMYA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>BLUE MOON EYES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>HELHEIM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>PERILLAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>GRACIOSA CANARIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>QUILLA MOON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>CAPOEIRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>GLORYTOF</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>GALILODGE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>GRAN BELGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>ATLANTA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>COOLMEEN VEGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>CORVERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>NASAB</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>IL DECAMERONE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>FURIOSO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>MARACAY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>ORZOWEI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>RUGBY INDUS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>ARENAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>SANSEVERO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>KOHOOF</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>DAIQUIRYA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>INCREDIT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>DERDAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>BATTLE OF WATERLOO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>TUNANTE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>MARIANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>CORRECAMINOS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>UPA LOLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>DAJLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>SUPER TRIP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>GUERREIRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>DIVA VICTORIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>CASILDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>KENNOCHA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>DEFINED</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>UROGALLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>SEVERUS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>DUTCH KIKI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>MAITRE YODA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>EMBAT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>TESSICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>PEKEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>PUELL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>CAT HACLYSME</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>VINCERO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>DEMOSTHENE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>BELLUNEZA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>CARISMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>AVE MUNDI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>YESWECAN BARELIERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>PINTURERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>AIFOS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>RUSHDY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>DABESPIR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>PIRLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>MACHU PICCHU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>MELODY FRANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>LIGHTNINGSUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>FAMILY TRIP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>HANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>VELETA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>BALLET STAR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>RAIKU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>SPEAK IN COLOURS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>KENDAYA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>EL BOSNIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>FINTAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>STOWEMAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>EL SOKHNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>ADAAYLIGHT DANCER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>TIHANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>SPANISH COLT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>VALE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>COSTA ESMERALDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>UNAMUNO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>ROBLE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>AARASH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>BOSTON BRUIN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>MUGUETAJARRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>PORTOFINO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>THE GAME</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>MEDIA STORM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>THE WAY OF BONNIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>PHILAU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>WHITE KING</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>GO WITH THE WIND</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>TARANTELA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>SAY GOOD BUY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>PAPER TROPHY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>FINELY TUNED</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>JASPEROID</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>LORDOFTHEHORIZON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>JO PICKETT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>GAHERIS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>DAYSHANN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>EL CANEY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>CHEVALIER CATHARE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>EL INGRATO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>BABY GROOM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>COLIBRI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>BABA KARAM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>EINAR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>ASTRAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>PEAKY BLINDERS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>HEADHUNTER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>NAVIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>KANE ORE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>KATARA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>IDOLE ROYALE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>FIRST CROWD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>BAILEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>ARAKA LA KANA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>MOM CHERIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>COSI COSI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>URBIETA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>LADY ANGIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>ALERTA ROJA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>URRACA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>AMAZING COOL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>HIGHDARK BLUE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>PICCOLISIMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>TRAPIO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>VIKING CITY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>SOFUNNY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>GOLD BEACH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>DROVER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>CHAM'S DREAM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>RUMBERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>DIMANCHE DE MAI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>CAROLINA WEST</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>IL DIVO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>AMANDINE BARELIERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>ROBAYERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>SIX RIVERS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>SANS ATTENDRE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>HIGHLAND MARKET</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>ALPHABETIC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>VEREMOS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>WINTER'S TALE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>IZAMAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>PAIS DE GALES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>SILVER DUST</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>ORBAYO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>COBRECES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>ARETHA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>MONZALVOS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>TRESEFES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>NATURAL TALENT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>CHAM'S PRIDE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>SINCELEJO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>TE KOOP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>SAMEDI RIEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>RODABALLO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>SIMPLY STRIKING</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>RESACON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>POWERFUL SOLE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>STAR OF BENGAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>BRAVO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>PRINCE HAMLET</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>BAADIRR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>WHAT'S UP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>MR HOBBS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>DANCE JUPITER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>GIPSY BLUE EYES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>CAPOEIRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>CORVERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>GRAN BELGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>NASAB</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>ALABAMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>PADERNO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>TURCO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>KOWALSKY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>WAR OF DANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>CANARION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>SWIFTWAY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>AMERICANO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>CASANDRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>PENRHEAD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>URABA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>KOMOTU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>FREE JAZZ</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>CONCEPCION LINE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>WINTON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>QUISQUILLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>RATIONALEXUBERANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>BRAVE TOWN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>LADY MOON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>SAFAGA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>PAMPLONA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>CLIFFORD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>WITIZA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>EL COMANDANTE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>LADY CLUNIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>BLUE MOON EYES</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>HARVAC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>TEYMUR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>NOOZHAH SUREÑA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>KATALINA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>PERILLAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>WAYNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>AURORITA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>CONSPIRACY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>BIDUL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>PRETTY EMERALD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>ATLANTA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>LADY MCQUEEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>UPSDAWN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>MARIANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>COLOSAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>LINDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>PREMIERE RHAPSODIE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>PANTXINETA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>ARENAS DE SANPEDRO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>GOYESCA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>TRITON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>CALCAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>VEGADEO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>TINTIN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>FINTAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>PERFECT SUNRISE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>TUNANTE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>BORONDON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>KOHOOF</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>EYE OF HEAVEN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>CARISMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>YESWECAN BARELIERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>FARNESIO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>MAHATMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>RED HOT ACTION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>UMBRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>ALERTA ROJA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>ADHARHA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>DABESPIR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>NATURAL TALENT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>ROBLE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>DROVER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>RUSHDY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>IZASKUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>JACINTHE BERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>SANBLASS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>BELLETTI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>NAYMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>COSSIO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>IL DIVO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>MELAMPO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>UPSILON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>BERTIE FROM BEAU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>DISILLUSION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>QUEEN OF GLORY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>FORTUNATO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>QUILLA MOON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>HELHEIM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>MAITRE YODA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>SPANISH COLT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>DUTCH KIKI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>MAX'S THUNDER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>RAMIRIQUI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>ARENAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>BABA KARAM</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>ELAMIRR</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>AARASH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>CHARMING LOOK</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>COSMIC HORIZON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>CASILDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>DIVA VICTORIA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470</th>\n",
              "      <td>UPA LOLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>AUNT AGATHA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>PORTALMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>DAJLA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>FIJI GOLD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>TIHANNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>GUERREIRA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>STAR OF BENGAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>VETONA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>RUGBY INDUS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>SALLAB</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>RESUMPTION</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>YESWECAN BARELIERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>ALL IRON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>KOWALSKY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>MONCAYO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>VOLCAN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>SHELBY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>CLIFFORD</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>AKBAHARI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>ROBLON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>KOMOTU</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>WAMBA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>WAYNA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>HARVAC</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>PEYRASSOL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>SPANISH RULER</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>ATREVIDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>FAMILY TRIP</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>MELODY FRANCE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94df8e41-5a99-4adb-ba5f-f14b58656032')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94df8e41-5a99-4adb-ba5f-f14b58656032 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94df8e41-5a99-4adb-ba5f-f14b58656032');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cszh0PPWqego"
      },
      "source": [
        "## **Tratamiento (Peso)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "U5ZxXKUOqooe"
      },
      "outputs": [],
      "source": [
        "train['Peso'] = train['Peso'].str.partition('-')[0]\n",
        "train['Peso'] = train['Peso'].str[:2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpO4SGiMqj1C",
        "outputId": "ca917373-2319-4b58-d335-cc1cf02ceb25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57    143\n",
              "56     87\n",
              "59     81\n",
              "58     74\n",
              "55     74\n",
              "54     72\n",
              "60     53\n",
              "61     45\n",
              "63     36\n",
              "62     34\n",
              "52     23\n",
              "53     17\n",
              "65      6\n",
              "66      3\n",
              "67      3\n",
              "69      2\n",
              "64      2\n",
              "70      2\n",
              "Name: Peso, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "train['Peso'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtUAW4RbsgBI"
      },
      "source": [
        "## **Tratamiento (Edad)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grrr_9F2sij7",
        "outputId": "38310e48-972a-449b-97f0-c00707341460"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3     213\n",
              "2     191\n",
              "4     153\n",
              "5     107\n",
              "6      49\n",
              "7      27\n",
              "8      14\n",
              "9       2\n",
              "10      1\n",
              "Name: Edad, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "train['Edad'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cG1h6OssuhQ"
      },
      "source": [
        "## **Tratamiento (Mantilla)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEVrL5dzswdZ",
        "outputId": "ce2ac089-c52b-46d5-dd07-a43e735d6293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2     80\n",
              "4     78\n",
              "5     77\n",
              "3     77\n",
              "1     77\n",
              "6     70\n",
              "7     64\n",
              "8     54\n",
              "9     47\n",
              "10    36\n",
              "11    29\n",
              "12    18\n",
              "13    14\n",
              "14    11\n",
              "15    10\n",
              "16     9\n",
              "17     4\n",
              "18     2\n",
              "Name: Mantilla, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "source": [
        "train['Mantilla'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h8B8ApIs-bo"
      },
      "source": [
        "## **Tratamiento (Propietario)(Train)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "pedFR5rLtC5y"
      },
      "outputs": [],
      "source": [
        "def changeNotFoundInListOwner(listTrain, listTest):\n",
        "  index = 0\n",
        "  listaApariciones = []\n",
        "  for element in listTrain:\n",
        "    if(element not in listTest):\n",
        "      train.at[index, 'Propietario'] = 'OtherOwner'\n",
        "    else:\n",
        "      listaApariciones.append(element)\n",
        "    index += 1\n",
        "  return listaApariciones\n",
        "    \n",
        "    \n",
        "\n",
        "#Lógica de lista de Test en \"Tratamiento(NombreCaballo)\"\n",
        "\n",
        "listTrainOwner = train['Propietario'].tolist()\n",
        "listTrainOwner= eraseBlank(listTrainOwner)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBp5FYGQte6u"
      },
      "source": [
        "## **Tratamiento (Preparador)(Train)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "oHjy0gIZtigN"
      },
      "outputs": [],
      "source": [
        "def changeNotFoundInListTrainer(listTrain, listTest):\n",
        "  index = 0\n",
        "  listaApariciones = []\n",
        "  for element in listTrain:\n",
        "    if(element not in listTest):\n",
        "      train.at[index, 'Preparador'] = 'OtherTrainer'\n",
        "    else:\n",
        "      listaApariciones.append(element)\n",
        "    index += 1\n",
        "  return listaApariciones\n",
        "\n",
        "#Lógica de lista de Test en \"Tratamiento(NombreCaballo)\"\n",
        "\n",
        "\n",
        "listTrainTrainer = train['Preparador'].tolist()\n",
        "listTrainTrainer= eraseBlank(listTrainTrainer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ-owC1wtr2D"
      },
      "source": [
        "## **Tratamiento (Jinete)(Train)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "SbcaVGyYtxmc"
      },
      "outputs": [],
      "source": [
        "def changeNotFoundInListJockey(listTrain, listTest):\n",
        "  index = 0\n",
        "  listaApariciones = []\n",
        "  for element in listTrain:\n",
        "    if(element not in listTest):\n",
        "      train.at[index, 'Jinete'] = 'OtherJockey'\n",
        "    else:\n",
        "      listaApariciones.append(element)\n",
        "    index += 1\n",
        "  return listaApariciones\n",
        "\n",
        "#Lógica de lista de Test en \"Tratamiento(NombreCaballo)\"\n",
        "\n",
        "\n",
        "listTrainJockey = train['Jinete'].tolist()\n",
        "listTrainJockey= eraseBlank(listTrainJockey)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tratamiento Change Not Found In Test(NombreCaballo)(Propieatario)(Preparador)(Jinete)**"
      ],
      "metadata": {
        "id": "-o4Y1o8zpBkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listAparicionesHorse= changeNotFoundInListHorse(listTrainHorse, listTestHorse)\n",
        "\n",
        "train['Propietario'] = train['Propietario'].str.strip()\n",
        "train['NombreCaballo'] = train['NombreCaballo'].str.strip()\n",
        "train['Jinete'] = train['Jinete'].str.strip()\n",
        "train['Preparador'] = train['Preparador'].str.strip()\n",
        "\n",
        "train['NombreCaballo'].head(65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnzs1eItOTd2",
        "outputId": "a5d006a2-ade4-43a9-9849-6fd1f12bc1fe"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encontrado: SAFAGA\n",
            "Encontrado: ROCK MOON\n",
            "Encontrado: ISOLDA\n",
            "Encontrado: LADY CLUNIA\n",
            "Encontrado: DE FIESTA\n",
            "Encontrado: ALERTA ROJA\n",
            "Encontrado: BAILEN\n",
            "Encontrado: ARAKA LA KANA\n",
            "Encontrado: WITIZA\n",
            "Encontrado: PERILLAN\n",
            "Encontrado: MOM CHERIE\n",
            "Encontrado: BLUE MOON EYES\n",
            "Encontrado: CLIFFORD\n",
            "Encontrado: TRAPIO\n",
            "Encontrado: CANARION\n",
            "Encontrado: MADERAS\n",
            "Encontrado: SIXTEEN TONS\n",
            "Encontrado: ATLANTIC NORTH\n",
            "Encontrado: GLOBALIZATION\n",
            "Encontrado: FREE JAZZ\n",
            "Encontrado: URABA\n",
            "Encontrado: QUISQUILLA\n",
            "Encontrado: SIDNEY\n",
            "Encontrado: BRAVE TOWN\n",
            "Encontrado: VALE\n",
            "Encontrado: BIG MACK\n",
            "Encontrado: AARASH\n",
            "Encontrado: ELAMIRR\n",
            "Encontrado: GRAN BELGA\n",
            "Encontrado: DUTCH KIKI\n",
            "Encontrado: ARENAL\n",
            "Encontrado: MAX'S THUNDER\n",
            "NOO Encontrado: MONTERREDONDO\n",
            "Encontrado: TIHANNA\n",
            "Encontrado: SPANISH COLT\n",
            "Encontrado: RAMIRIQUI\n",
            "Encontrado: RODABALLO\n",
            "Encontrado: RAIKU\n",
            "Encontrado: RESACON\n",
            "Encontrado: SPEAK IN COLOURS\n",
            "Encontrado: SMASH HIT\n",
            "Encontrado: NATURAL PATH\n",
            "Encontrado: UPA LOLA\n",
            "Encontrado: KENDAYA\n",
            "Encontrado: POWERFUL SOLE\n",
            "Encontrado: MAGIC WARRIOR\n",
            "Encontrado: SIMPLY STRIKING\n",
            "Encontrado: GUERREIRA\n",
            "NOO Encontrado: SOGALINDA\n",
            "Encontrado: DROVER\n",
            "Encontrado: BELLETTI\n",
            "Encontrado: SPANISH CAMELOT\n",
            "NOO Encontrado: ATLANTICO\n",
            "Encontrado: EINAR\n",
            "Encontrado: PIRLO\n",
            "NOO Encontrado: LA MAL AMADA\n",
            "Encontrado: NAYMA\n",
            "Encontrado: ROBLE\n",
            "Encontrado: PADERNO\n",
            "Encontrado: NAYADE\n",
            "Encontrado: CHAM'S DREAM\n",
            "Encontrado: FIRST CROWD\n",
            "NOO Encontrado: PIU BIRCH\n",
            "Encontrado: IZASKUN\n",
            "Encontrado: BRIAREO\n",
            "Encontrado: KWA HERI\n",
            "Encontrado: CASANDRA\n",
            "Encontrado: HELHEIM\n",
            "Encontrado: AL BARIN\n",
            "Encontrado: PAPYRUS ROAD\n",
            "Encontrado: BLUE MOON EYES\n",
            "Encontrado: HARVAC\n",
            "Encontrado: MARY JOE\n",
            "Encontrado: NOOZHAH SUREÑA\n",
            "Encontrado: ASTUTO\n",
            "Encontrado: ATLANTA\n",
            "Encontrado: PEKEN\n",
            "Encontrado: VEGADEO\n",
            "Encontrado: BORONDON\n",
            "Encontrado: COOLMEEN VEGA\n",
            "Encontrado: CAT HACLYSME\n",
            "Encontrado: FAMILY TRIP\n",
            "Encontrado: LADY MCQUEEN\n",
            "Encontrado: COLOSAL\n",
            "Encontrado: INCREDIT\n",
            "Encontrado: PREMIERE RHAPSODIE\n",
            "Encontrado: PERFECT SUNRISE\n",
            "Encontrado: BARBADO\n",
            "Encontrado: SOMETHING\n",
            "Encontrado: DERDAS\n",
            "Encontrado: DAIQUIRYA\n",
            "Encontrado: KOHOOF\n",
            "Encontrado: ALBORAN\n",
            "Encontrado: BRAVO\n",
            "Encontrado: NEW JACK SWING\n",
            "Encontrado: TUNANTE\n",
            "Encontrado: FINTAS\n",
            "Encontrado: ALNASHERAT\n",
            "Encontrado: ASTIMEGOESBY\n",
            "Encontrado: RUGBY INDUS\n",
            "Encontrado: TESSICO\n",
            "Encontrado: SMASH HIT\n",
            "Encontrado: ADAAYLIGHT DANCER\n",
            "Encontrado: WHAT'S UP\n",
            "Encontrado: BAADIRR\n",
            "Encontrado: SALLAB\n",
            "Encontrado: VETONA\n",
            "Encontrado: TIHANNA\n",
            "Encontrado: SINCELEJO\n",
            "Encontrado: CORPS DES PAGES\n",
            "Encontrado: GLORYTOF\n",
            "Encontrado: IL DECAMERONE\n",
            "Encontrado: MEDIA STORM\n",
            "Encontrado: MARACAY\n",
            "Encontrado: SIR ROQUE\n",
            "Encontrado: TARANTELA\n",
            "Encontrado: ALABAMA\n",
            "Encontrado: BABY GROOM\n",
            "Encontrado: COLIBRI\n",
            "NOO Encontrado: ASTURIAS\n",
            "NOO Encontrado: UPSILON\n",
            "Encontrado: SANBLASS\n",
            "Encontrado: ANAZ\n",
            "Encontrado: TE KOOP\n",
            "Encontrado: ISOLDA\n",
            "Encontrado: PRIAPO\n",
            "NOO Encontrado: SANCTI PETRI\n",
            "Encontrado: ATLANTIC NORTH\n",
            "Encontrado: BRIGADOON\n",
            "Encontrado: MORE LATE\n",
            "Encontrado: BEL EVENT\n",
            "Encontrado: CLASSIC RILEY\n",
            "Encontrado: BIG MACK\n",
            "Encontrado: DABESPIR\n",
            "NOO Encontrado: ATLANTICO\n",
            "Encontrado: AMIL\n",
            "Encontrado: LIGHTNINGSUN\n",
            "Encontrado: VELETA\n",
            "Encontrado: NATURAL TALENT\n",
            "Encontrado: VINCERO\n",
            "Encontrado: VEGADEO\n",
            "Encontrado: CONSPIRACY\n",
            "Encontrado: MACHU PICCHU\n",
            "Encontrado: BRIAREO\n",
            "Encontrado: HALAGADA\n",
            "NOO Encontrado: UPSDAWN\n",
            "Encontrado: MANIOKA\n",
            "Encontrado: HANNA\n",
            "NOO Encontrado: LADY RAZALMA\n",
            "NOO Encontrado: FAITH ROSE\n",
            "NOO Encontrado: ROBLON\n",
            "Encontrado: PINSAPO\n",
            "NOO Encontrado: STARSHADOW\n",
            "NOO Encontrado: SHELBY\n",
            "NOO Encontrado: WINTON\n",
            "NOO Encontrado: XILADO\n",
            "Encontrado: UMBRA\n",
            "NOO Encontrado: WARRIOR'S REVENGE\n",
            "Encontrado: AURORITA\n",
            "Encontrado: ATREVIDA\n",
            "NOO Encontrado: WALKING TO GLORY\n",
            "NOO Encontrado: FORTUNATO\n",
            "Encontrado: CHARMING LOOK\n",
            "Encontrado: COSTA ESMERALDA\n",
            "Encontrado: FIJI GOLD\n",
            "Encontrado: BABA KARAM\n",
            "Encontrado: TALES\n",
            "Encontrado: VALE\n",
            "Encontrado: EINAR\n",
            "Encontrado: PEAKY BLINDERS\n",
            "Encontrado: AARASH\n",
            "Encontrado: PADERNO\n",
            "Encontrado: BALLET STAR\n",
            "Encontrado: BIDUL\n",
            "Encontrado: SOMMERSUN\n",
            "Encontrado: PETRA\n",
            "Encontrado: WAR OF DANCE\n",
            "Encontrado: KOWALSKY\n",
            "Encontrado: PAMPLONA\n",
            "Encontrado: MONCAYO\n",
            "Encontrado: MAHATMA\n",
            "Encontrado: ARQUETU\n",
            "NOO Encontrado: ASTRAL\n",
            "NOO Encontrado: ORBAYO\n",
            "NOO Encontrado: TRES DE TREBOL\n",
            "Encontrado: SIX RIVERS\n",
            "Encontrado: TRESEFES\n",
            "Encontrado: COBRECES\n",
            "NOO Encontrado: FINELY TUNED\n",
            "Encontrado: UROGALLO\n",
            "Encontrado: RESUMPTION\n",
            "Encontrado: PENRHEAD\n",
            "Encontrado: VOLCAN\n",
            "Encontrado: AL BARIN\n",
            "Encontrado: KENMYA\n",
            "Encontrado: BLUE MOON EYES\n",
            "Encontrado: HELHEIM\n",
            "Encontrado: PERILLAN\n",
            "Encontrado: GRACIOSA CANARIA\n",
            "Encontrado: QUILLA MOON\n",
            "Encontrado: CAPOEIRA\n",
            "Encontrado: GLORYTOF\n",
            "Encontrado: GALILODGE\n",
            "Encontrado: GRAN BELGA\n",
            "Encontrado: ATLANTA\n",
            "Encontrado: COOLMEEN VEGA\n",
            "Encontrado: CORVERA\n",
            "Encontrado: NASAB\n",
            "Encontrado: IL DECAMERONE\n",
            "Encontrado: FURIOSO\n",
            "Encontrado: MARACAY\n",
            "Encontrado: ORZOWEI\n",
            "Encontrado: RUGBY INDUS\n",
            "Encontrado: ARENAL\n",
            "Encontrado: SANSEVERO\n",
            "Encontrado: KOHOOF\n",
            "Encontrado: DAIQUIRYA\n",
            "Encontrado: INCREDIT\n",
            "Encontrado: DERDAS\n",
            "Encontrado: BATTLE OF WATERLOO\n",
            "Encontrado: TUNANTE\n",
            "Encontrado: MARIANCE\n",
            "Encontrado: CORRECAMINOS\n",
            "Encontrado: UPA LOLA\n",
            "Encontrado: DAJLA\n",
            "Encontrado: SUPER TRIP\n",
            "Encontrado: GUERREIRA\n",
            "Encontrado: DIVA VICTORIA\n",
            "Encontrado: CASILDA\n",
            "Encontrado: KENNOCHA\n",
            "Encontrado: DEFINED\n",
            "NOO Encontrado: ASTURIAS\n",
            "Encontrado: UROGALLO\n",
            "NOO Encontrado: SEVERUS\n",
            "Encontrado: DUTCH KIKI\n",
            "NOO Encontrado: MAITRE YODA\n",
            "NOO Encontrado: EMBAT\n",
            "Encontrado: TESSICO\n",
            "Encontrado: PEKEN\n",
            "Encontrado: PUELL\n",
            "Encontrado: CAT HACLYSME\n",
            "Encontrado: VINCERO\n",
            "Encontrado: DEMOSTHENE\n",
            "Encontrado: BELLUNEZA\n",
            "Encontrado: CARISMA\n",
            "Encontrado: AVE MUNDI\n",
            "Encontrado: YESWECAN BARELIERE\n",
            "Encontrado: PINTURERA\n",
            "Encontrado: AIFOS\n",
            "Encontrado: RUSHDY\n",
            "Encontrado: DABESPIR\n",
            "Encontrado: PIRLO\n",
            "Encontrado: MACHU PICCHU\n",
            "Encontrado: MELODY FRANCE\n",
            "Encontrado: LIGHTNINGSUN\n",
            "Encontrado: FAMILY TRIP\n",
            "Encontrado: HANNA\n",
            "Encontrado: VELETA\n",
            "Encontrado: BALLET STAR\n",
            "Encontrado: RAIKU\n",
            "Encontrado: SPEAK IN COLOURS\n",
            "Encontrado: KENDAYA\n",
            "Encontrado: EL BOSNIA\n",
            "Encontrado: FINTAS\n",
            "Encontrado: STOWEMAN\n",
            "Encontrado: EL SOKHNA\n",
            "Encontrado: ADAAYLIGHT DANCER\n",
            "Encontrado: TIHANNA\n",
            "Encontrado: SPANISH COLT\n",
            "Encontrado: VALE\n",
            "Encontrado: COSTA ESMERALDA\n",
            "Encontrado: UNAMUNO\n",
            "Encontrado: ROBLE\n",
            "Encontrado: AARASH\n",
            "Encontrado: BOSTON BRUIN\n",
            "Encontrado: MUGUETAJARRA\n",
            "Encontrado: PORTOFINO\n",
            "NOO Encontrado: THE GAME\n",
            "Encontrado: MEDIA STORM\n",
            "Encontrado: THE WAY OF BONNIE\n",
            "Encontrado: PHILAU\n",
            "Encontrado: WHITE KING\n",
            "Encontrado: GO WITH THE WIND\n",
            "Encontrado: TARANTELA\n",
            "Encontrado: SAY GOOD BUY\n",
            "Encontrado: PAPER TROPHY\n",
            "NOO Encontrado: FINELY TUNED\n",
            "Encontrado: JASPEROID\n",
            "Encontrado: LORDOFTHEHORIZON\n",
            "Encontrado: JO PICKETT\n",
            "Encontrado: GAHERIS\n",
            "Encontrado: DAYSHANN\n",
            "NOO Encontrado: EL CANEY\n",
            "Encontrado: CHEVALIER CATHARE\n",
            "Encontrado: EL INGRATO\n",
            "Encontrado: BABY GROOM\n",
            "Encontrado: COLIBRI\n",
            "Encontrado: BABA KARAM\n",
            "Encontrado: EINAR\n",
            "NOO Encontrado: ASTRAL\n",
            "Encontrado: PEAKY BLINDERS\n",
            "Encontrado: HEADHUNTER\n",
            "Encontrado: NAVIA\n",
            "NOO Encontrado: KANE ORE\n",
            "Encontrado: KATARA\n",
            "Encontrado: IDOLE ROYALE\n",
            "Encontrado: FIRST CROWD\n",
            "Encontrado: BAILEN\n",
            "Encontrado: ARAKA LA KANA\n",
            "Encontrado: MOM CHERIE\n",
            "Encontrado: COSI COSI\n",
            "Encontrado: URBIETA\n",
            "Encontrado: LADY ANGIE\n",
            "Encontrado: ALERTA ROJA\n",
            "Encontrado: URRACA\n",
            "Encontrado: AMAZING COOL\n",
            "Encontrado: HIGHDARK BLUE\n",
            "Encontrado: PICCOLISIMA\n",
            "Encontrado: TRAPIO\n",
            "NOO Encontrado: VIKING CITY\n",
            "NOO Encontrado: SOFUNNY\n",
            "NOO Encontrado: SOGALINDA\n",
            "Encontrado: GOLD BEACH\n",
            "Encontrado: DROVER\n",
            "Encontrado: CHAM'S DREAM\n",
            "NOO Encontrado: RUMBERA\n",
            "Encontrado: DIMANCHE DE MAI\n",
            "NOO Encontrado: PIU BIRCH\n",
            "NOO Encontrado: CAROLINA WEST\n",
            "Encontrado: IL DIVO\n",
            "Encontrado: AMANDINE BARELIERE\n",
            "NOO Encontrado: ROBAYERA\n",
            "Encontrado: SIX RIVERS\n",
            "NOO Encontrado: SANS ATTENDRE\n",
            "NOO Encontrado: HIGHLAND MARKET\n",
            "Encontrado: ALPHABETIC\n",
            "Encontrado: VEREMOS\n",
            "Encontrado: WINTER'S TALE\n",
            "NOO Encontrado: IZAMAL\n",
            "Encontrado: PAIS DE GALES\n",
            "Encontrado: SILVER DUST\n",
            "NOO Encontrado: ORBAYO\n",
            "NOO Encontrado: ASTURIAS\n",
            "Encontrado: COBRECES\n",
            "NOO Encontrado: ARETHA\n",
            "Encontrado: MONZALVOS\n",
            "Encontrado: TRESEFES\n",
            "Encontrado: NATURAL TALENT\n",
            "Encontrado: CHAM'S PRIDE\n",
            "Encontrado: SINCELEJO\n",
            "NOO Encontrado: LA MAL AMADA\n",
            "Encontrado: TE KOOP\n",
            "Encontrado: SAMEDI RIEN\n",
            "Encontrado: RODABALLO\n",
            "Encontrado: SIMPLY STRIKING\n",
            "Encontrado: RESACON\n",
            "Encontrado: POWERFUL SOLE\n",
            "Encontrado: STAR OF BENGAL\n",
            "Encontrado: BRAVO\n",
            "Encontrado: PRINCE HAMLET\n",
            "Encontrado: BAADIRR\n",
            "Encontrado: WHAT'S UP\n",
            "Encontrado: MR HOBBS\n",
            "Encontrado: DANCE JUPITER\n",
            "Encontrado: GIPSY BLUE EYES\n",
            "Encontrado: CAPOEIRA\n",
            "Encontrado: CORVERA\n",
            "Encontrado: GRAN BELGA\n",
            "Encontrado: NASAB\n",
            "Encontrado: ALABAMA\n",
            "Encontrado: PADERNO\n",
            "Encontrado: TURCO\n",
            "Encontrado: KOWALSKY\n",
            "Encontrado: WAR OF DANCE\n",
            "Encontrado: CANARION\n",
            "Encontrado: SWIFTWAY\n",
            "NOO Encontrado: AMERICANO\n",
            "Encontrado: CASANDRA\n",
            "Encontrado: PENRHEAD\n",
            "Encontrado: URABA\n",
            "Encontrado: KOMOTU\n",
            "Encontrado: FREE JAZZ\n",
            "Encontrado: CONCEPCION LINE\n",
            "NOO Encontrado: WINTON\n",
            "Encontrado: QUISQUILLA\n",
            "Encontrado: RATIONALEXUBERANCE\n",
            "Encontrado: BRAVE TOWN\n",
            "Encontrado: LADY MOON\n",
            "Encontrado: SAFAGA\n",
            "Encontrado: PAMPLONA\n",
            "Encontrado: CLIFFORD\n",
            "Encontrado: WITIZA\n",
            "Encontrado: EL COMANDANTE\n",
            "NOO Encontrado: STARSHADOW\n",
            "Encontrado: LADY CLUNIA\n",
            "Encontrado: BLUE MOON EYES\n",
            "Encontrado: HARVAC\n",
            "Encontrado: TEYMUR\n",
            "Encontrado: NOOZHAH SUREÑA\n",
            "Encontrado: KATALINA\n",
            "Encontrado: PERILLAN\n",
            "Encontrado: WAYNA\n",
            "Encontrado: AURORITA\n",
            "Encontrado: CONSPIRACY\n",
            "Encontrado: BIDUL\n",
            "Encontrado: PRETTY EMERALD\n",
            "Encontrado: ATLANTA\n",
            "Encontrado: LADY MCQUEEN\n",
            "NOO Encontrado: UPSDAWN\n",
            "Encontrado: MARIANCE\n",
            "Encontrado: COLOSAL\n",
            "Encontrado: LINDA\n",
            "Encontrado: PREMIERE RHAPSODIE\n",
            "Encontrado: PANTXINETA\n",
            "Encontrado: ARENAS DE SANPEDRO\n",
            "Encontrado: GOYESCA\n",
            "Encontrado: TRITON\n",
            "Encontrado: CALCAS\n",
            "Encontrado: VEGADEO\n",
            "Encontrado: TINTIN\n",
            "Encontrado: FINTAS\n",
            "Encontrado: PERFECT SUNRISE\n",
            "Encontrado: TUNANTE\n",
            "Encontrado: BORONDON\n",
            "Encontrado: KOHOOF\n",
            "Encontrado: EYE OF HEAVEN\n",
            "Encontrado: CARISMA\n",
            "Encontrado: YESWECAN BARELIERE\n",
            "Encontrado: FARNESIO\n",
            "Encontrado: MAHATMA\n",
            "Encontrado: RED HOT ACTION\n",
            "Encontrado: UMBRA\n",
            "Encontrado: ALERTA ROJA\n",
            "Encontrado: ADHARHA\n",
            "NOO Encontrado: SOGALINDA\n",
            "Encontrado: DABESPIR\n",
            "Encontrado: NATURAL TALENT\n",
            "Encontrado: ROBLE\n",
            "Encontrado: DROVER\n",
            "Encontrado: RUSHDY\n",
            "Encontrado: IZASKUN\n",
            "NOO Encontrado: ATLANTICO\n",
            "Encontrado: JACINTHE BERE\n",
            "Encontrado: SANBLASS\n",
            "NOO Encontrado: PIU BIRCH\n",
            "Encontrado: BELLETTI\n",
            "Encontrado: NAYMA\n",
            "Encontrado: COSSIO\n",
            "Encontrado: IL DIVO\n",
            "Encontrado: MELAMPO\n",
            "NOO Encontrado: UPSILON\n",
            "Encontrado: BERTIE FROM BEAU\n",
            "Encontrado: DISILLUSION\n",
            "Encontrado: QUEEN OF GLORY\n",
            "NOO Encontrado: FORTUNATO\n",
            "Encontrado: QUILLA MOON\n",
            "Encontrado: HELHEIM\n",
            "NOO Encontrado: MAITRE YODA\n",
            "Encontrado: SPANISH COLT\n",
            "Encontrado: DUTCH KIKI\n",
            "Encontrado: MAX'S THUNDER\n",
            "Encontrado: RAMIRIQUI\n",
            "Encontrado: ARENAL\n",
            "Encontrado: BABA KARAM\n",
            "Encontrado: ELAMIRR\n",
            "Encontrado: AARASH\n",
            "Encontrado: CHARMING LOOK\n",
            "Encontrado: COSMIC HORIZON\n",
            "Encontrado: CASILDA\n",
            "Encontrado: DIVA VICTORIA\n",
            "Encontrado: UPA LOLA\n",
            "Encontrado: AUNT AGATHA\n",
            "Encontrado: PORTALMA\n",
            "Encontrado: DAJLA\n",
            "Encontrado: FIJI GOLD\n",
            "Encontrado: TIHANNA\n",
            "Encontrado: GUERREIRA\n",
            "Encontrado: STAR OF BENGAL\n",
            "Encontrado: VETONA\n",
            "Encontrado: RUGBY INDUS\n",
            "Encontrado: SALLAB\n",
            "Encontrado: RESUMPTION\n",
            "Encontrado: YESWECAN BARELIERE\n",
            "Encontrado: ALL IRON\n",
            "Encontrado: KOWALSKY\n",
            "Encontrado: MONCAYO\n",
            "Encontrado: VOLCAN\n",
            "NOO Encontrado: SHELBY\n",
            "Encontrado: CLIFFORD\n",
            "NOO Encontrado: STARSHADOW\n",
            "Encontrado: AKBAHARI\n",
            "NOO Encontrado: ROBLON\n",
            "Encontrado: KOMOTU\n",
            "Encontrado: WAMBA\n",
            "Encontrado: WAYNA\n",
            "Encontrado: HARVAC\n",
            "Encontrado: PEYRASSOL\n",
            "Encontrado: SPANISH RULER\n",
            "Encontrado: ATREVIDA\n",
            "Encontrado: FAMILY TRIP\n",
            "Encontrado: MELODY FRANCE\n",
            "Encontrado: PEKEN\n",
            "Encontrado: LIGHTNINGSUN\n",
            "Encontrado: BATTLE OF WATERLOO\n",
            "Encontrado: KOHOOF\n",
            "Encontrado: HANNA\n",
            "Encontrado: MACHU PICCHU\n",
            "Encontrado: BLACK VOICE\n",
            "Encontrado: IDEAL\n",
            "Encontrado: CONSPIRACY\n",
            "Encontrado: BODAK YELLOW\n",
            "Encontrado: VELETA\n",
            "Encontrado: OCEANIC BLUE\n",
            "Encontrado: GRISOL\n",
            "Encontrado: PIRLO\n",
            "Encontrado: LINDA\n",
            "Encontrado: BIDUL\n",
            "Encontrado: ATLANTA\n",
            "Encontrado: PRETTY EMERALD\n",
            "Encontrado: ALBORAN\n",
            "Encontrado: VALE\n",
            "Encontrado: DIMAX\n",
            "Encontrado: CUPPACOFFEE\n",
            "Encontrado: NEW JACK SWING\n",
            "Encontrado: DERDAS\n",
            "Encontrado: TIERMES\n",
            "Encontrado: BOSTON BRUIN\n",
            "Encontrado: WEERT\n",
            "Encontrado: GALILODGE\n",
            "Encontrado: TAZONES\n",
            "NOO Encontrado: UPSDAWN\n",
            "Encontrado: MARACAY\n",
            "Encontrado: DAYSHANN\n",
            "NOO Encontrado: THE GAME\n",
            "NOO Encontrado: FINELY TUNED\n",
            "Encontrado: GO WITH THE WIND\n",
            "Encontrado: LORDOFTHEHORIZON\n",
            "Encontrado: CHEVALIER CATHARE\n",
            "Encontrado: GAHERIS\n",
            "Encontrado: PHILAU\n",
            "Encontrado: GLORYTOF\n",
            "Encontrado: QATAR RIVER\n",
            "Encontrado: WHITE KING\n",
            "Encontrado: LISICLES\n",
            "NOO Encontrado: AUSTRALIA CAPE\n",
            "Encontrado: SIX RIVERS\n",
            "Encontrado: BEL OUEST\n",
            "NOO Encontrado: SEVERUS\n",
            "NOO Encontrado: HIGHLAND MARKET\n",
            "Encontrado: CHAMALE\n",
            "Encontrado: CAMARINES\n",
            "Encontrado: SPEAK IN COLOURS\n",
            "Encontrado: EL BOSNIA\n",
            "Encontrado: STOWEMAN\n",
            "Encontrado: KENDAYA\n",
            "Encontrado: BRAVO\n",
            "Encontrado: FINTAS\n",
            "Encontrado: CORRECAMINOS\n",
            "Encontrado: TRITON\n",
            "Encontrado: CALCAS\n",
            "Encontrado: MACE WINDU\n",
            "Encontrado: TINTIN\n",
            "Encontrado: KATARA\n",
            "Encontrado: BRIBON\n",
            "Encontrado: ARENAS DE SANPEDRO\n",
            "Encontrado: MAESTRANZA\n",
            "Encontrado: MAHATMA\n",
            "Encontrado: LADY MOON\n",
            "Encontrado: WAR OF DANCE\n",
            "Encontrado: WITIZA\n",
            "Encontrado: PERILLAN\n",
            "Encontrado: URABA\n",
            "Encontrado: COSI COSI\n",
            "NOO Encontrado: FORTUNATO\n",
            "Encontrado: TEYMUR\n",
            "Encontrado: PETRA\n",
            "Encontrado: ZHARINA\n",
            "Encontrado: ARIANE\n",
            "Encontrado: SAFAGA\n",
            "Encontrado: ENORINA\n",
            "Encontrado: QUEEN OF GLORY\n",
            "NOO Encontrado: WALKING TO GLORY\n",
            "NOO Encontrado: LADY RAZALMA\n",
            "Encontrado: ADHARHA\n",
            "NOO Encontrado: CAROLINA WEST\n",
            "NOO Encontrado: SOFUNNY\n",
            "NOO Encontrado: VIKING CITY\n",
            "Encontrado: VINCI LISA\n",
            "Encontrado: NATURAL TALENT\n",
            "Encontrado: HEY JUDE\n",
            "Encontrado: SHE IS FIERCE\n",
            "NOO Encontrado: ARETHA\n",
            "Encontrado: ALPHABETIC\n",
            "NOO Encontrado: PIU BIRCH\n",
            "Encontrado: RODABALLO\n",
            "Encontrado: RESACON\n",
            "Encontrado: GUERREIRA\n",
            "Encontrado: RAIKU\n",
            "Encontrado: SMASH HIT\n",
            "Encontrado: POWERFUL SOLE\n",
            "NOO Encontrado: TRES DE TREBOL\n",
            "NOO Encontrado: EMBAT\n",
            "NOO Encontrado: ORBAYO\n",
            "NOO Encontrado: ASTURIAS\n",
            "Encontrado: COLIBRI\n",
            "NOO Encontrado: MEDICEAN BLUE\n",
            "NOO Encontrado: ASTRAL\n",
            "Encontrado: BELLETTI\n",
            "Encontrado: DISILLUSION\n",
            "Encontrado: CASANDRA\n",
            "Encontrado: BLUE MOON EYES\n",
            "Encontrado: EL COMANDANTE\n",
            "Encontrado: NOOZHAH SUREÑA\n",
            "Encontrado: CONCEPCION LINE\n",
            "Encontrado: MARY JOE\n",
            "Encontrado: KOMOTU\n",
            "Encontrado: MAX MAGICAL\n",
            "Encontrado: WANDA\n",
            "Encontrado: FARNESIO\n",
            "NOO Encontrado: ROBLON\n",
            "Encontrado: PAMPLONA\n",
            "Encontrado: MADERAS\n",
            "Encontrado: SWIFTWAY\n",
            "Encontrado: HELHEIM\n",
            "Encontrado: MOM CHERIE\n",
            "Encontrado: BAILEN\n",
            "Encontrado: WILD HAWK\n",
            "NOO Encontrado: WHITE WINE\n",
            "NOO Encontrado: VITA BARELIERE\n",
            "Encontrado: SPANISH RULER\n",
            "Encontrado: FLAMING GLASS\n",
            "Encontrado: AURORITA\n",
            "Encontrado: LADY CLUNIA\n",
            "Encontrado: SIDNEY\n",
            "Encontrado: MAGIC WARRIOR\n",
            "Encontrado: MAAMUR\n",
            "Encontrado: SOMETHING\n",
            "Encontrado: BRAVO\n",
            "Encontrado: COLOSAL\n",
            "Encontrado: DIVA VICTORIA\n",
            "Encontrado: CASILDA\n",
            "Encontrado: CARISMA\n",
            "Encontrado: AVE MUNDI\n",
            "Encontrado: AZUL\n",
            "Encontrado: VINCENZO\n",
            "Encontrado: EL BOSNIA\n",
            "Encontrado: DERDAS\n",
            "Encontrado: MARIANCE\n",
            "Encontrado: CORRECAMINOS\n",
            "Encontrado: FINTAS\n",
            "Encontrado: BLACK VOICE\n",
            "Encontrado: TUNANTE\n",
            "Encontrado: PREMIERE RHAPSODIE\n",
            "Encontrado: NEW JACK SWING\n",
            "Encontrado: INES\n",
            "Encontrado: SUPER TRIP\n",
            "Encontrado: HARDPIA\n",
            "Encontrado: TARANTELA\n",
            "Encontrado: KENNOCHA\n",
            "Encontrado: FIRST CROWD\n",
            "Encontrado: NAVIA\n",
            "NOO Encontrado: KANE ORE\n",
            "Encontrado: AMIL\n",
            "Encontrado: DAIQUIRYA\n",
            "Encontrado: KOHOOF\n",
            "Encontrado: CONSPIRACY\n",
            "Encontrado: FAMILY TRIP\n",
            "Encontrado: SKIBO CASTLE\n",
            "Encontrado: BORONDON\n",
            "Encontrado: KRYPTON\n",
            "NOO Encontrado: CHUSQUEZ\n",
            "Encontrado: IDEAL\n",
            "Encontrado: BEE IN YOUR BONNET\n",
            "Encontrado: SANSEVERO\n",
            "Encontrado: IDEAL\n",
            "Encontrado: DERDAS\n",
            "Encontrado: BRIAREO\n",
            "Encontrado: MACADAMIA\n",
            "Encontrado: VEGADEO\n",
            "Encontrado: PANTXINETA\n",
            "NOO Encontrado: PIU BIRCH\n",
            "Encontrado: SAORI\n",
            "Encontrado: GRISOL\n",
            "Encontrado: CAMARINES\n",
            "Encontrado: VINCI LISA\n",
            "NOO Encontrado: SOFUNNY\n",
            "Encontrado: CHAM'S DREAM\n",
            "Encontrado: PEDRO EL GRANDE\n",
            "Encontrado: NAYMA\n",
            "Encontrado: IRUÑIA\n",
            "Encontrado: CHAM'S PRIDE\n",
            "Encontrado: MELAMPO\n",
            "Encontrado: DROVER\n",
            "Encontrado: KARLUVY\n",
            "NOO Encontrado: SOGALINDA\n",
            "Encontrado: SHE IS FIERCE\n",
            "Encontrado: AMERICANISM\n",
            "Encontrado: HAVANA MAGIC\n",
            "Encontrado: SINDAWER\n",
            "Encontrado: ROSICRUCIEN\n",
            "Encontrado: ROCHET ROUGE\n",
            "Encontrado: BELADOR\n",
            "Encontrado: NATURAL TALENT\n",
            "Encontrado: RUSHDY\n",
            "Encontrado: BALLET STAR\n",
            "Encontrado: CRIMSON MYSTERY\n",
            "Encontrado: HEADHUNTER\n",
            "NOO Encontrado: ORBAYO\n",
            "Encontrado: AS DE OROS\n",
            "Encontrado: BABA KARAM\n",
            "Encontrado: UROGALLO\n",
            "Encontrado: AS DE OROS\n",
            "Encontrado: MAUNA LOA\n",
            "Encontrado: PREMIERE RHAPSODIE\n",
            "Encontrado: BLACK VOICE\n",
            "Encontrado: CONSPIRACY\n",
            "Encontrado: IDEAL\n",
            "Encontrado: MELODY FRANCE\n",
            "Encontrado: CHAPMAN BILLY\n",
            "Encontrado: LIGHTNINGSUN\n",
            "Encontrado: SKIBO CASTLE\n",
            "Encontrado: PERFECT SUNRISE\n",
            "Encontrado: TRASGU\n",
            "Encontrado: MARIANCE\n",
            "Encontrado: NEW JACK SWING\n",
            "Encontrado: VINCENZO\n",
            "Encontrado: WEERT\n",
            "Encontrado: MELAMPO\n",
            "Encontrado: MACHU PICCHU\n",
            "Encontrado: VIOLETTA\n",
            "Encontrado: GIPSY BLUE EYES\n",
            "NOO Encontrado: FORTUNATO\n",
            "Encontrado: ASTUTO\n",
            "Encontrado: CANARION\n",
            "Encontrado: CASANDRA\n",
            "Encontrado: LADY ANGIE\n",
            "Encontrado: HADDAF\n",
            "Encontrado: VELETA\n",
            "Encontrado: VALE\n",
            "Encontrado: DARK PROFIT\n",
            "Encontrado: CHARMING LOOK\n",
            "Encontrado: BODAK YELLOW\n",
            "Encontrado: BERTIE FROM BEAU\n",
            "Encontrado: JACINTHE BERE\n",
            "Encontrado: CAT HACLYSME\n",
            "Encontrado: FITERO\n",
            "Encontrado: ROSICRUCIEN\n",
            "Encontrado: ALPHABETIC\n",
            "Encontrado: VALL DE RUDA\n",
            "Encontrado: PURITAINE SPRING\n",
            "Encontrado: MACADAMIA\n",
            "Encontrado: FIRST CROWD\n",
            "Encontrado: VEGADEO\n",
            "Encontrado: PEDRO EL GRANDE\n",
            "NOO Encontrado: VIKING CITY\n",
            "NOO Encontrado: PIU BIRCH\n",
            "Encontrado: SASLONG\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        OtherHorse\n",
              "1        OtherHorse\n",
              "2        OtherHorse\n",
              "3        OtherHorse\n",
              "4        OtherHorse\n",
              "5        OtherHorse\n",
              "6        OtherHorse\n",
              "7        OtherHorse\n",
              "8        OtherHorse\n",
              "9        OtherHorse\n",
              "10       OtherHorse\n",
              "11       OtherHorse\n",
              "12       OtherHorse\n",
              "13       OtherHorse\n",
              "14       OtherHorse\n",
              "15       OtherHorse\n",
              "16       OtherHorse\n",
              "17       OtherHorse\n",
              "18       OtherHorse\n",
              "19       OtherHorse\n",
              "20       OtherHorse\n",
              "21       OtherHorse\n",
              "22       OtherHorse\n",
              "23       OtherHorse\n",
              "24       OtherHorse\n",
              "25       OtherHorse\n",
              "26       OtherHorse\n",
              "27       OtherHorse\n",
              "28       OtherHorse\n",
              "29       OtherHorse\n",
              "30       OtherHorse\n",
              "31       OtherHorse\n",
              "32    MONTERREDONDO\n",
              "33       OtherHorse\n",
              "34       OtherHorse\n",
              "35       OtherHorse\n",
              "36       OtherHorse\n",
              "37       OtherHorse\n",
              "38       OtherHorse\n",
              "39       OtherHorse\n",
              "40       OtherHorse\n",
              "41       OtherHorse\n",
              "42       OtherHorse\n",
              "43       OtherHorse\n",
              "44       OtherHorse\n",
              "45       OtherHorse\n",
              "46       OtherHorse\n",
              "47       OtherHorse\n",
              "48        SOGALINDA\n",
              "49       OtherHorse\n",
              "50       OtherHorse\n",
              "51       OtherHorse\n",
              "52        ATLANTICO\n",
              "53       OtherHorse\n",
              "54       OtherHorse\n",
              "55     LA MAL AMADA\n",
              "56       OtherHorse\n",
              "57       OtherHorse\n",
              "58       OtherHorse\n",
              "59       OtherHorse\n",
              "60       OtherHorse\n",
              "61       OtherHorse\n",
              "62        PIU BIRCH\n",
              "63       OtherHorse\n",
              "64       OtherHorse\n",
              "Name: NombreCaballo, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listAparicionesOwner = changeNotFoundInListOwner(listTrainOwner, listTestOwner)\n",
        "train['Propietario'].head(15)"
      ],
      "metadata": {
        "id": "ygJDBeg4rANZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6924bc-7cde-473f-dc24-96c177b8b199"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     OtherOwner\n",
              "1     OtherOwner\n",
              "2     OtherOwner\n",
              "3     OtherOwner\n",
              "4     OtherOwner\n",
              "5     OtherOwner\n",
              "6     OtherOwner\n",
              "7     OtherOwner\n",
              "8     OtherOwner\n",
              "9     OtherOwner\n",
              "10    OtherOwner\n",
              "11           4 C\n",
              "12    OtherOwner\n",
              "13    OtherOwner\n",
              "14    OtherOwner\n",
              "Name: Propietario, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listAparicionesTrainer = changeNotFoundInListTrainer(listTrainTrainer, listTestTrainer)\n",
        "train['Preparador'].head(15)"
      ],
      "metadata": {
        "id": "iYbdAgbhq_VD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f53486-85ee-4504-ed96-b0e6bf035cbf"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      OtherTrainer\n",
              "1     G.ARIZKORRETA\n",
              "2        J.M.OSORIO\n",
              "3            B.RAMA\n",
              "4      OtherTrainer\n",
              "5            A.SOTO\n",
              "6           J.LOPEZ\n",
              "7           J.LOPEZ\n",
              "8           J.LOPEZ\n",
              "9     J.A.RODRIGUEZ\n",
              "10     OtherTrainer\n",
              "11    J.A.RODRIGUEZ\n",
              "12    G.ARIZKORRETA\n",
              "13       M&M RACING\n",
              "14     OtherTrainer\n",
              "Name: Preparador, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listAparicionesJockey = changeNotFoundInListJockey(listTrainJockey, listTestJockey)\n",
        "train['Jinete'].head(15)"
      ],
      "metadata": {
        "id": "-OiG4a9VrFqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e41d17f-2457-4c2b-81d8-d7271156fea4"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       OtherJockey\n",
              "1        V. JANACEK\n",
              "2        J.GELABERT\n",
              "3          B. FAYOS\n",
              "4         R.N.VALLE\n",
              "5          C. CADEL\n",
              "6        J.GELABERT\n",
              "7           C.PEREZ\n",
              "8          B. FAYOS\n",
              "9         R.N.VALLE\n",
              "10       V. JANACEK\n",
              "11     N. DE JULIAN\n",
              "12    J.L. MARTINEZ\n",
              "13      V.ALONSO V.\n",
              "14       V. JANACEK\n",
              "Name: Jinete, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['NombreCaballo'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuL1CSksnpek",
        "outputId": "6c7b644c-cb6d-4c0d-937c-b67338e932b0"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OtherHorse          665\n",
              "PIU BIRCH             6\n",
              "FORTUNATO             4\n",
              "ASTURIAS              4\n",
              "ORBAYO                4\n",
              "SOGALINDA             4\n",
              "VIKING CITY           3\n",
              "UPSDAWN               3\n",
              "SOFUNNY               3\n",
              "ROBLON                3\n",
              "ATLANTICO             3\n",
              "ASTRAL                3\n",
              "FINELY TUNED          3\n",
              "STARSHADOW            3\n",
              "TRES DE TREBOL        2\n",
              "WALKING TO GLORY      2\n",
              "LA MAL AMADA          2\n",
              "LADY RAZALMA          2\n",
              "SHELBY                2\n",
              "WINTON                2\n",
              "UPSILON               2\n",
              "MAITRE YODA           2\n",
              "ARETHA                2\n",
              "HIGHLAND MARKET       2\n",
              "CAROLINA WEST         2\n",
              "SEVERUS               2\n",
              "EMBAT                 2\n",
              "THE GAME              2\n",
              "KANE ORE              2\n",
              "MONTERREDONDO         1\n",
              "VITA BARELIERE        1\n",
              "WHITE WINE            1\n",
              "MEDICEAN BLUE         1\n",
              "AUSTRALIA CAPE        1\n",
              "AMERICANO             1\n",
              "Name: NombreCaballo, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm1c9qJnu4-u"
      },
      "source": [
        "## **Tratamiento (Problemas)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUInjzYlvls2",
        "outputId": "52af4377-41e3-4fae-a28d-5901bee8362c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-8           126\n",
              "-3            55\n",
              "(5)(8)        34\n",
              "(3)(8)        31\n",
              "-5            31\n",
              "8             27\n",
              "(4)(8)        22\n",
              "-4            19\n",
              "3             12\n",
              "-2             9\n",
              "(2)(5)(8)      8\n",
              "(2)(4)(8)      6\n",
              "5              5\n",
              "(2)(3)(8)      4\n",
              "(4)(6)         4\n",
              "(2)(8)         3\n",
              "-6             3\n",
              "(2)(4)         2\n",
              "(2)(5)         2\n",
              "4              2\n",
              "(4)(5)(8)      2\n",
              "(4)(5)         2\n",
              "(2)(3)         1\n",
              "(3)(8)(2)      1\n",
              "(3)(4)         1\n",
              "(6)(8)         1\n",
              "(3)(6)         1\n",
              "Name: Problemas, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ],
      "source": [
        "train['Problemas'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gC-9eodqu6bg",
        "outputId": "f704735c-6064-42be-c2ed-20088e972043"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Puesto NombreCaballo Peso  Edad  Mantilla Propietario     Preparador  \\\n",
              "0       0    OtherHorse   57     2         6  OtherOwner   OtherTrainer   \n",
              "1       1    OtherHorse   57     2         5  OtherOwner  G.ARIZKORRETA   \n",
              "2       1    OtherHorse   57     2         3  OtherOwner     J.M.OSORIO   \n",
              "3       2    OtherHorse   57     2         4  OtherOwner         B.RAMA   \n",
              "4       2    OtherHorse   57     2         2  OtherOwner   OtherTrainer   \n",
              "\n",
              "        Jinete Problemas UltimasActuaciones                    Fecha   Hora  \\\n",
              "0  OtherJockey         0         [02,04,01]  13 de noviembre de 2022  11:45   \n",
              "1   V. JANACEK         0            [04,06]  13 de noviembre de 2022  11:45   \n",
              "2   J.GELABERT         0               [01]  13 de noviembre de 2022  11:45   \n",
              "3     B. FAYOS        -8            [08,15]  13 de noviembre de 2022  11:45   \n",
              "4    R.N.VALLE        -8            [09,11]  13 de noviembre de 2022  11:45   \n",
              "\n",
              "      Terreno  Distancia  Tipo Categoría  SentidoHipodromo  \\\n",
              "0  H - Blando       1500  Liso         C                 0   \n",
              "1  H - Blando       1500  Liso         C                 0   \n",
              "2  H - Blando       1500  Liso         C                 0   \n",
              "3  H - Blando       1500  Liso         C                 0   \n",
              "4  H - Blando       1500  Liso         C                 0   \n",
              "\n",
              "           Meteorología  LLuvia  Viento  Temperatura    Hipodromo   FechaAux  \\\n",
              "0  Parcialmente nublado     0.0       6         15.0  La Zarzuela 2022-11-13   \n",
              "1  Parcialmente nublado     0.0       6         15.0  La Zarzuela 2022-11-13   \n",
              "2  Parcialmente nublado     0.0       6         15.0  La Zarzuela 2022-11-13   \n",
              "3  Parcialmente nublado     0.0       6         15.0  La Zarzuela 2022-11-13   \n",
              "4  Parcialmente nublado     0.0       6         15.0  La Zarzuela 2022-11-13   \n",
              "\n",
              "     year  month   day  Otoño  DiasDesdeCarrera  DaysSincePreviousRace  \\\n",
              "0  2022.0   11.0  13.0      0              37.0                   35.0   \n",
              "1  2022.0   11.0  13.0      0              37.0                   45.0   \n",
              "2  2022.0   11.0  13.0      0              37.0                   14.0   \n",
              "3  2022.0   11.0  13.0      0              37.0                   35.0   \n",
              "4  2022.0   11.0  13.0      0              37.0                   45.0   \n",
              "\n",
              "   Contrincantes  DestrezaDistancia  Problema_Nulo  Problema_1  Problema_2  \\\n",
              "0            6.0                0.0              1           0           0   \n",
              "1            6.0                0.0              1           0           0   \n",
              "2            6.0                0.0              1           0           0   \n",
              "3            6.0                0.0              0           0           0   \n",
              "4            6.0                0.0              0           0           0   \n",
              "\n",
              "   Problema_3  Problema_4  Problema_5  Problema_6  Problema_7  Problema_8  \n",
              "0           0           0           0           0           0           0  \n",
              "1           0           0           0           0           0           0  \n",
              "2           0           0           0           0           0           0  \n",
              "3           0           0           0           0           0           1  \n",
              "4           0           0           0           0           0           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc4c1e1d-7766-4e36-bd20-4d0269c7f1a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Puesto</th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "      <th>DaysSincePreviousRace</th>\n",
              "      <th>Contrincantes</th>\n",
              "      <th>DestrezaDistancia</th>\n",
              "      <th>Problema_Nulo</th>\n",
              "      <th>Problema_1</th>\n",
              "      <th>Problema_2</th>\n",
              "      <th>Problema_3</th>\n",
              "      <th>Problema_4</th>\n",
              "      <th>Problema_5</th>\n",
              "      <th>Problema_6</th>\n",
              "      <th>Problema_7</th>\n",
              "      <th>Problema_8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>OtherTrainer</td>\n",
              "      <td>OtherJockey</td>\n",
              "      <td>0</td>\n",
              "      <td>[02,04,01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>0</td>\n",
              "      <td>[04,06]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>0</td>\n",
              "      <td>[01]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[08,15]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>OtherTrainer</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>-8</td>\n",
              "      <td>[09,11]</td>\n",
              "      <td>13 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1500</td>\n",
              "      <td>Liso</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>2022-11-13</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc4c1e1d-7766-4e36-bd20-4d0269c7f1a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc4c1e1d-7766-4e36-bd20-4d0269c7f1a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc4c1e1d-7766-4e36-bd20-4d0269c7f1a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "train['Problemas'] = train['Problemas'].fillna('0')\n",
        "train['Problema_Nulo'] = np.where(train['Problemas'].str.contains('0'),1,0)\n",
        "train['Problema_1'] = np.where(train['Problemas'].str.contains('1'),1,0)\n",
        "train['Problema_2'] = np.where(train['Problemas'].str.contains('2'),1,0)\n",
        "train['Problema_3'] = np.where(train['Problemas'].str.contains('3'),1,0)\n",
        "train['Problema_4'] = np.where(train['Problemas'].str.contains('4'),1,0)\n",
        "train['Problema_5'] = np.where(train['Problemas'].str.contains('5'),1,0)\n",
        "train['Problema_6'] = np.where(train['Problemas'].str.contains('6'),1,0)\n",
        "train['Problema_7'] = np.where(train['Problemas'].str.contains('7'),1,0)\n",
        "train['Problema_8'] = np.where(train['Problemas'].str.contains('8'),1,0)\n",
        "train.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaTopoV10syt"
      },
      "source": [
        "## **Tratamiento (UltimasActuaciones)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn1Y7u_YRDx0",
        "outputId": "81e02946-3be6-4b4b-c361-b59517b87024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "******INDEX  2 *****\n",
            "Sum --> 8.740320469781409 /Len --> 6\n",
            "Result --> 1.4567200782969014 /Element --> 020401\n",
            "1\n",
            "1\n",
            "******INDEX  3 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0406\n",
            "2\n",
            "******INDEX  4 *****\n",
            "Sum --> 1 /Len --> 2\n",
            "Result --> 0.5 /Element --> 01\n",
            "3\n",
            "3\n",
            "******INDEX  5 *****\n",
            "Sum --> 23 /Len --> 4\n",
            "Result --> 5.75 /Element --> 0815\n",
            "4\n",
            "4\n",
            "******INDEX  6 *****\n",
            "Sum --> 20 /Len --> 4\n",
            "Result --> 5.0 /Element --> 0911\n",
            "5\n",
            "5\n",
            "******INDEX  7 *****\n",
            "Sum --> 12 /Len --> 4\n",
            "Result --> 3.0 /Element --> 0705\n",
            "6\n",
            "6\n",
            "******INDEX  8 *****\n",
            "Sum --> 9 /Len --> 4\n",
            "Result --> 2.25 /Element --> 0108\n",
            "7\n",
            "******INDEX  9 *****\n",
            "Sum --> 2 /Len --> 2\n",
            "Result --> 1.0 /Element --> 02\n",
            "8\n",
            "******INDEX  10 *****\n",
            "Sum --> 17.38131462655539 /Len --> 6\n",
            "Result --> 2.896885771092565 /Element --> 050404\n",
            "9\n",
            "9\n",
            "9\n",
            "******INDEX  11 *****\n",
            "Sum --> 44.28258458066108 /Len --> 10\n",
            "Result --> 4.428258458066108 /Element --> 0714050102\n",
            "10\n",
            "10\n",
            "******INDEX  12 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0307\n",
            "11\n",
            "11\n",
            "11\n",
            "******INDEX  13 *****\n",
            "Sum --> 29.00193129076989 /Len --> 10\n",
            "Result --> 2.900193129076989 /Element --> 0505090302\n",
            "12\n",
            "12\n",
            "******INDEX  14 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0406\n",
            "13\n",
            "******INDEX  15 *****\n",
            "Sum --> 12 /Len --> 2\n",
            "Result --> 6.0 /Element --> 12\n",
            "14\n",
            "14\n",
            "******INDEX  16 *****\n",
            "Sum --> 6 /Len --> 4\n",
            "Result --> 1.5 /Element --> 0303\n",
            "15\n",
            "******INDEX  17 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "16\n",
            "******INDEX  18 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "17\n",
            "******INDEX  19 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "18\n",
            "******INDEX  20 *****\n",
            "Sum --> 15.449199154039306 /Len --> 6\n",
            "Result --> 2.5748665256732175 /Element --> 030406\n",
            "19\n",
            "******INDEX  21 *****\n",
            "Sum --> 10 /Len --> 2\n",
            "Result --> 5.0 /Element --> 10\n",
            "20\n",
            "20\n",
            "******INDEX  22 *****\n",
            "Sum --> 14 /Len --> 4\n",
            "Result --> 3.5 /Element --> 0806\n",
            "21\n",
            "******INDEX  23 *****\n",
            "Sum --> 13 /Len --> 2\n",
            "Result --> 6.5 /Element --> 13\n",
            "22\n",
            "******INDEX  24 *****\n",
            "Sum --> 16 /Len --> 2\n",
            "Result --> 8.0 /Element --> 16\n",
            "23\n",
            "******INDEX  25 *****\n",
            "Sum --> 15 /Len --> 2\n",
            "Result --> 7.5 /Element --> 15\n",
            "24\n",
            "24\n",
            "24\n",
            "******INDEX  26 *****\n",
            "Sum --> 26.548650799549808 /Len --> 10\n",
            "Result --> 2.654865079954981 /Element --> 0604060302\n",
            "25\n",
            "25\n",
            "25\n",
            "******INDEX  27 *****\n",
            "Sum --> 18.73719281884655 /Len --> 10\n",
            "Result --> 1.873719281884655 /Element --> 0103010805\n",
            "26\n",
            "26\n",
            "26\n",
            "******INDEX  28 *****\n",
            "Sum --> 43.524370936469055 /Len --> 10\n",
            "Result --> 4.3524370936469055 /Element --> 0908090104\n",
            "27\n",
            "27\n",
            "27\n",
            "******INDEX  29 *****\n",
            "Sum --> 29.665720683435474 /Len --> 10\n",
            "Result --> 2.9665720683435475 /Element --> 0803030305\n",
            "28\n",
            "28\n",
            "28\n",
            "******INDEX  30 *****\n",
            "Sum --> 15.800059084888144 /Len --> 10\n",
            "Result --> 1.5800059084888143 /Element --> 0403010401\n",
            "29\n",
            "29\n",
            "29\n",
            "******INDEX  31 *****\n",
            "Sum --> 19.840475802310365 /Len --> 10\n",
            "Result --> 1.9840475802310364 /Element --> 0503010502\n",
            "30\n",
            "30\n",
            "30\n",
            "******INDEX  32 *****\n",
            "Sum --> 30.856433643089765 /Len --> 10\n",
            "Result --> 3.0856433643089765 /Element --> 0606050106\n",
            "31\n",
            "31\n",
            "31\n",
            "******INDEX  33 *****\n",
            "Sum --> 20.062866266041592 /Len --> 10\n",
            "Result --> 2.006286626604159 /Element --> 0401040504\n",
            "32\n",
            "32\n",
            "32\n",
            "******INDEX  34 *****\n",
            "Sum --> 29.82756138073856 /Len --> 10\n",
            "Result --> 2.9827561380738556 /Element --> 0704050601\n",
            "33\n",
            "33\n",
            "33\n",
            "******INDEX  35 *****\n",
            "Sum --> 29.69603511437994 /Len --> 10\n",
            "Result --> 2.969603511437994 /Element --> 0902080101\n",
            "34\n",
            "34\n",
            "34\n",
            "******INDEX  36 *****\n",
            "Sum --> 18.468564220941797 /Len --> 10\n",
            "Result --> 1.8468564220941797 /Element --> 0302030108\n",
            "35\n",
            "35\n",
            "35\n",
            "******INDEX  37 *****\n",
            "Sum --> 18.103282983463814 /Len --> 10\n",
            "Result --> 1.8103282983463813 /Element --> 0501020601\n",
            "36\n",
            "36\n",
            "36\n",
            "******INDEX  38 *****\n",
            "Sum --> 15.462288826689832 /Len --> 10\n",
            "Result --> 1.5462288826689832 /Element --> 0201060501\n",
            "37\n",
            "37\n",
            "37\n",
            "******INDEX  39 *****\n",
            "Sum --> 20.278031643091577 /Len --> 10\n",
            "Result --> 2.027803164309158 /Element --> 0104010310\n",
            "38\n",
            "38\n",
            "38\n",
            "******INDEX  40 *****\n",
            "Sum --> 17.360262976035663 /Len --> 10\n",
            "Result --> 1.7360262976035663 /Element --> 0402050202\n",
            "39\n",
            "39\n",
            "39\n",
            "******INDEX  41 *****\n",
            "Sum --> 9.462288826689832 /Len --> 10\n",
            "Result --> 0.9462288826689832 /Element --> 0201030102\n",
            "40\n",
            "40\n",
            "40\n",
            "******INDEX  42 *****\n",
            "Sum --> 29.296900043030913 /Len --> 10\n",
            "Result --> 2.929690004303091 /Element --> 0308050701\n",
            "41\n",
            "41\n",
            "41\n",
            "******INDEX  43 *****\n",
            "Sum --> 21.800059084888144 /Len --> 10\n",
            "Result --> 2.1800059084888144 /Element --> 0403050205\n",
            "42\n",
            "42\n",
            "42\n",
            "******INDEX  44 *****\n",
            "Sum --> 21.73719281884655 /Len --> 10\n",
            "Result --> 2.1737192818846554 /Element --> 0103050804\n",
            "43\n",
            "43\n",
            "43\n",
            "******INDEX  45 *****\n",
            "Sum --> 18.449199154039306 /Len --> 10\n",
            "Result --> 1.8449199154039306 /Element --> 0304030402\n",
            "44\n",
            "44\n",
            "44\n",
            "******INDEX  46 *****\n",
            "Sum --> 20.689097470095348 /Len --> 10\n",
            "Result --> 2.068909747009535 /Element --> 0506020101\n",
            "45\n",
            "45\n",
            "45\n",
            "******INDEX  47 *****\n",
            "Sum --> 14.278031643091577 /Len --> 10\n",
            "Result --> 1.4278031643091578 /Element --> 0104040202\n",
            "46\n",
            "46\n",
            "46\n",
            "******INDEX  48 *****\n",
            "Sum --> 24.296900043030913 /Len --> 10\n",
            "Result --> 2.429690004303091 /Element --> 0308050102\n",
            "47\n",
            "47\n",
            "47\n",
            "******INDEX  49 *****\n",
            "Sum --> 34.02947643127983 /Len --> 10\n",
            "Result --> 3.402947643127983 /Element --> 0409030704\n",
            "48\n",
            "48\n",
            "48\n",
            "******INDEX  50 *****\n",
            "Sum --> 18.17116751094773 /Len --> 10\n",
            "Result --> 1.817116751094773 /Element --> 0301050206\n",
            "49\n",
            "49\n",
            "49\n",
            "******INDEX  51 *****\n",
            "Sum --> 25.00193129076989 /Len --> 10\n",
            "Result --> 2.500193129076989 /Element --> 0505030403\n",
            "50\n",
            "50\n",
            "50\n",
            "******INDEX  52 *****\n",
            "Sum --> 55.41496332421684 /Len --> 10\n",
            "Result --> 5.541496332421684 /Element --> 1208060804\n",
            "51\n",
            "51\n",
            "51\n",
            "******INDEX  53 *****\n",
            "Sum --> 35.96151457334767 /Len --> 10\n",
            "Result --> 3.5961514573347673 /Element --> 0405091004\n",
            "52\n",
            "52\n",
            "52\n",
            "******INDEX  54 *****\n",
            "Sum --> 31.296900043030913 /Len --> 10\n",
            "Result --> 3.1296900043030913 /Element --> 0308070503\n",
            "53\n",
            "53\n",
            "53\n",
            "******INDEX  55 *****\n",
            "Sum --> 30.448178044953053 /Len --> 10\n",
            "Result --> 3.044817804495305 /Element --> 0705040403\n",
            "54\n",
            "54\n",
            "54\n",
            "******INDEX  56 *****\n",
            "Sum --> 53.0287855364237 /Len --> 10\n",
            "Result --> 5.30287855364237 /Element --> 0316020415\n",
            "55\n",
            "55\n",
            "55\n",
            "******INDEX  57 *****\n",
            "Sum --> 42.68981596853535 /Len --> 10\n",
            "Result --> 4.268981596853535 /Element --> 1003090505\n",
            "56\n",
            "56\n",
            "56\n",
            "******INDEX  58 *****\n",
            "Sum --> 68.64831446411208 /Len --> 10\n",
            "Result --> 6.864831446411207 /Element --> 1306090815\n",
            "57\n",
            "57\n",
            "57\n",
            "******INDEX  59 *****\n",
            "Sum --> 26.82756138073856 /Len --> 10\n",
            "Result --> 2.682756138073856 /Element --> 0704050301\n",
            "58\n",
            "58\n",
            "58\n",
            "******INDEX  60 *****\n",
            "Sum --> 38.53843763632033 /Len --> 10\n",
            "Result --> 3.853843763632033 /Element --> 1006050401\n",
            "59\n",
            "59\n",
            "59\n",
            "******INDEX  61 *****\n",
            "Sum --> 26.062866266041592 /Len --> 10\n",
            "Result --> 2.606286626604159 /Element --> 0401100405\n",
            "60\n",
            "60\n",
            "60\n",
            "******INDEX  62 *****\n",
            "Sum --> 38.548650799549804 /Len --> 10\n",
            "Result --> 3.8548650799549806 /Element --> 0604090113\n",
            "61\n",
            "61\n",
            "61\n",
            "******INDEX  63 *****\n",
            "Sum --> 29.689097470095348 /Len --> 10\n",
            "Result --> 2.9689097470095347 /Element --> 0506020407\n",
            "62\n",
            "62\n",
            "62\n",
            "******INDEX  64 *****\n",
            "Sum --> 55.16797533260983 /Len --> 10\n",
            "Result --> 5.516797533260982 /Element --> 0911100406\n",
            "63\n",
            "63\n",
            "63\n",
            "******INDEX  65 *****\n",
            "Sum --> 46.67526226973017 /Len --> 10\n",
            "Result --> 4.667526226973017 /Element --> 0708041305\n",
            "64\n",
            "64\n",
            "64\n",
            "******INDEX  66 *****\n",
            "Sum --> 52.41496332421684 /Len --> 10\n",
            "Result --> 5.241496332421685 /Element --> 1208040605\n",
            "65\n",
            "65\n",
            "65\n",
            "******INDEX  67 *****\n",
            "Sum --> 52.054260396672106 /Len --> 10\n",
            "Result --> 5.205426039667211 /Element --> 0808090610\n",
            "66\n",
            "******INDEX  68 *****\n",
            "Sum --> 16.5680158664523 /Len --> 6\n",
            "Result --> 2.76133597774205 /Element --> 060204\n",
            "67\n",
            "******INDEX  69 *****\n",
            "Sum --> 23.169267463764307 /Len --> 6\n",
            "Result --> 3.861544577294051 /Element --> 060506\n",
            "68\n",
            "68\n",
            "******INDEX  70 *****\n",
            "Sum --> 12.171167510947729 /Len --> 8\n",
            "Result --> 1.5213959388684661 /Element --> 03010304\n",
            "69\n",
            "******INDEX  71 *****\n",
            "Sum --> 18.846926447641053 /Len --> 6\n",
            "Result --> 3.1411544079401756 /Element --> 070204\n",
            "70\n",
            "70\n",
            "70\n",
            "******INDEX  72 *****\n",
            "Sum --> 33.06989314870205 /Len --> 10\n",
            "Result --> 3.306989314870205 /Element --> 0509030206\n",
            "71\n",
            "******INDEX  73 *****\n",
            "Sum --> 49.66623263004133 /Len --> 6\n",
            "Result --> 8.277705438340222 /Element --> 101308\n",
            "72\n",
            "******INDEX  74 *****\n",
            "Sum --> 30.879941868808846 /Len --> 6\n",
            "Result --> 5.146656978134808 /Element --> 070708\n",
            "73\n",
            "73\n",
            "******INDEX  75 *****\n",
            "Sum --> 44.18787909943973 /Len --> 8\n",
            "Result --> 5.523484887429967 /Element --> 12050606\n",
            "74\n",
            "74\n",
            "74\n",
            "******INDEX  76 *****\n",
            "Sum --> 16.360262976035663 /Len --> 10\n",
            "Result --> 1.6360262976035664 /Element --> 0402040202\n",
            "75\n",
            "75\n",
            "75\n",
            "******INDEX  77 *****\n",
            "Sum --> 27.38131462655539 /Len --> 10\n",
            "Result --> 2.738131462655539 /Element --> 0504030506\n",
            "76\n",
            "76\n",
            "76\n",
            "******INDEX  78 *****\n",
            "Sum --> 21.199481645536384 /Len --> 10\n",
            "Result --> 2.119948164553638 /Element --> 0203070602\n",
            "77\n",
            "77\n",
            "77\n",
            "******INDEX  79 *****\n",
            "Sum --> 36.9844528910174 /Len --> 10\n",
            "Result --> 3.69844528910174 /Element --> 0906060203\n",
            "78\n",
            "78\n",
            "78\n",
            "******INDEX  80 *****\n",
            "Sum --> 24.393278397203456 /Len --> 10\n",
            "Result --> 2.4393278397203457 /Element --> 0407010403\n",
            "79\n",
            "79\n",
            "79\n",
            "******INDEX  81 *****\n",
            "Sum --> 27.007811975304783 /Len --> 10\n",
            "Result --> 2.7007811975304783 /Element --> 0603040801\n",
            "80\n",
            "80\n",
            "******INDEX  82 *****\n",
            "Sum --> 7 /Len --> 4\n",
            "Result --> 1.75 /Element --> 0403\n",
            "81\n",
            "81\n",
            "81\n",
            "******INDEX  83 *****\n",
            "Sum --> 26.54952973764698 /Len --> 10\n",
            "Result --> 2.654952973764698 /Element --> 0701050503\n",
            "82\n",
            "82\n",
            "82\n",
            "******INDEX  84 *****\n",
            "Sum --> 16.840475802310365 /Len --> 10\n",
            "Result --> 1.6840475802310366 /Element --> 0503020102\n",
            "83\n",
            "83\n",
            "83\n",
            "******INDEX  85 *****\n",
            "Sum --> 33.82717617189499 /Len --> 10\n",
            "Result --> 3.382717617189499 /Element --> 0805050205\n",
            "84\n",
            "84\n",
            "84\n",
            "******INDEX  86 *****\n",
            "Sum --> 30.34089790913317 /Len --> 10\n",
            "Result --> 3.034089790913317 /Element --> 0404080308\n",
            "85\n",
            "85\n",
            "85\n",
            "******INDEX  87 *****\n",
            "Sum --> 43.80155507429993 /Len --> 10\n",
            "Result --> 4.3801555074299925 /Element --> 1010010106\n",
            "86\n",
            "86\n",
            "86\n",
            "******INDEX  88 *****\n",
            "Sum --> 11.462288826689832 /Len --> 10\n",
            "Result --> 1.1462288826689833 /Element --> 0201010106\n",
            "87\n",
            "87\n",
            "87\n",
            "******INDEX  89 *****\n",
            "Sum --> 37.77745978920005 /Len --> 10\n",
            "Result --> 3.777745978920005 /Element --> 0810040201\n",
            "88\n",
            "88\n",
            "88\n",
            "******INDEX  90 *****\n",
            "Sum --> 10.468564220941797 /Len --> 10\n",
            "Result --> 1.0468564220941796 /Element --> 0302010102\n",
            "89\n",
            "89\n",
            "89\n",
            "******INDEX  91 *****\n",
            "Sum --> 31.952214908074946 /Len --> 10\n",
            "Result --> 3.1952214908074947 /Element --> 0510040301\n",
            "90\n",
            "90\n",
            "90\n",
            "******INDEX  92 *****\n",
            "Sum --> 19.468564220941797 /Len --> 10\n",
            "Result --> 1.9468564220941798 /Element --> 0302030406\n",
            "91\n",
            "91\n",
            "91\n",
            "******INDEX  93 *****\n",
            "Sum --> 25.360937133995904 /Len --> 10\n",
            "Result --> 2.53609371339959 /Element --> 0205060307\n",
            "92\n",
            "92\n",
            "92\n",
            "******INDEX  94 *****\n",
            "Sum --> 44.816892463816345 /Len --> 10\n",
            "Result --> 4.4816892463816345 /Element --> 0513060108\n",
            "93\n",
            "93\n",
            "93\n",
            "******INDEX  95 *****\n",
            "Sum --> 30.448178044953053 /Len --> 10\n",
            "Result --> 3.044817804495305 /Element --> 0705040304\n",
            "94\n",
            "94\n",
            "94\n",
            "******INDEX  96 *****\n",
            "Sum --> 48.16797533260983 /Len --> 10\n",
            "Result --> 4.816797533260983 /Element --> 0911040108\n",
            "95\n",
            "95\n",
            "95\n",
            "******INDEX  97 *****\n",
            "Sum --> 35.28672255649353 /Len --> 10\n",
            "Result --> 3.528672255649353 /Element --> 0703090307\n",
            "96\n",
            "96\n",
            "96\n",
            "******INDEX  98 *****\n",
            "Sum --> 24.103282983463814 /Len --> 10\n",
            "Result --> 2.410328298346381 /Element --> 0501060702\n",
            "97\n",
            "97\n",
            "97\n",
            "******INDEX  99 *****\n",
            "Sum --> 32.92852786458892 /Len --> 10\n",
            "Result --> 3.292852786458892 /Element --> 0801070802\n",
            "98\n",
            "98\n",
            "98\n",
            "******INDEX  100 *****\n",
            "Sum --> 10.171167510947729 /Len --> 10\n",
            "Result --> 1.017116751094773 /Element --> 0301010202\n",
            "99\n",
            "99\n",
            "99\n",
            "******INDEX  101 *****\n",
            "Sum --> 34.840475802310365 /Len --> 10\n",
            "Result --> 3.4840475802310364 /Element --> 0503081104\n",
            "100\n",
            "100\n",
            "100\n",
            "******INDEX  102 *****\n",
            "Sum --> 25.278031643091577 /Len --> 10\n",
            "Result --> 2.527803164309158 /Element --> 0104050608\n",
            "101\n",
            "101\n",
            "101\n",
            "******INDEX  103 *****\n",
            "Sum --> 30.827176171894994 /Len --> 10\n",
            "Result --> 3.082717617189499 /Element --> 0805070101\n",
            "102\n",
            "102\n",
            "102\n",
            "******INDEX  104 *****\n",
            "Sum --> 18.278031643091577 /Len --> 10\n",
            "Result --> 1.8278031643091577 /Element --> 0104050403\n",
            "103\n",
            "103\n",
            "103\n",
            "******INDEX  105 *****\n",
            "Sum --> 27.74032046978141 /Len --> 10\n",
            "Result --> 2.774032046978141 /Element --> 0204031205\n",
            "104\n",
            "104\n",
            "104\n",
            "******INDEX  106 *****\n",
            "Sum --> 7.0 /Len --> 10\n",
            "Result --> 0.7 /Element --> 0101010301\n",
            "105\n",
            "105\n",
            "105\n",
            "******INDEX  107 *****\n",
            "Sum --> 17.062866266041592 /Len --> 10\n",
            "Result --> 1.7062866266041592 /Element --> 0401030205\n",
            "106\n",
            "106\n",
            "106\n",
            "******INDEX  108 *****\n",
            "Sum --> 25.360937133995904 /Len --> 10\n",
            "Result --> 2.53609371339959 /Element --> 0205010807\n",
            "107\n",
            "107\n",
            "107\n",
            "******INDEX  109 *****\n",
            "Sum --> 20.588021358773016 /Len --> 10\n",
            "Result --> 2.0588021358773014 /Element --> 0208010104\n",
            "108\n",
            "108\n",
            "108\n",
            "******INDEX  110 *****\n",
            "Sum --> 28.29728671169194 /Len --> 10\n",
            "Result --> 2.829728671169194 /Element --> 0905010201\n",
            "109\n",
            "109\n",
            "109\n",
            "******INDEX  111 *****\n",
            "Sum --> 10.468564220941797 /Len --> 10\n",
            "Result --> 1.0468564220941796 /Element --> 0302010102\n",
            "110\n",
            "110\n",
            "110\n",
            "******INDEX  112 *****\n",
            "Sum --> 30.311220751300965 /Len --> 10\n",
            "Result --> 3.0311220751300967 /Element --> 0210020208\n",
            "111\n",
            "111\n",
            "111\n",
            "******INDEX  113 *****\n",
            "Sum --> 15.0 /Len --> 10\n",
            "Result --> 1.5 /Element --> 0101010606\n",
            "112\n",
            "112\n",
            "112\n",
            "******INDEX  114 *****\n",
            "Sum --> 15.360937133995906 /Len --> 10\n",
            "Result --> 1.5360937133995907 /Element --> 0205030102\n",
            "113\n",
            "113\n",
            "113\n",
            "******INDEX  115 *****\n",
            "Sum --> 18.17116751094773 /Len --> 10\n",
            "Result --> 1.817116751094773 /Element --> 0301080401\n",
            "114\n",
            "114\n",
            "114\n",
            "******INDEX  116 *****\n",
            "Sum --> 36.054260396672106 /Len --> 10\n",
            "Result --> 3.6054260396672104 /Element --> 0808030303\n",
            "115\n",
            "115\n",
            "115\n",
            "******INDEX  117 *****\n",
            "Sum --> 39.82756138073856 /Len --> 10\n",
            "Result --> 3.9827561380738556 /Element --> 0704091003\n",
            "116\n",
            "116\n",
            "116\n",
            "******INDEX  118 *****\n",
            "Sum --> 23.40067969345788 /Len --> 10\n",
            "Result --> 2.340067969345788 /Element --> 0502011002\n",
            "117\n",
            "117\n",
            "117\n",
            "******INDEX  119 *****\n",
            "Sum --> 17.462288826689832 /Len --> 10\n",
            "Result --> 1.7462288826689831 /Element --> 0201020606\n",
            "118\n",
            "118\n",
            "118\n",
            "******INDEX  120 *****\n",
            "Sum --> 22.0698158182538 /Len --> 10\n",
            "Result --> 2.20698158182538 /Element --> 0305010406\n",
            "119\n",
            "119\n",
            "119\n",
            "******INDEX  121 *****\n",
            "Sum --> 18.759685536683904 /Len --> 10\n",
            "Result --> 1.8759685536683903 /Element --> 0202040307\n",
            "120\n",
            "120\n",
            "120\n",
            "******INDEX  122 *****\n",
            "Sum --> 63.671213079890755 /Len --> 10\n",
            "Result --> 6.367121307989075 /Element --> 1705030806\n",
            "121\n",
            "121\n",
            "121\n",
            "******INDEX  123 *****\n",
            "Sum --> 35.85127145699487 /Len --> 10\n",
            "Result --> 3.585127145699487 /Element --> 1005040302\n",
            "122\n",
            "122\n",
            "122\n",
            "******INDEX  124 *****\n",
            "Sum --> 39.054260396672106 /Len --> 10\n",
            "Result --> 3.9054260396672107 /Element --> 0808040503\n",
            "123\n",
            "123\n",
            "123\n",
            "******INDEX  125 *****\n",
            "Sum --> 31.32169336909384 /Len --> 10\n",
            "Result --> 3.132169336909384 /Element --> 1103020201\n",
            "132\n",
            "132\n",
            "132\n",
            "******INDEX  134 *****\n",
            "Sum --> 20.17116751094773 /Len --> 10\n",
            "Result --> 2.0171167510947727 /Element --> 0301080502\n",
            "133\n",
            "133\n",
            "133\n",
            "******INDEX  135 *****\n",
            "Sum --> 15.759685536683902 /Len --> 10\n",
            "Result --> 1.5759685536683903 /Element --> 0202030404\n",
            "134\n",
            "134\n",
            "134\n",
            "******INDEX  136 *****\n",
            "Sum --> 34.25893999575079 /Len --> 10\n",
            "Result --> 3.4258939995750786 /Element --> 0807050301\n",
            "135\n",
            "135\n",
            "135\n",
            "******INDEX  137 *****\n",
            "Sum --> 21.297396709994068 /Len --> 10\n",
            "Result --> 2.129739670999407 /Element --> 0102050805\n",
            "136\n",
            "136\n",
            "136\n",
            "******INDEX  138 *****\n",
            "Sum --> 35.548650799549804 /Len --> 10\n",
            "Result --> 3.5548650799549804 /Element --> 0604071003\n",
            "137\n",
            "137\n",
            "137\n",
            "******INDEX  139 *****\n",
            "Sum --> 52.112247884738395 /Len --> 10\n",
            "Result --> 5.21122478847384 /Element --> 0913020605\n",
            "138\n",
            "138\n",
            "138\n",
            "******INDEX  140 *****\n",
            "Sum --> 25.286722556493533 /Len --> 10\n",
            "Result --> 2.5286722556493535 /Element --> 0703050202\n",
            "139\n",
            "139\n",
            "******INDEX  141 *****\n",
            "Sum --> 32.229015515547 /Len --> 8\n",
            "Result --> 4.028626939443375 /Element --> 05080804\n",
            "140\n",
            "140\n",
            "140\n",
            "******INDEX  142 *****\n",
            "Sum --> 24.856433643089765 /Len --> 10\n",
            "Result --> 2.4856433643089764 /Element --> 0606020301\n",
            "141\n",
            "141\n",
            "141\n",
            "******INDEX  143 *****\n",
            "Sum --> 28.769336928223957 /Len --> 10\n",
            "Result --> 2.8769336928223956 /Element --> 0111040303\n",
            "142\n",
            "142\n",
            "142\n",
            "******INDEX  144 *****\n",
            "Sum --> 27.188598798124776 /Len --> 10\n",
            "Result --> 2.7188598798124777 /Element --> 0408020403\n",
            "143\n",
            "143\n",
            "143\n",
            "******INDEX  145 *****\n",
            "Sum --> 39.206559507680495 /Len --> 10\n",
            "Result --> 3.9206559507680496 /Element --> 0804060508\n",
            "144\n",
            "144\n",
            "144\n",
            "******INDEX  146 *****\n",
            "Sum --> 39.665720683435474 /Len --> 10\n",
            "Result --> 3.9665720683435475 /Element --> 0803031107\n",
            "145\n",
            "145\n",
            "145\n",
            "******INDEX  147 *****\n",
            "Sum --> 44.128237181934196 /Len --> 10\n",
            "Result --> 4.41282371819342 /Element --> 0616030102\n",
            "146\n",
            "146\n",
            "146\n",
            "******INDEX  148 *****\n",
            "Sum --> 28.74032046978141 /Len --> 10\n",
            "Result --> 2.874032046978141 /Element --> 0204010614\n",
            "147\n",
            "147\n",
            "147\n",
            "******INDEX  149 *****\n",
            "Sum --> 30.258939995750787 /Len --> 10\n",
            "Result --> 3.0258939995750787 /Element --> 0807010103\n",
            "148\n",
            "******INDEX  150 *****\n",
            "Sum --> 25.229015515546998 /Len --> 6\n",
            "Result --> 4.204835919257833 /Element --> 050805\n",
            "149\n",
            "******INDEX  151 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "150\n",
            "******INDEX  152 *****\n",
            "Sum --> 27.69603511437994 /Len --> 6\n",
            "Result --> 4.616005852396657 /Element --> 090208\n",
            "151\n",
            "151\n",
            "******INDEX  153 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0307\n",
            "152\n",
            "******INDEX  154 *****\n",
            "Sum --> 27.879941868808846 /Len --> 6\n",
            "Result --> 4.646656978134808 /Element --> 070705\n",
            "153\n",
            "******INDEX  155 *****\n",
            "Sum --> 21.00193129076989 /Len --> 6\n",
            "Result --> 3.5003218817949815 /Element --> 050506\n",
            "154\n",
            "******INDEX  156 *****\n",
            "Sum --> 12 /Len --> 2\n",
            "Result --> 6.0 /Element --> 12\n",
            "155\n",
            "******INDEX  157 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "156\n",
            "******INDEX  158 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "157\n",
            "157\n",
            "157\n",
            "******INDEX  159 *****\n",
            "Sum --> 22.0698158182538 /Len --> 10\n",
            "Result --> 2.20698158182538 /Element --> 0305030305\n",
            "158\n",
            "158\n",
            "******INDEX  160 *****\n",
            "Sum --> 30 /Len --> 4\n",
            "Result --> 7.5 /Element --> 1614\n",
            "159\n",
            "******INDEX  161 *****\n",
            "Sum --> 16 /Len --> 2\n",
            "Result --> 8.0 /Element --> 16\n",
            "160\n",
            "160\n",
            "160\n",
            "******INDEX  162 *****\n",
            "Sum --> 33.44817804495305 /Len --> 10\n",
            "Result --> 3.3448178044953054 /Element --> 0705050504\n",
            "161\n",
            "161\n",
            "******INDEX  163 *****\n",
            "Sum --> 20.296900043030913 /Len --> 8\n",
            "Result --> 2.537112505378864 /Element --> 03080103\n",
            "162\n",
            "162\n",
            "162\n",
            "******INDEX  164 *****\n",
            "Sum --> 31.851271456994873 /Len --> 10\n",
            "Result --> 3.185127145699487 /Element --> 1005010301\n",
            "163\n",
            "163\n",
            "163\n",
            "******INDEX  165 *****\n",
            "Sum --> 15.400679693457883 /Len --> 10\n",
            "Result --> 1.5400679693457884 /Element --> 0502030101\n",
            "164\n",
            "164\n",
            "164\n",
            "******INDEX  166 *****\n",
            "Sum --> 21.54952973764698 /Len --> 10\n",
            "Result --> 2.154952973764698 /Element --> 0701010502\n",
            "165\n",
            "165\n",
            "165\n",
            "******INDEX  167 *****\n",
            "Sum --> 20.393278397203456 /Len --> 10\n",
            "Result --> 2.039327839720346 /Element --> 0407010201\n",
            "166\n",
            "166\n",
            "166\n",
            "******INDEX  168 *****\n",
            "Sum --> 26.18731078089654 /Len --> 10\n",
            "Result --> 2.6187310780896538 /Element --> 0212020101\n",
            "167\n",
            "167\n",
            "167\n",
            "******INDEX  169 *****\n",
            "Sum --> 21.648680752673126 /Len --> 10\n",
            "Result --> 2.1648680752673126 /Element --> 0406030202\n",
            "168\n",
            "168\n",
            "168\n",
            "******INDEX  170 *****\n",
            "Sum --> 22.38131462655539 /Len --> 10\n",
            "Result --> 2.238131462655539 /Element --> 0504040302\n",
            "169\n",
            "169\n",
            "169\n",
            "******INDEX  171 *****\n",
            "Sum --> 32.84692644764105 /Len --> 10\n",
            "Result --> 3.2846926447641054 /Element --> 0702020808\n",
            "170\n",
            "170\n",
            "170\n",
            "******INDEX  172 *****\n",
            "Sum --> 36.89513802982715 /Len --> 10\n",
            "Result --> 3.6895138029827153 /Element --> 0809010403\n",
            "171\n",
            "171\n",
            "171\n",
            "******INDEX  173 *****\n",
            "Sum --> 23.169267463764307 /Len --> 10\n",
            "Result --> 2.3169267463764305 /Element --> 0605040101\n",
            "172\n",
            "172\n",
            "172\n",
            "******INDEX  174 *****\n",
            "Sum --> 43.80155507429993 /Len --> 10\n",
            "Result --> 4.3801555074299925 /Element --> 1010030401\n",
            "173\n",
            "173\n",
            "173\n",
            "******INDEX  175 *****\n",
            "Sum --> 14.759685536683902 /Len --> 10\n",
            "Result --> 1.4759685536683902 /Element --> 0202060202\n",
            "174\n",
            "174\n",
            "******INDEX  176 *****\n",
            "Sum --> 6.0 /Len --> 8\n",
            "Result --> 0.75 /Element --> 01010103\n",
            "175\n",
            "175\n",
            "175\n",
            "******INDEX  177 *****\n",
            "Sum --> 17.125732532083184 /Len --> 10\n",
            "Result --> 1.7125732532083184 /Element --> 0108010102\n",
            "176\n",
            "176\n",
            "******INDEX  178 *****\n",
            "Sum --> 5 /Len --> 4\n",
            "Result --> 1.25 /Element --> 0203\n",
            "177\n",
            "177\n",
            "******INDEX  179 *****\n",
            "Sum --> 3 /Len --> 4\n",
            "Result --> 0.75 /Element --> 0102\n",
            "178\n",
            "178\n",
            "******INDEX  180 *****\n",
            "Sum --> 6 /Len --> 4\n",
            "Result --> 1.5 /Element --> 0303\n",
            "179\n",
            "179\n",
            "179\n",
            "******INDEX  181 *****\n",
            "Sum --> 18.468564220941797 /Len --> 10\n",
            "Result --> 1.8468564220941797 /Element --> 0302040305\n",
            "180\n",
            "180\n",
            "******INDEX  182 *****\n",
            "Sum --> 16.462288826689832 /Len --> 8\n",
            "Result --> 2.057786103336229 /Element --> 02010706\n",
            "181\n",
            "******INDEX  183 *****\n",
            "Sum --> 16.588021358773016 /Len --> 6\n",
            "Result --> 2.764670226462169 /Element --> 020802\n",
            "182\n",
            "182\n",
            "182\n",
            "******INDEX  184 *****\n",
            "Sum --> 32.601031287620096 /Len --> 10\n",
            "Result --> 3.2601031287620095 /Element --> 0607050304\n",
            "183\n",
            "183\n",
            "183\n",
            "******INDEX  185 *****\n",
            "Sum --> 18.73719281884655 /Len --> 10\n",
            "Result --> 1.873719281884655 /Element --> 0103070205\n",
            "184\n",
            "184\n",
            "184\n",
            "******INDEX  186 *****\n",
            "Sum --> 18.585814486631534 /Len --> 10\n",
            "Result --> 1.8585814486631533 /Element --> 0106030402\n",
            "185\n",
            "185\n",
            "185\n",
            "******INDEX  187 *****\n",
            "Sum --> 23.449199154039306 /Len --> 10\n",
            "Result --> 2.3449199154039304 /Element --> 0304030803\n",
            "186\n",
            "186\n",
            "186\n",
            "******INDEX  188 *****\n",
            "Sum --> 19.5680158664523 /Len --> 10\n",
            "Result --> 1.95680158664523 /Element --> 0602030301\n",
            "187\n",
            "187\n",
            "187\n",
            "******INDEX  189 *****\n",
            "Sum --> 22.0698158182538 /Len --> 10\n",
            "Result --> 2.20698158182538 /Element --> 0305090101\n",
            "188\n",
            "188\n",
            "188\n",
            "******INDEX  190 *****\n",
            "Sum --> 47.23065479278037 /Len --> 10\n",
            "Result --> 4.723065479278037 /Element --> 1004041206\n",
            "189\n",
            "189\n",
            "189\n",
            "******INDEX  191 *****\n",
            "Sum --> 11.90836032979428 /Len --> 10\n",
            "Result --> 1.190836032979428 /Element --> 0303020101\n",
            "190\n",
            "190\n",
            "190\n",
            "******INDEX  192 *****\n",
            "Sum --> 40.43369511462568 /Len --> 10\n",
            "Result --> 4.043369511462568 /Element --> 0507081004\n",
            "191\n",
            "******INDEX  193 *****\n",
            "Sum --> 24.448178044953053 /Len --> 6\n",
            "Result --> 4.074696340825509 /Element --> 070505\n",
            "192\n",
            "******INDEX  194 *****\n",
            "Sum --> 20.961514573347667 /Len --> 6\n",
            "Result --> 3.4935857622246114 /Element --> 040508\n",
            "193\n",
            "******INDEX  195 *****\n",
            "Sum --> 8.737192818846552 /Len --> 6\n",
            "Result --> 1.456198803141092 /Element --> 010304\n",
            "194\n",
            "194\n",
            "******INDEX  196 *****\n",
            "Sum --> 15.898648307306074 /Len --> 8\n",
            "Result --> 1.9873310384132592 /Element --> 01050107\n",
            "195\n",
            "195\n",
            "195\n",
            "******INDEX  197 *****\n",
            "Sum --> 35.135831223232415 /Len --> 10\n",
            "Result --> 3.5135831223232414 /Element --> 0903020606\n",
            "196\n",
            "196\n",
            "******INDEX  198 *****\n",
            "Sum --> 11 /Len --> 4\n",
            "Result --> 2.75 /Element --> 0506\n",
            "197\n",
            "197\n",
            "197\n",
            "******INDEX  199 *****\n",
            "Sum --> 49.799215160526025 /Len --> 10\n",
            "Result --> 4.9799215160526025 /Element --> 1405010209\n",
            "199\n",
            "199\n",
            "******INDEX  201 *****\n",
            "Sum --> 20.961514573347667 /Len --> 8\n",
            "Result --> 2.6201893216684584 /Element --> 04050701\n",
            "200\n",
            "200\n",
            "200\n",
            "******INDEX  202 *****\n",
            "Sum --> 17.898648307306075 /Len --> 10\n",
            "Result --> 1.7898648307306075 /Element --> 0105010504\n",
            "201\n",
            "201\n",
            "201\n",
            "******INDEX  203 *****\n",
            "Sum --> 34.250019859682865 /Len --> 10\n",
            "Result --> 3.4250019859682865 /Element --> 1002020802\n",
            "202\n",
            "202\n",
            "202\n",
            "******INDEX  204 *****\n",
            "Sum --> 72.75818487869591 /Len --> 10\n",
            "Result --> 7.275818487869591 /Element --> 1416070304\n",
            "203\n",
            "203\n",
            "203\n",
            "******INDEX  205 *****\n",
            "Sum --> 13.171167510947729 /Len --> 10\n",
            "Result --> 1.317116751094773 /Element --> 0301040103\n",
            "204\n",
            "204\n",
            "204\n",
            "******INDEX  206 *****\n",
            "Sum --> 26.800059084888144 /Len --> 10\n",
            "Result --> 2.6800059084888144 /Element --> 0403050606\n",
            "205\n",
            "205\n",
            "205\n",
            "******INDEX  207 *****\n",
            "Sum --> 22.449199154039306 /Len --> 10\n",
            "Result --> 2.2449199154039308 /Element --> 0304080104\n",
            "206\n",
            "206\n",
            "206\n",
            "******INDEX  208 *****\n",
            "Sum --> 12.759685536683902 /Len --> 10\n",
            "Result --> 1.2759685536683902 /Element --> 0202010205\n",
            "207\n",
            "207\n",
            "207\n",
            "******INDEX  209 *****\n",
            "Sum --> 33.832203194265546 /Len --> 10\n",
            "Result --> 3.3832203194265547 /Element --> 0411040501\n",
            "208\n",
            "208\n",
            "208\n",
            "******INDEX  210 *****\n",
            "Sum --> 20.0 /Len --> 10\n",
            "Result --> 2.0 /Element --> 0101060606\n",
            "209\n",
            "209\n",
            "209\n",
            "******INDEX  211 *****\n",
            "Sum --> 33.250019859682865 /Len --> 10\n",
            "Result --> 3.3250019859682864 /Element --> 1002010109\n",
            "210\n",
            "210\n",
            "210\n",
            "******INDEX  212 *****\n",
            "Sum --> 19.125732532083184 /Len --> 10\n",
            "Result --> 1.9125732532083184 /Element --> 0108040101\n",
            "211\n",
            "211\n",
            "211\n",
            "******INDEX  213 *****\n",
            "Sum --> 22.846926447641053 /Len --> 10\n",
            "Result --> 2.2846926447641054 /Element --> 0702060101\n",
            "212\n",
            "212\n",
            "212\n",
            "******INDEX  214 *****\n",
            "Sum --> 32.29690004303092 /Len --> 10\n",
            "Result --> 3.229690004303092 /Element --> 0308110401\n",
            "213\n",
            "213\n",
            "213\n",
            "******INDEX  215 *****\n",
            "Sum --> 25.169267463764307 /Len --> 10\n",
            "Result --> 2.5169267463764307 /Element --> 0605010601\n",
            "214\n",
            "214\n",
            "******INDEX  216 *****\n",
            "Sum --> 13.468564220941797 /Len --> 8\n",
            "Result --> 1.6835705276177246 /Element --> 03020106\n",
            "215\n",
            "215\n",
            "215\n",
            "******INDEX  217 *****\n",
            "Sum --> 30.689097470095348 /Len --> 10\n",
            "Result --> 3.068909747009535 /Element --> 0506030704\n",
            "216\n",
            "216\n",
            "216\n",
            "******INDEX  218 *****\n",
            "Sum --> 26.199481645536384 /Len --> 10\n",
            "Result --> 2.619948164553638 /Element --> 0203040610\n",
            "217\n",
            "217\n",
            "217\n",
            "******INDEX  219 *****\n",
            "Sum --> 31.188598798124776 /Len --> 10\n",
            "Result --> 3.1188598798124776 /Element --> 0408030802\n",
            "218\n",
            "218\n",
            "218\n",
            "******INDEX  220 *****\n",
            "Sum --> 33.23065479278037 /Len --> 10\n",
            "Result --> 3.323065479278037 /Element --> 1004030104\n",
            "219\n",
            "219\n",
            "219\n",
            "******INDEX  221 *****\n",
            "Sum --> 17.40067969345788 /Len --> 10\n",
            "Result --> 1.7400679693457881 /Element --> 0502020104\n",
            "220\n",
            "220\n",
            "220\n",
            "******INDEX  222 *****\n",
            "Sum --> 31.137777676185962 /Len --> 10\n",
            "Result --> 3.113777767618596 /Element --> 0309030703\n",
            "221\n",
            "221\n",
            "221\n",
            "******INDEX  223 *****\n",
            "Sum --> 33.44817804495305 /Len --> 10\n",
            "Result --> 3.3448178044953054 /Element --> 0705030605\n",
            "222\n",
            "222\n",
            "222\n",
            "******INDEX  224 *****\n",
            "Sum --> 36.13534422427851 /Len --> 10\n",
            "Result --> 3.6135344224278514 /Element --> 0706010509\n",
            "223\n",
            "223\n",
            "223\n",
            "******INDEX  225 *****\n",
            "Sum --> 25.0698158182538 /Len --> 10\n",
            "Result --> 2.50698158182538 /Element --> 0305080402\n",
            "224\n",
            "224\n",
            "224\n",
            "******INDEX  226 *****\n",
            "Sum --> 38.396351688541415 /Len --> 10\n",
            "Result --> 3.8396351688541417 /Element --> 0608130201\n",
            "225\n",
            "225\n",
            "225\n",
            "******INDEX  227 *****\n",
            "Sum --> 14.462288826689832 /Len --> 10\n",
            "Result --> 1.4462288826689833 /Element --> 0201020108\n",
            "226\n",
            "226\n",
            "226\n",
            "******INDEX  228 *****\n",
            "Sum --> 44.135831223232415 /Len --> 10\n",
            "Result --> 4.413583122323241 /Element --> 0903070412\n",
            "227\n",
            "227\n",
            "227\n",
            "******INDEX  229 *****\n",
            "Sum --> 24.048103313321363 /Len --> 10\n",
            "Result --> 2.4048103313321363 /Element --> 0206060304\n",
            "228\n",
            "228\n",
            "228\n",
            "******INDEX  230 *****\n",
            "Sum --> 26.330412131161864 /Len --> 10\n",
            "Result --> 2.6330412131161864 /Element --> 0107050505\n",
            "229\n",
            "229\n",
            "229\n",
            "******INDEX  231 *****\n",
            "Sum --> 34.29728671169194 /Len --> 10\n",
            "Result --> 3.429728671169194 /Element --> 0905020206\n",
            "230\n",
            "230\n",
            "******INDEX  232 *****\n",
            "Sum --> 10.171167510947729 /Len --> 8\n",
            "Result --> 1.2713959388684661 /Element --> 03010401\n",
            "231\n",
            "231\n",
            "231\n",
            "******INDEX  233 *****\n",
            "Sum --> 24.74032046978141 /Len --> 10\n",
            "Result --> 2.4740320469781407 /Element --> 0204030707\n",
            "232\n",
            "232\n",
            "232\n",
            "******INDEX  234 *****\n",
            "Sum --> 9.468564220941797 /Len --> 10\n",
            "Result --> 0.9468564220941798 /Element --> 0302010101\n",
            "233\n",
            "233\n",
            "233\n",
            "******INDEX  235 *****\n",
            "Sum --> 24.007811975304783 /Len --> 10\n",
            "Result --> 2.4007811975304785 /Element --> 0603050203\n",
            "234\n",
            "234\n",
            "234\n",
            "******INDEX  236 *****\n",
            "Sum --> 17.17116751094773 /Len --> 10\n",
            "Result --> 1.717116751094773 /Element --> 0301050205\n",
            "235\n",
            "235\n",
            "235\n",
            "******INDEX  237 *****\n",
            "Sum --> 17.73719281884655 /Len --> 10\n",
            "Result --> 1.7737192818846552 /Element --> 0103030307\n",
            "236\n",
            "236\n",
            "236\n",
            "******INDEX  238 *****\n",
            "Sum --> 24.74032046978141 /Len --> 10\n",
            "Result --> 2.4740320469781407 /Element --> 0204100304\n",
            "237\n",
            "237\n",
            "237\n",
            "******INDEX  239 *****\n",
            "Sum --> 32.96151457334767 /Len --> 10\n",
            "Result --> 3.296151457334767 /Element --> 0405060806\n",
            "238\n",
            "238\n",
            "238\n",
            "******INDEX  240 *****\n",
            "Sum --> 26.501579642109593 /Len --> 10\n",
            "Result --> 2.650157964210959 /Element --> 0307060204\n",
            "240\n",
            "******INDEX  242 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "241\n",
            "******INDEX  243 *****\n",
            "Sum --> 31.054260396672106 /Len --> 6\n",
            "Result --> 5.175710066112018 /Element --> 080804\n",
            "242\n",
            "242\n",
            "242\n",
            "******INDEX  244 *****\n",
            "Sum --> 60.18787909943973 /Len --> 10\n",
            "Result --> 6.018787909943973 /Element --> 1205060913\n",
            "244\n",
            "******INDEX  246 *****\n",
            "Sum --> 32.67526226973017 /Len --> 6\n",
            "Result --> 5.4458770449550284 /Element --> 070808\n",
            "245\n",
            "245\n",
            "******INDEX  247 *****\n",
            "Sum --> 16 /Len --> 4\n",
            "Result --> 4.0 /Element --> 0907\n",
            "246\n",
            "246\n",
            "******INDEX  248 *****\n",
            "Sum --> 14 /Len --> 4\n",
            "Result --> 3.5 /Element --> 0806\n",
            "248\n",
            "******INDEX  250 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "249\n",
            "249\n",
            "249\n",
            "******INDEX  251 *****\n",
            "Sum --> 23.007811975304783 /Len --> 10\n",
            "Result --> 2.3007811975304784 /Element --> 0603020304\n",
            "250\n",
            "250\n",
            "250\n",
            "******INDEX  252 *****\n",
            "Sum --> 16.199481645536384 /Len --> 10\n",
            "Result --> 1.6199481645536384 /Element --> 0203040402\n",
            "251\n",
            "251\n",
            "251\n",
            "******INDEX  253 *****\n",
            "Sum --> 64.05574406989919 /Len --> 10\n",
            "Result --> 6.405574406989919 /Element --> 1602041506\n",
            "252\n",
            "252\n",
            "252\n",
            "******INDEX  254 *****\n",
            "Sum --> 27.22592457458299 /Len --> 10\n",
            "Result --> 2.722592457458299 /Element --> 0802040303\n",
            "253\n",
            "253\n",
            "253\n",
            "******INDEX  255 *****\n",
            "Sum --> 24.360937133995904 /Len --> 10\n",
            "Result --> 2.4360937133995906 /Element --> 0205040308\n",
            "254\n",
            "254\n",
            "254\n",
            "******INDEX  256 *****\n",
            "Sum --> 33.39327839720346 /Len --> 10\n",
            "Result --> 3.339327839720346 /Element --> 0407100304\n",
            "255\n",
            "255\n",
            "255\n",
            "******INDEX  257 *****\n",
            "Sum --> 20.898648307306075 /Len --> 10\n",
            "Result --> 2.0898648307306074 /Element --> 0105050305\n",
            "256\n",
            "256\n",
            "256\n",
            "******INDEX  258 *****\n",
            "Sum --> 24.54952973764698 /Len --> 10\n",
            "Result --> 2.4549529737646982 /Element --> 0701010307\n",
            "257\n",
            "257\n",
            "257\n",
            "******INDEX  259 *****\n",
            "Sum --> 47.35989668747461 /Len --> 10\n",
            "Result --> 4.735989668747461 /Element --> 1302060506\n",
            "258\n",
            "258\n",
            "258\n",
            "******INDEX  260 *****\n",
            "Sum --> 30.68981596853535 /Len --> 10\n",
            "Result --> 3.068981596853535 /Element --> 1003040102\n",
            "259\n",
            "******INDEX  261 *****\n",
            "Sum --> 10.062866266041592 /Len --> 6\n",
            "Result --> 1.6771443776735986 /Element --> 040103\n",
            "260\n",
            "260\n",
            "260\n",
            "******INDEX  262 *****\n",
            "Sum --> 13.737192818846552 /Len --> 10\n",
            "Result --> 1.373719281884655 /Element --> 0103010206\n",
            "261\n",
            "261\n",
            "261\n",
            "******INDEX  263 *****\n",
            "Sum --> 18.800059084888144 /Len --> 10\n",
            "Result --> 1.8800059084888143 /Element --> 0403040203\n",
            "262\n",
            "262\n",
            "262\n",
            "******INDEX  264 *****\n",
            "Sum --> 12.199481645536384 /Len --> 10\n",
            "Result --> 1.2199481645536383 /Element --> 0203010401\n",
            "263\n",
            "263\n",
            "263\n",
            "******INDEX  265 *****\n",
            "Sum --> 29.585814486631534 /Len --> 10\n",
            "Result --> 2.9585814486631534 /Element --> 0106070211\n",
            "264\n",
            "264\n",
            "264\n",
            "******INDEX  266 *****\n",
            "Sum --> 14.171167510947729 /Len --> 10\n",
            "Result --> 1.4171167510947729 /Element --> 0301020403\n",
            "265\n",
            "265\n",
            "265\n",
            "******INDEX  267 *****\n",
            "Sum --> 17.449199154039306 /Len --> 10\n",
            "Result --> 1.7449199154039305 /Element --> 0304050102\n",
            "266\n",
            "266\n",
            "266\n",
            "******INDEX  268 *****\n",
            "Sum --> 28.961514573347667 /Len --> 10\n",
            "Result --> 2.8961514573347666 /Element --> 0405040309\n",
            "267\n",
            "267\n",
            "267\n",
            "******INDEX  269 *****\n",
            "Sum --> 23.928527864588922 /Len --> 10\n",
            "Result --> 2.392852786458892 /Element --> 0801010403\n",
            "268\n",
            "268\n",
            "268\n",
            "******INDEX  270 *****\n",
            "Sum --> 20.199481645536384 /Len --> 10\n",
            "Result --> 2.0199481645536386 /Element --> 0203010805\n",
            "269\n",
            "269\n",
            "269\n",
            "******INDEX  271 *****\n",
            "Sum --> 26.007811975304783 /Len --> 10\n",
            "Result --> 2.600781197530478 /Element --> 0603020208\n",
            "270\n",
            "270\n",
            "270\n",
            "******INDEX  272 *****\n",
            "Sum --> 16.199481645536384 /Len --> 10\n",
            "Result --> 1.6199481645536384 /Element --> 0203010108\n",
            "271\n",
            "271\n",
            "271\n",
            "******INDEX  273 *****\n",
            "Sum --> 27.00193129076989 /Len --> 10\n",
            "Result --> 2.7001931290769887 /Element --> 0505040107\n",
            "272\n",
            "272\n",
            "272\n",
            "******INDEX  274 *****\n",
            "Sum --> 19.961514573347667 /Len --> 10\n",
            "Result --> 1.9961514573347667 /Element --> 0405030103\n",
            "273\n",
            "273\n",
            "273\n",
            "******INDEX  275 *****\n",
            "Sum --> 30.398638404385867 /Len --> 10\n",
            "Result --> 3.0398638404385867 /Element --> 0901040305\n",
            "274\n",
            "274\n",
            "274\n",
            "******INDEX  276 *****\n",
            "Sum --> 49.41496332421684 /Len --> 10\n",
            "Result --> 4.941496332421684 /Element --> 1208040107\n",
            "275\n",
            "275\n",
            "275\n",
            "******INDEX  277 *****\n",
            "Sum --> 28.5680158664523 /Len --> 10\n",
            "Result --> 2.85680158664523 /Element --> 0602050605\n",
            "276\n",
            "276\n",
            "276\n",
            "******INDEX  278 *****\n",
            "Sum --> 30.840475802310365 /Len --> 10\n",
            "Result --> 3.0840475802310365 /Element --> 0503040708\n",
            "277\n",
            "277\n",
            "277\n",
            "******INDEX  279 *****\n",
            "Sum --> 29.137777676185962 /Len --> 10\n",
            "Result --> 2.9137777676185963 /Element --> 0309010505\n",
            "278\n",
            "278\n",
            "278\n",
            "******INDEX  280 *****\n",
            "Sum --> 20.840475802310365 /Len --> 10\n",
            "Result --> 2.0840475802310365 /Element --> 0503010206\n",
            "279\n",
            "279\n",
            "279\n",
            "******INDEX  281 *****\n",
            "Sum --> 8.737192818846552 /Len --> 10\n",
            "Result --> 0.8737192818846552 /Element --> 0103020101\n",
            "280\n",
            "280\n",
            "280\n",
            "******INDEX  282 *****\n",
            "Sum --> 23.69603511437994 /Len --> 10\n",
            "Result --> 2.369603511437994 /Element --> 0902010102\n",
            "281\n",
            "281\n",
            "281\n",
            "******INDEX  283 *****\n",
            "Sum --> 36.78157891381218 /Len --> 10\n",
            "Result --> 3.6781578913812183 /Element --> 0115010108\n",
            "282\n",
            "282\n",
            "282\n",
            "******INDEX  284 *****\n",
            "Sum --> 24.40067969345788 /Len --> 10\n",
            "Result --> 2.440067969345788 /Element --> 0502010805\n",
            "283\n",
            "283\n",
            "283\n",
            "******INDEX  285 *****\n",
            "Sum --> 35.02947643127983 /Len --> 10\n",
            "Result --> 3.502947643127983 /Element --> 0409100302\n",
            "284\n",
            "284\n",
            "284\n",
            "******INDEX  286 *****\n",
            "Sum --> 21.898648307306075 /Len --> 10\n",
            "Result --> 2.1898648307306074 /Element --> 0105050702\n",
            "285\n",
            "285\n",
            "285\n",
            "******INDEX  287 *****\n",
            "Sum --> 21.54952973764698 /Len --> 10\n",
            "Result --> 2.154952973764698 /Element --> 0701010304\n",
            "286\n",
            "286\n",
            "286\n",
            "******INDEX  288 *****\n",
            "Sum --> 35.34089790913317 /Len --> 10\n",
            "Result --> 3.534089790913317 /Element --> 0404120606\n",
            "287\n",
            "287\n",
            "287\n",
            "******INDEX  289 *****\n",
            "Sum --> 22.75698199757926 /Len --> 10\n",
            "Result --> 2.275698199757926 /Element --> 0306040204\n",
            "288\n",
            "288\n",
            "288\n",
            "******INDEX  290 *****\n",
            "Sum --> 35.23722932169647 /Len --> 10\n",
            "Result --> 3.5237229321696466 /Element --> 0609040304\n",
            "289\n",
            "289\n",
            "289\n",
            "******INDEX  291 *****\n",
            "Sum --> 32.125732532083184 /Len --> 10\n",
            "Result --> 3.212573253208318 /Element --> 0108120502\n",
            "290\n",
            "290\n",
            "290\n",
            "******INDEX  292 *****\n",
            "Sum --> 26.22592457458299 /Len --> 10\n",
            "Result --> 2.622592457458299 /Element --> 0802030303\n",
            "291\n",
            "291\n",
            "291\n",
            "******INDEX  293 *****\n",
            "Sum --> 25.74032046978141 /Len --> 10\n",
            "Result --> 2.574032046978141 /Element --> 0204040608\n",
            "292\n",
            "292\n",
            "292\n",
            "******INDEX  294 *****\n",
            "Sum --> 19.360262976035663 /Len --> 10\n",
            "Result --> 1.9360262976035663 /Element --> 0402010505\n",
            "293\n",
            "293\n",
            "293\n",
            "******INDEX  295 *****\n",
            "Sum --> 42.13534422427851 /Len --> 10\n",
            "Result --> 4.213534422427851 /Element --> 0706050808\n",
            "294\n",
            "294\n",
            "294\n",
            "******INDEX  296 *****\n",
            "Sum --> 18.468564220941797 /Len --> 10\n",
            "Result --> 1.8468564220941797 /Element --> 0302030207\n",
            "295\n",
            "295\n",
            "295\n",
            "******INDEX  297 *****\n",
            "Sum --> 16.297396709994068 /Len --> 10\n",
            "Result --> 1.6297396709994068 /Element --> 0102060601\n",
            "296\n",
            "296\n",
            "296\n",
            "******INDEX  298 *****\n",
            "Sum --> 21.103282983463814 /Len --> 10\n",
            "Result --> 2.1103282983463814 /Element --> 0501040602\n",
            "297\n",
            "297\n",
            "297\n",
            "******INDEX  299 *****\n",
            "Sum --> 18.54952973764698 /Len --> 10\n",
            "Result --> 1.8549529737646981 /Element --> 0701020102\n",
            "298\n",
            "298\n",
            "298\n",
            "******INDEX  300 *****\n",
            "Sum --> 18.34089790913317 /Len --> 10\n",
            "Result --> 1.834089790913317 /Element --> 0404030202\n",
            "299\n",
            "299\n",
            "299\n",
            "******INDEX  301 *****\n",
            "Sum --> 27.448178044953053 /Len --> 10\n",
            "Result --> 2.7448178044953053 /Element --> 0705030401\n",
            "300\n",
            "******INDEX  302 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "301\n",
            "301\n",
            "301\n",
            "******INDEX  303 *****\n",
            "Sum --> 24.5680158664523 /Len --> 10\n",
            "Result --> 2.4568015866452297 /Element --> 0602010506\n",
            "302\n",
            "302\n",
            "302\n",
            "******INDEX  304 *****\n",
            "Sum --> 35.31886666587094 /Len --> 10\n",
            "Result --> 3.531886666587094 /Element --> 0711020102\n",
            "303\n",
            "303\n",
            "303\n",
            "******INDEX  305 *****\n",
            "Sum --> 40.25893999575079 /Len --> 10\n",
            "Result --> 4.025893999575079 /Element --> 0807050505\n",
            "304\n",
            "304\n",
            "304\n",
            "******INDEX  306 *****\n",
            "Sum --> 34.06989314870205 /Len --> 10\n",
            "Result --> 3.406989314870205 /Element --> 0509030306\n",
            "305\n",
            "305\n",
            "305\n",
            "******INDEX  307 *****\n",
            "Sum --> 24.928527864588922 /Len --> 10\n",
            "Result --> 2.4928527864588923 /Element --> 0801030402\n",
            "306\n",
            "306\n",
            "306\n",
            "******INDEX  308 *****\n",
            "Sum --> 24.5680158664523 /Len --> 10\n",
            "Result --> 2.4568015866452297 /Element --> 0602040701\n",
            "307\n",
            "******INDEX  309 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "309\n",
            "******INDEX  311 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "310\n",
            "310\n",
            "310\n",
            "******INDEX  312 *****\n",
            "Sum --> 28.286722556493533 /Len --> 10\n",
            "Result --> 2.8286722556493533 /Element --> 0703010407\n",
            "311\n",
            "******INDEX  313 *****\n",
            "Sum --> 21.689097470095348 /Len --> 6\n",
            "Result --> 3.6148495783492245 /Element --> 050605\n",
            "312\n",
            "******INDEX  314 *****\n",
            "Sum --> 5 /Len --> 2\n",
            "Result --> 2.5 /Element --> 05\n",
            "313\n",
            "******INDEX  315 *****\n",
            "Sum --> 5 /Len --> 2\n",
            "Result --> 2.5 /Element --> 05\n",
            "315\n",
            "315\n",
            "******INDEX  317 *****\n",
            "Sum --> 21.38131462655539 /Len --> 8\n",
            "Result --> 2.672664328319424 /Element --> 05040206\n",
            "317\n",
            "******INDEX  319 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "319\n",
            "319\n",
            "319\n",
            "******INDEX  321 *****\n",
            "Sum --> 23.468564220941797 /Len --> 10\n",
            "Result --> 2.34685642209418 /Element --> 0302050408\n",
            "320\n",
            "320\n",
            "320\n",
            "******INDEX  322 *****\n",
            "Sum --> 15.199481645536384 /Len --> 10\n",
            "Result --> 1.5199481645536383 /Element --> 0203020502\n",
            "321\n",
            "321\n",
            "321\n",
            "******INDEX  323 *****\n",
            "Sum --> 17.898648307306075 /Len --> 10\n",
            "Result --> 1.7898648307306075 /Element --> 0105020602\n",
            "322\n",
            "322\n",
            "******INDEX  324 *****\n",
            "Sum --> 6 /Len --> 4\n",
            "Result --> 1.5 /Element --> 0105\n",
            "323\n",
            "323\n",
            "323\n",
            "******INDEX  325 *****\n",
            "Sum --> 22.840475802310365 /Len --> 10\n",
            "Result --> 2.2840475802310367 /Element --> 0503040304\n",
            "324\n",
            "324\n",
            "324\n",
            "******INDEX  326 *****\n",
            "Sum --> 40.02947643127983 /Len --> 10\n",
            "Result --> 4.002947643127984 /Element --> 0409011207\n",
            "325\n",
            "325\n",
            "325\n",
            "******INDEX  327 *****\n",
            "Sum --> 33.68909747009535 /Len --> 10\n",
            "Result --> 3.3689097470095346 /Element --> 0506080306\n",
            "326\n",
            "326\n",
            "326\n",
            "******INDEX  328 *****\n",
            "Sum --> 43.25893999575079 /Len --> 10\n",
            "Result --> 4.3258939995750785 /Element --> 0807070407\n",
            "327\n",
            "327\n",
            "327\n",
            "******INDEX  329 *****\n",
            "Sum --> 54.43343247485842 /Len --> 10\n",
            "Result --> 5.443343247485842 /Element --> 1110040606\n",
            "328\n",
            "328\n",
            "328\n",
            "******INDEX  330 *****\n",
            "Sum --> 21.297396709994068 /Len --> 10\n",
            "Result --> 2.129739670999407 /Element --> 0102021105\n",
            "329\n",
            "329\n",
            "******INDEX  331 *****\n",
            "Sum --> 60.92588324472302 /Len --> 8\n",
            "Result --> 7.615735405590377 /Element --> 15080708\n",
            "330\n",
            "******INDEX  332 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "331\n",
            "331\n",
            "331\n",
            "******INDEX  333 *****\n",
            "Sum --> 27.360262976035663 /Len --> 10\n",
            "Result --> 2.7360262976035665 /Element --> 0402031105\n",
            "332\n",
            "332\n",
            "332\n",
            "******INDEX  334 *****\n",
            "Sum --> 31.800059084888144 /Len --> 10\n",
            "Result --> 3.1800059084888144 /Element --> 0403080311\n",
            "333\n",
            "333\n",
            "333\n",
            "******INDEX  335 *****\n",
            "Sum --> 46.516139902885214 /Len --> 10\n",
            "Result --> 4.651613990288522 /Element --> 0709050609\n",
            "334\n",
            "334\n",
            "334\n",
            "******INDEX  336 *****\n",
            "Sum --> 30.879941868808846 /Len --> 10\n",
            "Result --> 3.0879941868808847 /Element --> 0707030302\n",
            "335\n",
            "335\n",
            "******INDEX  337 *****\n",
            "Sum --> 44.9844528910174 /Len --> 8\n",
            "Result --> 5.623056611377175 /Element --> 09060217\n",
            "338\n",
            "******INDEX  340 *****\n",
            "Sum --> 30.135344224278512 /Len --> 6\n",
            "Result --> 5.022557370713085 /Element --> 070609\n",
            "339\n",
            "339\n",
            "******INDEX  341 *****\n",
            "Sum --> 20 /Len --> 4\n",
            "Result --> 5.0 /Element --> 1307\n",
            "340\n",
            "******INDEX  342 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "341\n",
            "341\n",
            "341\n",
            "******INDEX  343 *****\n",
            "Sum --> 22.501579642109593 /Len --> 10\n",
            "Result --> 2.2501579642109593 /Element --> 0307020501\n",
            "342\n",
            "342\n",
            "342\n",
            "******INDEX  344 *****\n",
            "Sum --> 31.800059084888144 /Len --> 10\n",
            "Result --> 3.1800059084888144 /Element --> 0403070708\n",
            "343\n",
            "343\n",
            "343\n",
            "******INDEX  345 *****\n",
            "Sum --> 31.06989314870205 /Len --> 10\n",
            "Result --> 3.106989314870205 /Element --> 0509010107\n",
            "344\n",
            "344\n",
            "344\n",
            "******INDEX  346 *****\n",
            "Sum --> 27.206559507680495 /Len --> 10\n",
            "Result --> 2.7206559507680494 /Element --> 0804010501\n",
            "345\n",
            "345\n",
            "345\n",
            "******INDEX  347 *****\n",
            "Sum --> 20.898648307306075 /Len --> 10\n",
            "Result --> 2.0898648307306074 /Element --> 0105050107\n",
            "346\n",
            "346\n",
            "346\n",
            "******INDEX  348 *****\n",
            "Sum --> 19.199481645536384 /Len --> 10\n",
            "Result --> 1.9199481645536385 /Element --> 0203030109\n",
            "347\n",
            "347\n",
            "347\n",
            "******INDEX  349 *****\n",
            "Sum --> 19.0698158182538 /Len --> 10\n",
            "Result --> 1.9069815818253801 /Element --> 0305020204\n",
            "348\n",
            "348\n",
            "348\n",
            "******INDEX  350 *****\n",
            "Sum --> 15.462288826689832 /Len --> 10\n",
            "Result --> 1.5462288826689832 /Element --> 0201011001\n",
            "349\n",
            "349\n",
            "******INDEX  351 *****\n",
            "Sum --> 12.103282983463814 /Len --> 8\n",
            "Result --> 1.5129103729329767 /Element --> 05010201\n",
            "350\n",
            "350\n",
            "350\n",
            "******INDEX  352 *****\n",
            "Sum --> 34.13777767618596 /Len --> 10\n",
            "Result --> 3.4137777676185963 /Element --> 0309050506\n",
            "351\n",
            "351\n",
            "351\n",
            "******INDEX  353 *****\n",
            "Sum --> 10.468564220941797 /Len --> 10\n",
            "Result --> 1.0468564220941796 /Element --> 0302020101\n",
            "352\n",
            "352\n",
            "352\n",
            "******INDEX  354 *****\n",
            "Sum --> 11.278031643091577 /Len --> 10\n",
            "Result --> 1.1278031643091577 /Element --> 0104010301\n",
            "353\n",
            "353\n",
            "353\n",
            "******INDEX  355 *****\n",
            "Sum --> 22.585814486631534 /Len --> 10\n",
            "Result --> 2.2585814486631532 /Element --> 0106050107\n",
            "354\n",
            "354\n",
            "354\n",
            "******INDEX  356 *****\n",
            "Sum --> 35.92852786458892 /Len --> 10\n",
            "Result --> 3.5928527864588924 /Element --> 0801080408\n",
            "355\n",
            "355\n",
            "355\n",
            "******INDEX  357 *****\n",
            "Sum --> 16.360937133995904 /Len --> 10\n",
            "Result --> 1.6360937133995903 /Element --> 0205020203\n",
            "356\n",
            "356\n",
            "356\n",
            "******INDEX  358 *****\n",
            "Sum --> 16.5680158664523 /Len --> 10\n",
            "Result --> 1.65680158664523 /Element --> 0602010102\n",
            "357\n",
            "357\n",
            "357\n",
            "******INDEX  359 *****\n",
            "Sum --> 20.278031643091577 /Len --> 10\n",
            "Result --> 2.027803164309158 /Element --> 0104010409\n",
            "358\n",
            "358\n",
            "358\n",
            "******INDEX  360 *****\n",
            "Sum --> 21.38131462655539 /Len --> 10\n",
            "Result --> 2.138131462655539 /Element --> 0504030401\n",
            "359\n",
            "359\n",
            "359\n",
            "******INDEX  361 *****\n",
            "Sum --> 27.792700957851697 /Len --> 10\n",
            "Result --> 2.7792700957851695 /Element --> 0207100302\n",
            "360\n",
            "360\n",
            "360\n",
            "******INDEX  362 *****\n",
            "Sum --> 14.0 /Len --> 10\n",
            "Result --> 1.4 /Element --> 0101030108\n",
            "361\n",
            "361\n",
            "361\n",
            "******INDEX  363 *****\n",
            "Sum --> 32.80005908488815 /Len --> 10\n",
            "Result --> 3.280005908488815 /Element --> 0403120506\n",
            "362\n",
            "362\n",
            "362\n",
            "******INDEX  364 *****\n",
            "Sum --> 22.90836032979428 /Len --> 10\n",
            "Result --> 2.290836032979428 /Element --> 0303070305\n",
            "363\n",
            "363\n",
            "363\n",
            "******INDEX  365 *****\n",
            "Sum --> 36.75698199757926 /Len --> 10\n",
            "Result --> 3.675698199757926 /Element --> 0306021012\n",
            "364\n",
            "364\n",
            "364\n",
            "******INDEX  366 *****\n",
            "Sum --> 35.840475802310365 /Len --> 10\n",
            "Result --> 3.5840475802310365 /Element --> 0503050514\n",
            "365\n",
            "365\n",
            "365\n",
            "******INDEX  367 *****\n",
            "Sum --> 22.103282983463814 /Len --> 10\n",
            "Result --> 2.2103282983463814 /Element --> 0501050404\n",
            "366\n",
            "366\n",
            "366\n",
            "******INDEX  368 *****\n",
            "Sum --> 13.462288826689832 /Len --> 10\n",
            "Result --> 1.3462288826689832 /Element --> 0201020503\n",
            "367\n",
            "367\n",
            "367\n",
            "******INDEX  369 *****\n",
            "Sum --> 15.278031643091577 /Len --> 10\n",
            "Result --> 1.5278031643091576 /Element --> 0104010305\n",
            "368\n",
            "368\n",
            "368\n",
            "******INDEX  370 *****\n",
            "Sum --> 43.862532193338865 /Len --> 10\n",
            "Result --> 4.386253219333886 /Element --> 1104050110\n",
            "369\n",
            "369\n",
            "******INDEX  371 *****\n",
            "Sum --> 15.462288826689832 /Len --> 8\n",
            "Result --> 1.932786103336229 /Element --> 02011002\n",
            "370\n",
            "370\n",
            "370\n",
            "******INDEX  372 *****\n",
            "Sum --> 19.38131462655539 /Len --> 10\n",
            "Result --> 1.938131462655539 /Element --> 0504010104\n",
            "371\n",
            "371\n",
            "371\n",
            "******INDEX  373 *****\n",
            "Sum --> 40.89513802982715 /Len --> 10\n",
            "Result --> 4.089513802982715 /Element --> 0809060303\n",
            "372\n",
            "******INDEX  374 *****\n",
            "Sum --> 2 /Len --> 2\n",
            "Result --> 1.0 /Element --> 02\n",
            "373\n",
            "******INDEX  375 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "374\n",
            "******INDEX  376 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "375\n",
            "******INDEX  377 *****\n",
            "Sum --> 5 /Len --> 2\n",
            "Result --> 2.5 /Element --> 05\n",
            "376\n",
            "376\n",
            "******INDEX  378 *****\n",
            "Sum --> 22.393278397203456 /Len --> 8\n",
            "Result --> 2.799159799650432 /Element --> 04070204\n",
            "377\n",
            "377\n",
            "******INDEX  379 *****\n",
            "Sum --> 6 /Len --> 4\n",
            "Result --> 1.5 /Element --> 0204\n",
            "378\n",
            "378\n",
            "******INDEX  380 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0505\n",
            "379\n",
            "******INDEX  381 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "380\n",
            "380\n",
            "******INDEX  382 *****\n",
            "Sum --> 18 /Len --> 4\n",
            "Result --> 4.5 /Element --> 1008\n",
            "382\n",
            "382\n",
            "******INDEX  384 *****\n",
            "Sum --> 14 /Len --> 4\n",
            "Result --> 3.5 /Element --> 0608\n",
            "385\n",
            "******INDEX  387 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "387\n",
            "******INDEX  389 *****\n",
            "Sum --> 6.462288826689832 /Len --> 6\n",
            "Result --> 1.0770481377816388 /Element --> 020103\n",
            "388\n",
            "388\n",
            "******INDEX  390 *****\n",
            "Sum --> 5 /Len --> 4\n",
            "Result --> 1.25 /Element --> 0401\n",
            "389\n",
            "******INDEX  391 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "390\n",
            "******INDEX  392 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "391\n",
            "391\n",
            "******INDEX  393 *****\n",
            "Sum --> 8 /Len --> 4\n",
            "Result --> 2.0 /Element --> 0404\n",
            "392\n",
            "392\n",
            "******INDEX  394 *****\n",
            "Sum --> 11 /Len --> 4\n",
            "Result --> 2.75 /Element --> 0407\n",
            "393\n",
            "393\n",
            "******INDEX  395 *****\n",
            "Sum --> 12 /Len --> 4\n",
            "Result --> 3.0 /Element --> 0705\n",
            "394\n",
            "******INDEX  396 *****\n",
            "Sum --> 15 /Len --> 2\n",
            "Result --> 7.5 /Element --> 15\n",
            "395\n",
            "395\n",
            "******INDEX  397 *****\n",
            "Sum --> 18.468564220941797 /Len --> 8\n",
            "Result --> 2.3085705276177246 /Element --> 03020606\n",
            "396\n",
            "396\n",
            "******INDEX  398 *****\n",
            "Sum --> 21 /Len --> 4\n",
            "Result --> 5.25 /Element --> 1308\n",
            "397\n",
            "397\n",
            "******INDEX  399 *****\n",
            "Sum --> 12 /Len --> 4\n",
            "Result --> 3.0 /Element --> 0903\n",
            "398\n",
            "******INDEX  400 *****\n",
            "Sum --> 22.689097470095348 /Len --> 6\n",
            "Result --> 3.7815162450158915 /Element --> 050606\n",
            "399\n",
            "399\n",
            "******INDEX  401 *****\n",
            "Sum --> 15.400679693457883 /Len --> 8\n",
            "Result --> 1.9250849616822354 /Element --> 05020104\n",
            "400\n",
            "400\n",
            "******INDEX  402 *****\n",
            "Sum --> 20.103282983463814 /Len --> 8\n",
            "Result --> 2.5129103729329767 /Element --> 05010209\n",
            "401\n",
            "401\n",
            "******INDEX  403 *****\n",
            "Sum --> 38.56726243522523 /Len --> 8\n",
            "Result --> 4.820907804403154 /Element --> 12040503\n",
            "402\n",
            "******INDEX  404 *****\n",
            "Sum --> 14 /Len --> 2\n",
            "Result --> 7.0 /Element --> 14\n",
            "403\n",
            "403\n",
            "403\n",
            "******INDEX  405 *****\n",
            "Sum --> 38.862532193338865 /Len --> 10\n",
            "Result --> 3.8862532193338866 /Element --> 1104030305\n",
            "404\n",
            "404\n",
            "404\n",
            "******INDEX  406 *****\n",
            "Sum --> 18.048103313321363 /Len --> 10\n",
            "Result --> 1.8048103313321362 /Element --> 0206020203\n",
            "405\n",
            "405\n",
            "405\n",
            "******INDEX  407 *****\n",
            "Sum --> 20.800059084888144 /Len --> 10\n",
            "Result --> 2.0800059084888143 /Element --> 0403020306\n",
            "406\n",
            "406\n",
            "406\n",
            "******INDEX  408 *****\n",
            "Sum --> 32.0698158182538 /Len --> 10\n",
            "Result --> 3.20698158182538 /Element --> 0305060609\n",
            "407\n",
            "407\n",
            "407\n",
            "******INDEX  409 *****\n",
            "Sum --> 13.468564220941797 /Len --> 10\n",
            "Result --> 1.3468564220941797 /Element --> 0302010204\n",
            "408\n",
            "408\n",
            "408\n",
            "******INDEX  410 *****\n",
            "Sum --> 48.495540178751675 /Len --> 10\n",
            "Result --> 4.8495540178751675 /Element --> 1603010205\n",
            "409\n",
            "409\n",
            "409\n",
            "******INDEX  411 *****\n",
            "Sum --> 27.840475802310365 /Len --> 10\n",
            "Result --> 2.7840475802310367 /Element --> 0503060505\n",
            "410\n",
            "410\n",
            "410\n",
            "******INDEX  412 *****\n",
            "Sum --> 28.00193129076989 /Len --> 10\n",
            "Result --> 2.800193129076989 /Element --> 0505020506\n",
            "411\n",
            "411\n",
            "411\n",
            "******INDEX  413 *****\n",
            "Sum --> 15.585814486631532 /Len --> 10\n",
            "Result --> 1.5585814486631533 /Element --> 0106010302\n",
            "412\n",
            "412\n",
            "412\n",
            "******INDEX  414 *****\n",
            "Sum --> 31.952623149688797 /Len --> 10\n",
            "Result --> 3.19526231496888 /Element --> 1001010604\n",
            "413\n",
            "413\n",
            "413\n",
            "******INDEX  415 *****\n",
            "Sum --> 16.199481645536384 /Len --> 10\n",
            "Result --> 1.6199481645536384 /Element --> 0203050104\n",
            "414\n",
            "414\n",
            "414\n",
            "******INDEX  416 *****\n",
            "Sum --> 26.82756138073856 /Len --> 10\n",
            "Result --> 2.682756138073856 /Element --> 0704020304\n",
            "415\n",
            "415\n",
            "415\n",
            "******INDEX  417 *****\n",
            "Sum --> 21.330412131161864 /Len --> 10\n",
            "Result --> 2.1330412131161864 /Element --> 0107010801\n",
            "416\n",
            "416\n",
            "416\n",
            "******INDEX  418 *****\n",
            "Sum --> 10.29739670999407 /Len --> 10\n",
            "Result --> 1.0297396709994069 /Element --> 0102010105\n",
            "417\n",
            "417\n",
            "417\n",
            "******INDEX  419 *****\n",
            "Sum --> 21.74032046978141 /Len --> 10\n",
            "Result --> 2.174032046978141 /Element --> 0204050801\n",
            "418\n",
            "418\n",
            "418\n",
            "******INDEX  420 *****\n",
            "Sum --> 19.5680158664523 /Len --> 10\n",
            "Result --> 1.95680158664523 /Element --> 0602030103\n",
            "419\n",
            "419\n",
            "419\n",
            "******INDEX  421 *****\n",
            "Sum --> 24.188598798124776 /Len --> 10\n",
            "Result --> 2.4188598798124774 /Element --> 0408030201\n",
            "420\n",
            "420\n",
            "420\n",
            "******INDEX  422 *****\n",
            "Sum --> 40.601031287620096 /Len --> 10\n",
            "Result --> 4.06010312876201 /Element --> 0607021107\n",
            "421\n",
            "421\n",
            "421\n",
            "******INDEX  423 *****\n",
            "Sum --> 14.0 /Len --> 10\n",
            "Result --> 1.4 /Element --> 0101010605\n",
            "422\n",
            "422\n",
            "422\n",
            "******INDEX  424 *****\n",
            "Sum --> 36.135831223232415 /Len --> 10\n",
            "Result --> 3.6135831223232415 /Element --> 0903070305\n",
            "423\n",
            "423\n",
            "423\n",
            "******INDEX  425 *****\n",
            "Sum --> 25.54952973764698 /Len --> 10\n",
            "Result --> 2.5549529737646983 /Element --> 0701040305\n",
            "424\n",
            "424\n",
            "424\n",
            "******INDEX  426 *****\n",
            "Sum --> 26.007811975304783 /Len --> 10\n",
            "Result --> 2.600781197530478 /Element --> 0603070401\n",
            "425\n",
            "425\n",
            "******INDEX  427 *****\n",
            "Sum --> 13 /Len --> 4\n",
            "Result --> 3.25 /Element --> 0508\n",
            "426\n",
            "426\n",
            "******INDEX  428 *****\n",
            "Sum --> 16 /Len --> 4\n",
            "Result --> 4.0 /Element --> 0808\n",
            "427\n",
            "******INDEX  429 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "428\n",
            "******INDEX  430 *****\n",
            "Sum --> 1 /Len --> 2\n",
            "Result --> 0.5 /Element --> 01\n",
            "429\n",
            "******INDEX  431 *****\n",
            "Sum --> 17.330412131161864 /Len --> 6\n",
            "Result --> 2.8884020218603106 /Element --> 010706\n",
            "430\n",
            "430\n",
            "******INDEX  432 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0901\n",
            "431\n",
            "******INDEX  433 *****\n",
            "Sum --> 29.676670047477444 /Len --> 6\n",
            "Result --> 4.946111674579574 /Element --> 090407\n",
            "434\n",
            "434\n",
            "434\n",
            "******INDEX  436 *****\n",
            "Sum --> 22.40067969345788 /Len --> 10\n",
            "Result --> 2.2400679693457883 /Element --> 0502060204\n",
            "435\n",
            "435\n",
            "435\n",
            "******INDEX  437 *****\n",
            "Sum --> 32.449199154039306 /Len --> 10\n",
            "Result --> 3.2449199154039308 /Element --> 0304021506\n",
            "436\n",
            "436\n",
            "436\n",
            "******INDEX  438 *****\n",
            "Sum --> 20.40067969345788 /Len --> 10\n",
            "Result --> 2.040067969345788 /Element --> 0502020404\n",
            "437\n",
            "437\n",
            "437\n",
            "******INDEX  439 *****\n",
            "Sum --> 23.840475802310365 /Len --> 10\n",
            "Result --> 2.3840475802310364 /Element --> 0503010308\n",
            "438\n",
            "438\n",
            "438\n",
            "******INDEX  440 *****\n",
            "Sum --> 17.449199154039306 /Len --> 10\n",
            "Result --> 1.7449199154039305 /Element --> 0304030401\n",
            "439\n",
            "439\n",
            "439\n",
            "******INDEX  441 *****\n",
            "Sum --> 21.468564220941797 /Len --> 10\n",
            "Result --> 2.1468564220941797 /Element --> 0302030408\n",
            "440\n",
            "440\n",
            "******INDEX  442 *****\n",
            "Sum --> 38.206559507680495 /Len --> 8\n",
            "Result --> 4.775819938460062 /Element --> 08041305\n",
            "441\n",
            "441\n",
            "441\n",
            "******INDEX  443 *****\n",
            "Sum --> 24.448178044953053 /Len --> 10\n",
            "Result --> 2.4448178044953055 /Element --> 0705030101\n",
            "442\n",
            "******INDEX  444 *****\n",
            "Sum --> 9.468564220941797 /Len --> 6\n",
            "Result --> 1.578094036823633 /Element --> 030203\n",
            "443\n",
            "443\n",
            "443\n",
            "******INDEX  445 *****\n",
            "Sum --> 25.38131462655539 /Len --> 10\n",
            "Result --> 2.538131462655539 /Element --> 0504030207\n",
            "444\n",
            "444\n",
            "444\n",
            "******INDEX  446 *****\n",
            "Sum --> 41.23065479278037 /Len --> 10\n",
            "Result --> 4.123065479278037 /Element --> 1004060604\n",
            "445\n",
            "445\n",
            "445\n",
            "******INDEX  447 *****\n",
            "Sum --> 40.51434235122045 /Len --> 10\n",
            "Result --> 4.051434235122045 /Element --> 0806080405\n",
            "446\n",
            "446\n",
            "446\n",
            "******INDEX  448 *****\n",
            "Sum --> 56.23722932169647 /Len --> 10\n",
            "Result --> 5.623722932169647 /Element --> 0609081509\n",
            "447\n",
            "447\n",
            "447\n",
            "******INDEX  449 *****\n",
            "Sum --> 20.0 /Len --> 10\n",
            "Result --> 2.0 /Element --> 0101020808\n",
            "448\n",
            "******INDEX  450 *****\n",
            "Sum --> 33.25893999575079 /Len --> 6\n",
            "Result --> 5.543156665958464 /Element --> 080708\n",
            "449\n",
            "449\n",
            "449\n",
            "******INDEX  451 *****\n",
            "Sum --> 17.468564220941797 /Len --> 10\n",
            "Result --> 1.7468564220941798 /Element --> 0302020207\n",
            "450\n",
            "450\n",
            "450\n",
            "******INDEX  452 *****\n",
            "Sum --> 28.840475802310365 /Len --> 10\n",
            "Result --> 2.8840475802310364 /Element --> 0503080603\n",
            "451\n",
            "451\n",
            "451\n",
            "******INDEX  453 *****\n",
            "Sum --> 30.297396709994068 /Len --> 10\n",
            "Result --> 3.029739670999407 /Element --> 0102101007\n",
            "452\n",
            "452\n",
            "******INDEX  454 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0109\n",
            "453\n",
            "453\n",
            "******INDEX  455 *****\n",
            "Sum --> 16.5680158664523 /Len --> 8\n",
            "Result --> 2.0710019833065374 /Element --> 06020103\n",
            "454\n",
            "******INDEX  456 *****\n",
            "Sum --> 18.928527864588922 /Len --> 6\n",
            "Result --> 3.1547546440981535 /Element --> 080103\n",
            "455\n",
            "******INDEX  457 *****\n",
            "Sum --> 19.433695114625678 /Len --> 6\n",
            "Result --> 3.2389491857709465 /Element --> 050701\n",
            "456\n",
            "******INDEX  458 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "457\n",
            "457\n",
            "457\n",
            "******INDEX  459 *****\n",
            "Sum --> 25.90836032979428 /Len --> 10\n",
            "Result --> 2.590836032979428 /Element --> 0303030708\n",
            "458\n",
            "458\n",
            "458\n",
            "******INDEX  460 *****\n",
            "Sum --> 19.17116751094773 /Len --> 10\n",
            "Result --> 1.9171167510947729 /Element --> 0301080501\n",
            "459\n",
            "459\n",
            "459\n",
            "******INDEX  461 *****\n",
            "Sum --> 18.898648307306075 /Len --> 10\n",
            "Result --> 1.8898648307306076 /Element --> 0105020504\n",
            "460\n",
            "460\n",
            "460\n",
            "******INDEX  462 *****\n",
            "Sum --> 21.278031643091577 /Len --> 10\n",
            "Result --> 2.1278031643091575 /Element --> 0104050406\n",
            "461\n",
            "461\n",
            "461\n",
            "******INDEX  463 *****\n",
            "Sum --> 14.29739670999407 /Len --> 10\n",
            "Result --> 1.429739670999407 /Element --> 0102060104\n",
            "462\n",
            "462\n",
            "462\n",
            "******INDEX  464 *****\n",
            "Sum --> 18.103282983463814 /Len --> 10\n",
            "Result --> 1.8103282983463813 /Element --> 0501060102\n",
            "463\n",
            "463\n",
            "463\n",
            "******INDEX  465 *****\n",
            "Sum --> 8.29739670999407 /Len --> 10\n",
            "Result --> 0.8297396709994069 /Element --> 0102010202\n",
            "464\n",
            "464\n",
            "464\n",
            "******INDEX  466 *****\n",
            "Sum --> 22.90836032979428 /Len --> 10\n",
            "Result --> 2.290836032979428 /Element --> 0303030507\n",
            "465\n",
            "465\n",
            "465\n",
            "******INDEX  467 *****\n",
            "Sum --> 27.73719281884655 /Len --> 10\n",
            "Result --> 2.773719281884655 /Element --> 0103050513\n",
            "466\n",
            "466\n",
            "466\n",
            "******INDEX  468 *****\n",
            "Sum --> 18.103282983463814 /Len --> 10\n",
            "Result --> 1.8103282983463813 /Element --> 0501030105\n",
            "467\n",
            "467\n",
            "467\n",
            "******INDEX  469 *****\n",
            "Sum --> 36.676670047477444 /Len --> 10\n",
            "Result --> 3.667667004747744 /Element --> 0904030110\n",
            "468\n",
            "468\n",
            "468\n",
            "******INDEX  470 *****\n",
            "Sum --> 33.44817804495305 /Len --> 10\n",
            "Result --> 3.3448178044953054 /Element --> 0705050504\n",
            "469\n",
            "469\n",
            "469\n",
            "******INDEX  471 *****\n",
            "Sum --> 26.856433643089765 /Len --> 10\n",
            "Result --> 2.6856433643089765 /Element --> 0606030401\n",
            "470\n",
            "470\n",
            "470\n",
            "******INDEX  472 *****\n",
            "Sum --> 28.229015515546998 /Len --> 10\n",
            "Result --> 2.8229015515546996 /Element --> 0508040202\n",
            "471\n",
            "471\n",
            "471\n",
            "******INDEX  473 *****\n",
            "Sum --> 21.17116751094773 /Len --> 10\n",
            "Result --> 2.117116751094773 /Element --> 0301010708\n",
            "472\n",
            "472\n",
            "472\n",
            "******INDEX  474 *****\n",
            "Sum --> 29.29728671169194 /Len --> 10\n",
            "Result --> 2.929728671169194 /Element --> 0905020201\n",
            "473\n",
            "473\n",
            "473\n",
            "******INDEX  475 *****\n",
            "Sum --> 46.64213734494145 /Len --> 10\n",
            "Result --> 4.664213734494146 /Element --> 0813020107\n",
            "474\n",
            "474\n",
            "474\n",
            "******INDEX  476 *****\n",
            "Sum --> 11.0 /Len --> 10\n",
            "Result --> 1.1 /Element --> 0101050202\n",
            "475\n",
            "475\n",
            "475\n",
            "******INDEX  477 *****\n",
            "Sum --> 19.0 /Len --> 10\n",
            "Result --> 1.9 /Element --> 0101040310\n",
            "476\n",
            "476\n",
            "476\n",
            "******INDEX  478 *****\n",
            "Sum --> 40.50157964210959 /Len --> 10\n",
            "Result --> 4.050157964210959 /Element --> 0307041210\n",
            "478\n",
            "478\n",
            "******INDEX  480 *****\n",
            "Sum --> 15 /Len --> 4\n",
            "Result --> 3.75 /Element --> 0708\n",
            "479\n",
            "479\n",
            "479\n",
            "******INDEX  481 *****\n",
            "Sum --> 42.69786479281288 /Len --> 10\n",
            "Result --> 4.269786479281288 /Element --> 0811040105\n",
            "480\n",
            "480\n",
            "480\n",
            "******INDEX  482 *****\n",
            "Sum --> 29.103282983463814 /Len --> 10\n",
            "Result --> 2.910328298346381 /Element --> 0501080705\n",
            "481\n",
            "481\n",
            "481\n",
            "******INDEX  483 *****\n",
            "Sum --> 23.062866266041592 /Len --> 10\n",
            "Result --> 2.3062866266041593 /Element --> 0401040903\n",
            "482\n",
            "482\n",
            "482\n",
            "******INDEX  484 *****\n",
            "Sum --> 19.73719281884655 /Len --> 10\n",
            "Result --> 1.9737192818846552 /Element --> 0103020508\n",
            "485\n",
            "485\n",
            "******INDEX  487 *****\n",
            "Sum --> 15.740320469781409 /Len --> 8\n",
            "Result --> 1.9675400587226761 /Element --> 02040305\n",
            "486\n",
            "486\n",
            "******INDEX  488 *****\n",
            "Sum --> 13 /Len --> 4\n",
            "Result --> 3.25 /Element --> 0508\n",
            "487\n",
            "487\n",
            "******INDEX  489 *****\n",
            "Sum --> 11 /Len --> 4\n",
            "Result --> 2.75 /Element --> 0506\n",
            "489\n",
            "******INDEX  491 *****\n",
            "Sum --> 5 /Len --> 2\n",
            "Result --> 2.5 /Element --> 05\n",
            "491\n",
            "491\n",
            "******INDEX  493 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0208\n",
            "492\n",
            "******INDEX  494 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "493\n",
            "******INDEX  495 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "494\n",
            "******INDEX  496 *****\n",
            "Sum --> 15.961514573347667 /Len --> 6\n",
            "Result --> 2.660252428891278 /Element --> 040503\n",
            "495\n",
            "******INDEX  497 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "496\n",
            "******INDEX  498 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "497\n",
            "******INDEX  499 *****\n",
            "Sum --> 12 /Len --> 2\n",
            "Result --> 6.0 /Element --> 12\n",
            "499\n",
            "499\n",
            "499\n",
            "******INDEX  501 *****\n",
            "Sum --> 28.00193129076989 /Len --> 10\n",
            "Result --> 2.800193129076989 /Element --> 0505030505\n",
            "500\n",
            "500\n",
            "500\n",
            "******INDEX  502 *****\n",
            "Sum --> 27.38131462655539 /Len --> 10\n",
            "Result --> 2.738131462655539 /Element --> 0504030803\n",
            "501\n",
            "501\n",
            "501\n",
            "******INDEX  503 *****\n",
            "Sum --> 32.13534422427851 /Len --> 10\n",
            "Result --> 3.213534422427851 /Element --> 0706020405\n",
            "502\n",
            "502\n",
            "502\n",
            "******INDEX  504 *****\n",
            "Sum --> 39.398461662258114 /Len --> 10\n",
            "Result --> 3.9398461662258115 /Element --> 0710030404\n",
            "503\n",
            "503\n",
            "503\n",
            "******INDEX  505 *****\n",
            "Sum --> 22.759685536683904 /Len --> 10\n",
            "Result --> 2.2759685536683905 /Element --> 0202010413\n",
            "504\n",
            "504\n",
            "504\n",
            "******INDEX  506 *****\n",
            "Sum --> 24.501579642109593 /Len --> 10\n",
            "Result --> 2.4501579642109594 /Element --> 0307040105\n",
            "505\n",
            "505\n",
            "505\n",
            "******INDEX  507 *****\n",
            "Sum --> 15.0 /Len --> 10\n",
            "Result --> 1.5 /Element --> 0101030703\n",
            "506\n",
            "506\n",
            "506\n",
            "******INDEX  508 *****\n",
            "Sum --> 15.740320469781409 /Len --> 10\n",
            "Result --> 1.5740320469781408 /Element --> 0204030302\n",
            "507\n",
            "507\n",
            "507\n",
            "******INDEX  509 *****\n",
            "Sum --> 35.22592457458299 /Len --> 10\n",
            "Result --> 3.522592457458299 /Element --> 0802030708\n",
            "508\n",
            "508\n",
            "508\n",
            "******INDEX  510 *****\n",
            "Sum --> 37.250019859682865 /Len --> 10\n",
            "Result --> 3.7250019859682864 /Element --> 1002040407\n",
            "509\n",
            "509\n",
            "509\n",
            "******INDEX  511 *****\n",
            "Sum --> 25.800059084888144 /Len --> 10\n",
            "Result --> 2.5800059084888143 /Element --> 0403030508\n",
            "510\n",
            "510\n",
            "******INDEX  512 *****\n",
            "Sum --> 45.17031503687882 /Len --> 8\n",
            "Result --> 5.646289379609852 /Element --> 11060806\n",
            "511\n",
            "511\n",
            "511\n",
            "******INDEX  513 *****\n",
            "Sum --> 32.04810331332136 /Len --> 10\n",
            "Result --> 3.204810331332136 /Element --> 0206050610\n",
            "512\n",
            "512\n",
            "512\n",
            "******INDEX  514 *****\n",
            "Sum --> 44.07835568177198 /Len --> 10\n",
            "Result --> 4.407835568177198 /Element --> 1008020406\n",
            "513\n",
            "******INDEX  515 *****\n",
            "Sum --> 31.601031287620096 /Len --> 6\n",
            "Result --> 5.266838547936683 /Element --> 060711\n",
            "514\n",
            "514\n",
            "514\n",
            "******INDEX  516 *****\n",
            "Sum --> 34.74032046978141 /Len --> 10\n",
            "Result --> 3.4740320469781407 /Element --> 0204150606\n",
            "515\n",
            "515\n",
            "515\n",
            "******INDEX  517 *****\n",
            "Sum --> 25.0698158182538 /Len --> 10\n",
            "Result --> 2.50698158182538 /Element --> 0305020606\n",
            "516\n",
            "516\n",
            "516\n",
            "******INDEX  518 *****\n",
            "Sum --> 47.862532193338865 /Len --> 10\n",
            "Result --> 4.7862532193338865 /Element --> 1104010811\n",
            "517\n",
            "517\n",
            "517\n",
            "******INDEX  519 *****\n",
            "Sum --> 13.103282983463814 /Len --> 10\n",
            "Result --> 1.3103282983463813 /Element --> 0501020101\n",
            "518\n",
            "518\n",
            "518\n",
            "******INDEX  520 *****\n",
            "Sum --> 63.08875949106699 /Len --> 10\n",
            "Result --> 6.308875949106699 /Element --> 1607030409\n",
            "519\n",
            "519\n",
            "519\n",
            "******INDEX  521 *****\n",
            "Sum --> 19.468564220941797 /Len --> 10\n",
            "Result --> 1.9468564220941798 /Element --> 0302020803\n",
            "520\n",
            "520\n",
            "520\n",
            "******INDEX  522 *****\n",
            "Sum --> 14.740320469781409 /Len --> 10\n",
            "Result --> 1.474032046978141 /Element --> 0204030103\n",
            "521\n",
            "521\n",
            "521\n",
            "******INDEX  523 *****\n",
            "Sum --> 17.800059084888144 /Len --> 10\n",
            "Result --> 1.7800059084888145 /Element --> 0403010403\n",
            "522\n",
            "522\n",
            "522\n",
            "******INDEX  524 *****\n",
            "Sum --> 23.468564220941797 /Len --> 10\n",
            "Result --> 2.34685642209418 /Element --> 0302030608\n",
            "523\n",
            "523\n",
            "523\n",
            "******INDEX  525 *****\n",
            "Sum --> 24.27061915645823 /Len --> 10\n",
            "Result --> 2.4270619156458233 /Element --> 0601030208\n",
            "524\n",
            "524\n",
            "524\n",
            "******INDEX  526 *****\n",
            "Sum --> 20.0 /Len --> 10\n",
            "Result --> 2.0 /Element --> 0101021006\n",
            "525\n",
            "525\n",
            "525\n",
            "******INDEX  527 *****\n",
            "Sum --> 18.17116751094773 /Len --> 10\n",
            "Result --> 1.817116751094773 /Element --> 0301020506\n",
            "526\n",
            "526\n",
            "526\n",
            "******INDEX  528 *****\n",
            "Sum --> 27.5680158664523 /Len --> 10\n",
            "Result --> 2.75680158664523 /Element --> 0602020310\n",
            "527\n",
            "527\n",
            "527\n",
            "******INDEX  529 *****\n",
            "Sum --> 49.648314464112076 /Len --> 10\n",
            "Result --> 4.964831446411208 /Element --> 1306010804\n",
            "528\n",
            "528\n",
            "528\n",
            "******INDEX  530 *****\n",
            "Sum --> 26.548650799549808 /Len --> 10\n",
            "Result --> 2.654865079954981 /Element --> 0604010307\n",
            "529\n",
            "529\n",
            "529\n",
            "******INDEX  531 *****\n",
            "Sum --> 33.68909747009535 /Len --> 10\n",
            "Result --> 3.3689097470095346 /Element --> 0506060902\n",
            "530\n",
            "530\n",
            "530\n",
            "******INDEX  532 *****\n",
            "Sum --> 35.206559507680495 /Len --> 10\n",
            "Result --> 3.5206559507680497 /Element --> 0804010707\n",
            "531\n",
            "531\n",
            "531\n",
            "******INDEX  533 *****\n",
            "Sum --> 24.206559507680495 /Len --> 10\n",
            "Result --> 2.4206559507680496 /Element --> 0804010102\n",
            "532\n",
            "532\n",
            "532\n",
            "******INDEX  534 *****\n",
            "Sum --> 28.34089790913317 /Len --> 10\n",
            "Result --> 2.8340897909133167 /Element --> 0404060803\n",
            "533\n",
            "533\n",
            "533\n",
            "******INDEX  535 *****\n",
            "Sum --> 31.398638404385867 /Len --> 10\n",
            "Result --> 3.139863840438587 /Element --> 0901050503\n",
            "534\n",
            "534\n",
            "534\n",
            "******INDEX  536 *****\n",
            "Sum --> 43.7878882202483 /Len --> 10\n",
            "Result --> 4.37878882202483 /Element --> 0412060606\n",
            "535\n",
            "535\n",
            "535\n",
            "******INDEX  537 *****\n",
            "Sum --> 17.462288826689832 /Len --> 10\n",
            "Result --> 1.7462288826689831 /Element --> 0201080501\n",
            "536\n",
            "536\n",
            "536\n",
            "******INDEX  538 *****\n",
            "Sum --> 39.676670047477444 /Len --> 10\n",
            "Result --> 3.9676670047477445 /Element --> 0904030410\n",
            "537\n",
            "537\n",
            "******INDEX  539 *****\n",
            "Sum --> 26.169267463764307 /Len --> 8\n",
            "Result --> 3.2711584329705383 /Element --> 06050801\n",
            "538\n",
            "538\n",
            "538\n",
            "******INDEX  540 *****\n",
            "Sum --> 14.199481645536384 /Len --> 10\n",
            "Result --> 1.4199481645536385 /Element --> 0203030302\n",
            "539\n",
            "539\n",
            "539\n",
            "******INDEX  541 *****\n",
            "Sum --> 9.462288826689832 /Len --> 10\n",
            "Result --> 0.9462288826689832 /Element --> 0201010203\n",
            "540\n",
            "540\n",
            "******INDEX  542 *****\n",
            "Sum --> 103.7596855366839 /Len --> 8\n",
            "Result --> 12.969960692085488 /Element --> 02029702\n",
            "541\n",
            "541\n",
            "541\n",
            "******INDEX  543 *****\n",
            "Sum --> 24.501579642109593 /Len --> 10\n",
            "Result --> 2.4501579642109594 /Element --> 0307010702\n",
            "542\n",
            "542\n",
            "542\n",
            "******INDEX  544 *****\n",
            "Sum --> 44.800150712639834 /Len --> 10\n",
            "Result --> 4.480015071263983 /Element --> 1501010801\n",
            "543\n",
            "543\n",
            "543\n",
            "******INDEX  545 *****\n",
            "Sum --> 16.0 /Len --> 10\n",
            "Result --> 1.6 /Element --> 0101070205\n",
            "544\n",
            "544\n",
            "544\n",
            "******INDEX  546 *****\n",
            "Sum --> 10.278031643091577 /Len --> 10\n",
            "Result --> 1.0278031643091576 /Element --> 0104010201\n",
            "545\n",
            "545\n",
            "545\n",
            "******INDEX  547 *****\n",
            "Sum --> 37.29690004303092 /Len --> 10\n",
            "Result --> 3.729690004303092 /Element --> 0308031107\n",
            "546\n",
            "546\n",
            "546\n",
            "******INDEX  548 *****\n",
            "Sum --> 7.0 /Len --> 10\n",
            "Result --> 0.7 /Element --> 0101010202\n",
            "547\n",
            "547\n",
            "547\n",
            "******INDEX  549 *****\n",
            "Sum --> 17.0698158182538 /Len --> 10\n",
            "Result --> 1.7069815818253802 /Element --> 0305020301\n",
            "548\n",
            "548\n",
            "548\n",
            "******INDEX  550 *****\n",
            "Sum --> 29.90836032979428 /Len --> 10\n",
            "Result --> 2.990836032979428 /Element --> 0303020614\n",
            "549\n",
            "******INDEX  551 *****\n",
            "Sum --> 14.90836032979428 /Len --> 6\n",
            "Result --> 2.4847267216323803 /Element --> 030307\n",
            "550\n",
            "550\n",
            "550\n",
            "******INDEX  552 *****\n",
            "Sum --> 13.737192818846552 /Len --> 10\n",
            "Result --> 1.373719281884655 /Element --> 0103040401\n",
            "551\n",
            "551\n",
            "551\n",
            "******INDEX  553 *****\n",
            "Sum --> 22.17116751094773 /Len --> 10\n",
            "Result --> 2.217116751094773 /Element --> 0301020609\n",
            "552\n",
            "552\n",
            "552\n",
            "******INDEX  554 *****\n",
            "Sum --> 11.171167510947729 /Len --> 10\n",
            "Result --> 1.1171167510947728 /Element --> 0301040101\n",
            "553\n",
            "553\n",
            "553\n",
            "******INDEX  555 *****\n",
            "Sum --> 13.29739670999407 /Len --> 10\n",
            "Result --> 1.329739670999407 /Element --> 0102040303\n",
            "554\n",
            "554\n",
            "554\n",
            "******INDEX  556 *****\n",
            "Sum --> 19.449199154039306 /Len --> 10\n",
            "Result --> 1.9449199154039305 /Element --> 0304020305\n",
            "555\n",
            "555\n",
            "555\n",
            "******INDEX  557 *****\n",
            "Sum --> 15.800059084888144 /Len --> 10\n",
            "Result --> 1.5800059084888143 /Element --> 0403040101\n",
            "556\n",
            "556\n",
            "556\n",
            "******INDEX  558 *****\n",
            "Sum --> 41.84692644764105 /Len --> 10\n",
            "Result --> 4.184692644764105 /Element --> 0702110709\n",
            "557\n",
            "557\n",
            "557\n",
            "******INDEX  559 *****\n",
            "Sum --> 26.27061915645823 /Len --> 10\n",
            "Result --> 2.627061915645823 /Element --> 0601050901\n",
            "558\n",
            "558\n",
            "558\n",
            "******INDEX  560 *****\n",
            "Sum --> 16.462288826689832 /Len --> 10\n",
            "Result --> 1.6462288826689833 /Element --> 0201010507\n",
            "559\n",
            "559\n",
            "559\n",
            "******INDEX  561 *****\n",
            "Sum --> 29.961514573347667 /Len --> 10\n",
            "Result --> 2.9961514573347667 /Element --> 0405080108\n",
            "560\n",
            "560\n",
            "560\n",
            "******INDEX  562 *****\n",
            "Sum --> 36.27061915645823 /Len --> 10\n",
            "Result --> 3.627061915645823 /Element --> 0601120508\n",
            "561\n",
            "561\n",
            "******INDEX  563 *****\n",
            "Sum --> 21.665720683435474 /Len --> 8\n",
            "Result --> 2.7082150854294342 /Element --> 08030201\n",
            "562\n",
            "562\n",
            "******INDEX  564 *****\n",
            "Sum --> 30.13583122323242 /Len --> 8\n",
            "Result --> 3.7669789029040524 /Element --> 09030306\n",
            "563\n",
            "563\n",
            "563\n",
            "******INDEX  565 *****\n",
            "Sum --> 20.800059084888144 /Len --> 10\n",
            "Result --> 2.0800059084888143 /Element --> 0403020504\n",
            "564\n",
            "564\n",
            "564\n",
            "******INDEX  566 *****\n",
            "Sum --> 16.360262976035663 /Len --> 10\n",
            "Result --> 1.6360262976035664 /Element --> 0402030401\n",
            "565\n",
            "565\n",
            "565\n",
            "******INDEX  567 *****\n",
            "Sum --> 51.229015515547 /Len --> 10\n",
            "Result --> 5.122901551554699 /Element --> 0508091408\n",
            "566\n",
            "566\n",
            "******INDEX  568 *****\n",
            "Sum --> 13 /Len --> 4\n",
            "Result --> 3.25 /Element --> 0706\n",
            "567\n",
            "567\n",
            "******INDEX  569 *****\n",
            "Sum --> 4 /Len --> 4\n",
            "Result --> 1.0 /Element --> 0103\n",
            "569\n",
            "******INDEX  571 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "570\n",
            "******INDEX  572 *****\n",
            "Sum --> 12.29739670999407 /Len --> 6\n",
            "Result --> 2.049566118332345 /Element --> 010209\n",
            "572\n",
            "572\n",
            "******INDEX  574 *****\n",
            "Sum --> 16.17116751094773 /Len --> 8\n",
            "Result --> 2.021395938868466 /Element --> 03010407\n",
            "573\n",
            "573\n",
            "******INDEX  575 *****\n",
            "Sum --> 4 /Len --> 4\n",
            "Result --> 1.0 /Element --> 0103\n",
            "574\n",
            "******INDEX  576 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "575\n",
            "575\n",
            "******INDEX  577 *****\n",
            "Sum --> 18.928527864588922 /Len --> 8\n",
            "Result --> 2.3660659830736153 /Element --> 08010102\n",
            "576\n",
            "576\n",
            "******INDEX  578 *****\n",
            "Sum --> 7 /Len --> 4\n",
            "Result --> 1.75 /Element --> 0106\n",
            "577\n",
            "******INDEX  579 *****\n",
            "Sum --> 10.199481645536384 /Len --> 6\n",
            "Result --> 1.6999136075893972 /Element --> 020304\n",
            "578\n",
            "******INDEX  580 *****\n",
            "Sum --> 1 /Len --> 2\n",
            "Result --> 0.5 /Element --> 01\n",
            "579\n",
            "579\n",
            "******INDEX  581 *****\n",
            "Sum --> 8.462288826689832 /Len --> 8\n",
            "Result --> 1.057786103336229 /Element --> 02010302\n",
            "580\n",
            "******INDEX  582 *****\n",
            "Sum --> 6.462288826689832 /Len --> 6\n",
            "Result --> 1.0770481377816388 /Element --> 020103\n",
            "581\n",
            "******INDEX  583 *****\n",
            "Sum --> 19.00193129076989 /Len --> 6\n",
            "Result --> 3.166988548461648 /Element --> 050504\n",
            "582\n",
            "582\n",
            "******INDEX  584 *****\n",
            "Sum --> 10 /Len --> 4\n",
            "Result --> 2.5 /Element --> 0505\n",
            "583\n",
            "583\n",
            "******INDEX  585 *****\n",
            "Sum --> 11 /Len --> 4\n",
            "Result --> 2.75 /Element --> 0407\n",
            "584\n",
            "584\n",
            "584\n",
            "******INDEX  586 *****\n",
            "Sum --> 23.759685536683904 /Len --> 10\n",
            "Result --> 2.3759685536683905 /Element --> 0202110503\n",
            "585\n",
            "585\n",
            "585\n",
            "******INDEX  587 *****\n",
            "Sum --> 23.468564220941797 /Len --> 10\n",
            "Result --> 2.34685642209418 /Element --> 0302050210\n",
            "586\n",
            "586\n",
            "586\n",
            "******INDEX  588 *****\n",
            "Sum --> 22.360937133995904 /Len --> 10\n",
            "Result --> 2.2360937133995904 /Element --> 0205040801\n",
            "587\n",
            "587\n",
            "587\n",
            "******INDEX  589 *****\n",
            "Sum --> 27.22592457458299 /Len --> 10\n",
            "Result --> 2.722592457458299 /Element --> 0802020305\n",
            "588\n",
            "588\n",
            "588\n",
            "******INDEX  590 *****\n",
            "Sum --> 21.759685536683904 /Len --> 10\n",
            "Result --> 2.1759685536683904 /Element --> 0202040409\n",
            "589\n",
            "589\n",
            "589\n",
            "******INDEX  591 *****\n",
            "Sum --> 30.840475802310365 /Len --> 10\n",
            "Result --> 3.0840475802310365 /Element --> 0503060409\n",
            "590\n",
            "590\n",
            "590\n",
            "******INDEX  592 *****\n",
            "Sum --> 33.548650799549804 /Len --> 10\n",
            "Result --> 3.3548650799549806 /Element --> 0604030609\n",
            "591\n",
            "591\n",
            "591\n",
            "******INDEX  593 *****\n",
            "Sum --> 14.062866266041592 /Len --> 10\n",
            "Result --> 1.4062866266041592 /Element --> 0401050101\n",
            "592\n",
            "******INDEX  594 *****\n",
            "Sum --> 29.5680158664523 /Len --> 6\n",
            "Result --> 4.928002644408717 /Element --> 060217\n",
            "593\n",
            "593\n",
            "593\n",
            "******INDEX  595 *****\n",
            "Sum --> 25.648680752673126 /Len --> 10\n",
            "Result --> 2.5648680752673125 /Element --> 0406060401\n",
            "594\n",
            "594\n",
            "594\n",
            "******INDEX  596 *****\n",
            "Sum --> 26.169267463764307 /Len --> 10\n",
            "Result --> 2.6169267463764307 /Element --> 0605010701\n",
            "595\n",
            "595\n",
            "595\n",
            "******INDEX  597 *****\n",
            "Sum --> 16.40067969345788 /Len --> 10\n",
            "Result --> 1.6400679693457882 /Element --> 0502020301\n",
            "596\n",
            "596\n",
            "596\n",
            "******INDEX  598 *****\n",
            "Sum --> 44.7878882202483 /Len --> 10\n",
            "Result --> 4.47878882202483 /Element --> 0412100405\n",
            "597\n",
            "597\n",
            "******INDEX  599 *****\n",
            "Sum --> 4 /Len --> 4\n",
            "Result --> 1.0 /Element --> 0103\n",
            "598\n",
            "598\n",
            "598\n",
            "******INDEX  600 *****\n",
            "Sum --> 26.54952973764698 /Len --> 10\n",
            "Result --> 2.654952973764698 /Element --> 0701010903\n",
            "599\n",
            "599\n",
            "599\n",
            "******INDEX  601 *****\n",
            "Sum --> 7.462288826689832 /Len --> 10\n",
            "Result --> 0.7462288826689832 /Element --> 0201020101\n",
            "600\n",
            "600\n",
            "600\n",
            "******INDEX  602 *****\n",
            "Sum --> 22.449199154039306 /Len --> 10\n",
            "Result --> 2.2449199154039308 /Element --> 0304020209\n",
            "601\n",
            "601\n",
            "601\n",
            "******INDEX  603 *****\n",
            "Sum --> 29.911798190652725 /Len --> 10\n",
            "Result --> 2.9911798190652723 /Element --> 0410030401\n",
            "602\n",
            "602\n",
            "602\n",
            "******INDEX  604 *****\n",
            "Sum --> 21.846926447641053 /Len --> 10\n",
            "Result --> 2.1846926447641053 /Element --> 0702050101\n",
            "603\n",
            "603\n",
            "603\n",
            "******INDEX  605 *****\n",
            "Sum --> 122.50157964210959 /Len --> 10\n",
            "Result --> 12.25015796421096 /Element --> 0307079704\n",
            "604\n",
            "604\n",
            "604\n",
            "******INDEX  606 *****\n",
            "Sum --> 15.278031643091577 /Len --> 10\n",
            "Result --> 1.5278031643091576 /Element --> 0104060201\n",
            "605\n",
            "605\n",
            "605\n",
            "******INDEX  607 *****\n",
            "Sum --> 13.759685536683902 /Len --> 10\n",
            "Result --> 1.37596855366839 /Element --> 0202040203\n",
            "606\n",
            "606\n",
            "606\n",
            "******INDEX  608 *****\n",
            "Sum --> 22.840475802310365 /Len --> 10\n",
            "Result --> 2.2840475802310367 /Element --> 0503040106\n",
            "607\n",
            "607\n",
            "607\n",
            "******INDEX  609 *****\n",
            "Sum --> 265.4438195344706 /Len --> 10\n",
            "Result --> 26.54438195344706 /Element --> 0697040504\n",
            "608\n",
            "******INDEX  610 *****\n",
            "Sum --> 9 /Len --> 2\n",
            "Result --> 4.5 /Element --> 09\n",
            "609\n",
            "******INDEX  611 *****\n",
            "Sum --> 4 /Len --> 2\n",
            "Result --> 2.0 /Element --> 04\n",
            "610\n",
            "******INDEX  612 *****\n",
            "Sum --> 17.048103313321363 /Len --> 6\n",
            "Result --> 2.841350552220227 /Element --> 020606\n",
            "611\n",
            "******INDEX  613 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "612\n",
            "612\n",
            "******INDEX  614 *****\n",
            "Sum --> 12 /Len --> 4\n",
            "Result --> 3.0 /Element --> 0606\n",
            "613\n",
            "******INDEX  615 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "614\n",
            "614\n",
            "******INDEX  616 *****\n",
            "Sum --> 15 /Len --> 4\n",
            "Result --> 3.75 /Element --> 0708\n",
            "616\n",
            "******INDEX  618 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "617\n",
            "******INDEX  619 *****\n",
            "Sum --> 5 /Len --> 2\n",
            "Result --> 2.5 /Element --> 05\n",
            "619\n",
            "******INDEX  621 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "634\n",
            "634\n",
            "634\n",
            "******INDEX  636 *****\n",
            "Sum --> 17.34089790913317 /Len --> 10\n",
            "Result --> 1.7340897909133168 /Element --> 0404020202\n",
            "635\n",
            "635\n",
            "******INDEX  637 *****\n",
            "Sum --> 10.278031643091577 /Len --> 8\n",
            "Result --> 1.284753955386447 /Element --> 01040103\n",
            "636\n",
            "636\n",
            "636\n",
            "******INDEX  638 *****\n",
            "Sum --> 14.462288826689832 /Len --> 10\n",
            "Result --> 1.4462288826689833 /Element --> 0201010208\n",
            "637\n",
            "637\n",
            "637\n",
            "******INDEX  639 *****\n",
            "Sum --> 13.449199154039306 /Len --> 10\n",
            "Result --> 1.3449199154039306 /Element --> 0304010102\n",
            "638\n",
            "638\n",
            "638\n",
            "******INDEX  640 *****\n",
            "Sum --> 22.40067969345788 /Len --> 10\n",
            "Result --> 2.2400679693457883 /Element --> 0502050601\n",
            "639\n",
            "639\n",
            "639\n",
            "******INDEX  641 *****\n",
            "Sum --> 21.007811975304783 /Len --> 10\n",
            "Result --> 2.100781197530478 /Element --> 0603040102\n",
            "640\n",
            "640\n",
            "640\n",
            "******INDEX  642 *****\n",
            "Sum --> 25.00193129076989 /Len --> 10\n",
            "Result --> 2.500193129076989 /Element --> 0505050401\n",
            "641\n",
            "******INDEX  643 *****\n",
            "Sum --> 8 /Len --> 2\n",
            "Result --> 4.0 /Element --> 08\n",
            "642\n",
            "******INDEX  644 *****\n",
            "Sum --> 7 /Len --> 2\n",
            "Result --> 3.5 /Element --> 07\n",
            "643\n",
            "643\n",
            "643\n",
            "******INDEX  645 *****\n",
            "Sum --> 15.759685536683902 /Len --> 10\n",
            "Result --> 1.5759685536683903 /Element --> 0202010802\n",
            "644\n",
            "644\n",
            "644\n",
            "******INDEX  646 *****\n",
            "Sum --> 14.400679693457883 /Len --> 10\n",
            "Result --> 1.4400679693457883 /Element --> 0502020101\n",
            "645\n",
            "645\n",
            "645\n",
            "******INDEX  647 *****\n",
            "Sum --> 10.278031643091577 /Len --> 10\n",
            "Result --> 1.0278031643091576 /Element --> 0104010102\n",
            "646\n",
            "646\n",
            "646\n",
            "******INDEX  648 *****\n",
            "Sum --> 15.171167510947729 /Len --> 10\n",
            "Result --> 1.517116751094773 /Element --> 0301040303\n",
            "647\n",
            "647\n",
            "647\n",
            "******INDEX  649 *****\n",
            "Sum --> 26.75698199757926 /Len --> 10\n",
            "Result --> 2.675698199757926 /Element --> 0306050504\n",
            "648\n",
            "648\n",
            "648\n",
            "******INDEX  650 *****\n",
            "Sum --> 21.898648307306075 /Len --> 10\n",
            "Result --> 2.1898648307306074 /Element --> 0105090104\n",
            "649\n",
            "649\n",
            "649\n",
            "******INDEX  651 *****\n",
            "Sum --> 39.23162575491379 /Len --> 10\n",
            "Result --> 3.9231625754913795 /Element --> 0211070903\n",
            "650\n",
            "650\n",
            "650\n",
            "******INDEX  652 *****\n",
            "Sum --> 28.199481645536384 /Len --> 10\n",
            "Result --> 2.8199481645536384 /Element --> 0203070807\n",
            "651\n",
            "651\n",
            "651\n",
            "******INDEX  653 *****\n",
            "Sum --> 24.501579642109593 /Len --> 10\n",
            "Result --> 2.4501579642109594 /Element --> 0307030502\n",
            "652\n",
            "652\n",
            "652\n",
            "******INDEX  654 *****\n",
            "Sum --> 21.0 /Len --> 10\n",
            "Result --> 2.1 /Element --> 0101060409\n",
            "653\n",
            "653\n",
            "653\n",
            "******INDEX  655 *****\n",
            "Sum --> 39.062866266041596 /Len --> 10\n",
            "Result --> 3.9062866266041594 /Element --> 0401081113\n",
            "654\n",
            "654\n",
            "654\n",
            "******INDEX  656 *****\n",
            "Sum --> 41.054260396672106 /Len --> 10\n",
            "Result --> 4.10542603966721 /Element --> 0808080105\n",
            "655\n",
            "655\n",
            "655\n",
            "******INDEX  657 *****\n",
            "Sum --> 13.29739670999407 /Len --> 10\n",
            "Result --> 1.329739670999407 /Element --> 0102010801\n",
            "656\n",
            "656\n",
            "656\n",
            "******INDEX  658 *****\n",
            "Sum --> 10.0 /Len --> 10\n",
            "Result --> 1.0 /Element --> 0101040301\n",
            "657\n",
            "657\n",
            "657\n",
            "******INDEX  659 *****\n",
            "Sum --> 41.247570328997 /Len --> 10\n",
            "Result --> 4.1247570328997 /Element --> 0910030203\n",
            "658\n",
            "658\n",
            "658\n",
            "******INDEX  660 *****\n",
            "Sum --> 16.74032046978141 /Len --> 10\n",
            "Result --> 1.674032046978141 /Element --> 0204040302\n",
            "659\n",
            "659\n",
            "659\n",
            "******INDEX  661 *****\n",
            "Sum --> 21.74032046978141 /Len --> 10\n",
            "Result --> 2.174032046978141 /Element --> 0204070106\n",
            "660\n",
            "660\n",
            "660\n",
            "******INDEX  662 *****\n",
            "Sum --> 36.88189726024136 /Len --> 10\n",
            "Result --> 3.688189726024136 /Element --> 1102010209\n",
            "661\n",
            "661\n",
            "******INDEX  663 *****\n",
            "Sum --> 12 /Len --> 4\n",
            "Result --> 3.0 /Element --> 0705\n",
            "662\n",
            "662\n",
            "662\n",
            "******INDEX  664 *****\n",
            "Sum --> 27.360937133995904 /Len --> 10\n",
            "Result --> 2.7360937133995904 /Element --> 0205080505\n",
            "663\n",
            "663\n",
            "******INDEX  665 *****\n",
            "Sum --> 25.449199154039306 /Len --> 8\n",
            "Result --> 3.1811498942549132 /Element --> 03040610\n",
            "664\n",
            "664\n",
            "664\n",
            "******INDEX  666 *****\n",
            "Sum --> 25.82756138073856 /Len --> 10\n",
            "Result --> 2.5827561380738557 /Element --> 0704010502\n",
            "665\n",
            "665\n",
            "665\n",
            "******INDEX  667 *****\n",
            "Sum --> 27.90836032979428 /Len --> 10\n",
            "Result --> 2.790836032979428 /Element --> 0303050807\n",
            "666\n",
            "666\n",
            "666\n",
            "******INDEX  668 *****\n",
            "Sum --> 27.840475802310365 /Len --> 10\n",
            "Result --> 2.7840475802310367 /Element --> 0503050506\n",
            "667\n",
            "667\n",
            "667\n",
            "******INDEX  669 *****\n",
            "Sum --> 41.25893999575079 /Len --> 10\n",
            "Result --> 4.125893999575078 /Element --> 0807070702\n",
            "668\n",
            "668\n",
            "668\n",
            "******INDEX  670 *****\n",
            "Sum --> 22.278031643091577 /Len --> 10\n",
            "Result --> 2.2278031643091576 /Element --> 0104030508\n",
            "669\n",
            "669\n",
            "669\n",
            "******INDEX  671 *****\n",
            "Sum --> 35.43369511462568 /Len --> 10\n",
            "Result --> 3.5433695114625676 /Element --> 0507040409\n",
            "670\n",
            "670\n",
            "670\n",
            "******INDEX  672 *****\n",
            "Sum --> 55.23722932169647 /Len --> 10\n",
            "Result --> 5.523722932169647 /Element --> 0609160609\n",
            "671\n",
            "671\n",
            "671\n",
            "******INDEX  673 *****\n",
            "Sum --> 23.74032046978141 /Len --> 10\n",
            "Result --> 2.374032046978141 /Element --> 0204040705\n",
            "672\n",
            "672\n",
            "672\n",
            "******INDEX  674 *****\n",
            "Sum --> 26.449199154039306 /Len --> 10\n",
            "Result --> 2.6449199154039307 /Element --> 0304090602\n",
            "673\n",
            "673\n",
            "******INDEX  675 *****\n",
            "Sum --> 29.237229321696468 /Len --> 8\n",
            "Result --> 3.6546536652120585 /Element --> 06090401\n",
            "674\n",
            "674\n",
            "674\n",
            "******INDEX  676 *****\n",
            "Sum --> 24.34089790913317 /Len --> 10\n",
            "Result --> 2.434089790913317 /Element --> 0404070501\n",
            "675\n",
            "675\n",
            "675\n",
            "******INDEX  677 *****\n",
            "Sum --> 20.278031643091577 /Len --> 10\n",
            "Result --> 2.027803164309158 /Element --> 0104030308\n",
            "676\n",
            "676\n",
            "676\n",
            "******INDEX  678 *****\n",
            "Sum --> 39.16926746376431 /Len --> 10\n",
            "Result --> 3.9169267463764306 /Element --> 0605081103\n",
            "677\n",
            "677\n",
            "677\n",
            "******INDEX  679 *****\n",
            "Sum --> 9.0 /Len --> 10\n",
            "Result --> 0.9 /Element --> 0101030202\n",
            "678\n",
            "678\n",
            "678\n",
            "******INDEX  680 *****\n",
            "Sum --> 17.17116751094773 /Len --> 10\n",
            "Result --> 1.717116751094773 /Element --> 0301030306\n",
            "679\n",
            "679\n",
            "679\n",
            "******INDEX  681 *****\n",
            "Sum --> 22.103282983463814 /Len --> 10\n",
            "Result --> 2.2103282983463814 /Element --> 0501040405\n",
            "680\n",
            "680\n",
            "******INDEX  682 *****\n",
            "Sum --> 23.856433643089765 /Len --> 8\n",
            "Result --> 2.9820542053862207 /Element --> 06060401\n",
            "681\n",
            "681\n",
            "681\n",
            "******INDEX  683 *****\n",
            "Sum --> 32.00193129076989 /Len --> 10\n",
            "Result --> 3.2001931290769887 /Element --> 0505021005\n",
            "682\n",
            "682\n",
            "******INDEX  684 *****\n",
            "Sum --> 18 /Len --> 4\n",
            "Result --> 4.5 /Element --> 0711\n",
            "683\n",
            "683\n",
            "683\n",
            "******INDEX  685 *****\n",
            "Sum --> 16.449199154039306 /Len --> 10\n",
            "Result --> 1.6449199154039307 /Element --> 0304040102\n",
            "684\n",
            "684\n",
            "684\n",
            "******INDEX  686 *****\n",
            "Sum --> 27.548650799549808 /Len --> 10\n",
            "Result --> 2.754865079954981 /Element --> 0604030603\n",
            "685\n",
            "685\n",
            "******INDEX  687 *****\n",
            "Sum --> 38.39863840438586 /Len --> 8\n",
            "Result --> 4.799829800548233 /Element --> 09011307\n",
            "686\n",
            "686\n",
            "686\n",
            "******INDEX  688 *****\n",
            "Sum --> 17.199481645536384 /Len --> 10\n",
            "Result --> 1.7199481645536383 /Element --> 0203050204\n",
            "687\n",
            "687\n",
            "687\n",
            "******INDEX  689 *****\n",
            "Sum --> 29.360937133995904 /Len --> 10\n",
            "Result --> 2.9360937133995906 /Element --> 0205021008\n",
            "688\n",
            "688\n",
            "688\n",
            "******INDEX  690 *****\n",
            "Sum --> 26.961514573347667 /Len --> 10\n",
            "Result --> 2.696151457334767 /Element --> 0405070304\n",
            "689\n",
            "689\n",
            "689\n",
            "******INDEX  691 *****\n",
            "Sum --> 61.524370936469055 /Len --> 10\n",
            "Result --> 6.152437093646905 /Element --> 0908150908\n",
            "690\n",
            "690\n",
            "690\n",
            "******INDEX  692 *****\n",
            "Sum --> 26.848931924611133 /Len --> 10\n",
            "Result --> 2.6848931924611135 /Element --> 0110010306\n",
            "691\n",
            "691\n",
            "691\n",
            "******INDEX  693 *****\n",
            "Sum --> 26.792700957851697 /Len --> 10\n",
            "Result --> 2.67927009578517 /Element --> 0207080303\n",
            "692\n",
            "692\n",
            "692\n",
            "******INDEX  694 *****\n",
            "Sum --> 30.062866266041592 /Len --> 10\n",
            "Result --> 3.006286626604159 /Element --> 0401040712\n",
            "693\n",
            "693\n",
            "693\n",
            "******INDEX  695 *****\n",
            "Sum --> 14.462288826689832 /Len --> 10\n",
            "Result --> 1.4462288826689833 /Element --> 0201010307\n",
            "694\n",
            "694\n",
            "694\n",
            "******INDEX  696 *****\n",
            "Sum --> 25.048103313321363 /Len --> 10\n",
            "Result --> 2.5048103313321364 /Element --> 0206020408\n",
            "695\n",
            "695\n",
            "695\n",
            "******INDEX  697 *****\n",
            "Sum --> 32.80005908488815 /Len --> 10\n",
            "Result --> 3.280005908488815 /Element --> 0403060908\n",
            "696\n",
            "696\n",
            "696\n",
            "******INDEX  698 *****\n",
            "Sum --> 28.468564220941797 /Len --> 10\n",
            "Result --> 2.84685642209418 /Element --> 0302080707\n",
            "697\n",
            "697\n",
            "697\n",
            "******INDEX  699 *****\n",
            "Sum --> 43.381314626555394 /Len --> 10\n",
            "Result --> 4.338131462655539 /Element --> 0504071805\n",
            "698\n",
            "698\n",
            "698\n",
            "******INDEX  700 *****\n",
            "Sum --> 45.23162575491379 /Len --> 10\n",
            "Result --> 4.523162575491379 /Element --> 0211070711\n",
            "699\n",
            "699\n",
            "699\n",
            "******INDEX  701 *****\n",
            "Sum --> 39.00193129076989 /Len --> 10\n",
            "Result --> 3.900193129076989 /Element --> 0505080808\n",
            "700\n",
            "700\n",
            "******INDEX  702 *****\n",
            "Sum --> 43.82717617189499 /Len --> 8\n",
            "Result --> 5.478397021486874 /Element --> 08050913\n",
            "701\n",
            "701\n",
            "701\n",
            "******INDEX  703 *****\n",
            "Sum --> 9.0 /Len --> 10\n",
            "Result --> 0.9 /Element --> 0101020104\n",
            "702\n",
            "702\n",
            "702\n",
            "******INDEX  704 *****\n",
            "Sum --> 29.74032046978141 /Len --> 10\n",
            "Result --> 2.9740320469781407 /Element --> 0204040909\n",
            "703\n",
            "703\n",
            "703\n",
            "******INDEX  705 *****\n",
            "Sum --> 20.199481645536384 /Len --> 10\n",
            "Result --> 2.0199481645536386 /Element --> 0203040802\n",
            "704\n",
            "704\n",
            "704\n",
            "******INDEX  706 *****\n",
            "Sum --> 16.297396709994068 /Len --> 10\n",
            "Result --> 1.6297396709994068 /Element --> 0102020902\n",
            "705\n",
            "705\n",
            "705\n",
            "******INDEX  707 *****\n",
            "Sum --> 21.468564220941797 /Len --> 10\n",
            "Result --> 2.1468564220941797 /Element --> 0302080106\n",
            "706\n",
            "706\n",
            "706\n",
            "******INDEX  708 *****\n",
            "Sum --> 15.462288826689832 /Len --> 10\n",
            "Result --> 1.5462288826689832 /Element --> 0201050601\n",
            "707\n",
            "707\n",
            "707\n",
            "******INDEX  709 *****\n",
            "Sum --> 13.360937133995906 /Len --> 10\n",
            "Result --> 1.3360937133995905 /Element --> 0205010102\n",
            "708\n",
            "708\n",
            "708\n",
            "******INDEX  710 *****\n",
            "Sum --> 17.468564220941797 /Len --> 10\n",
            "Result --> 1.7468564220941798 /Element --> 0302010604\n",
            "709\n",
            "709\n",
            "709\n",
            "******INDEX  711 *****\n",
            "Sum --> 19.462288826689832 /Len --> 10\n",
            "Result --> 1.9462288826689833 /Element --> 0201020212\n",
            "710\n",
            "710\n",
            "710\n",
            "******INDEX  712 *****\n",
            "Sum --> 19.0 /Len --> 10\n",
            "Result --> 1.9 /Element --> 0101010610\n",
            "711\n",
            "711\n",
            "711\n",
            "******INDEX  713 *****\n",
            "Sum --> 24.462288826689832 /Len --> 10\n",
            "Result --> 2.4462288826689833 /Element --> 0201060411\n",
            "712\n",
            "712\n",
            "712\n",
            "******INDEX  714 *****\n",
            "Sum --> 37.04810331332136 /Len --> 10\n",
            "Result --> 3.704810331332136 /Element --> 0206071306\n",
            "713\n",
            "713\n",
            "713\n",
            "******INDEX  715 *****\n",
            "Sum --> 30.585814486631534 /Len --> 10\n",
            "Result --> 3.0585814486631535 /Element --> 0106040908\n",
            "714\n",
            "714\n",
            "714\n",
            "******INDEX  716 *****\n",
            "Sum --> 34.50157964210959 /Len --> 10\n",
            "Result --> 3.450157964210959 /Element --> 0307080705\n",
            "715\n",
            "715\n",
            "715\n",
            "******INDEX  717 *****\n",
            "Sum --> 34.0698158182538 /Len --> 10\n",
            "Result --> 3.40698158182538 /Element --> 0305080708\n",
            "716\n",
            "716\n",
            "716\n",
            "******INDEX  718 *****\n",
            "Sum --> 24.393278397203456 /Len --> 10\n",
            "Result --> 2.4393278397203457 /Element --> 0407050102\n",
            "717\n",
            "717\n",
            "717\n",
            "******INDEX  719 *****\n",
            "Sum --> 27.800059084888144 /Len --> 10\n",
            "Result --> 2.7800059084888145 /Element --> 0403080307\n",
            "718\n",
            "718\n",
            "718\n",
            "******INDEX  720 *****\n",
            "Sum --> 51.601031287620096 /Len --> 10\n",
            "Result --> 5.160103128762009 /Element --> 0607080815\n",
            "719\n",
            "719\n",
            "719\n",
            "******INDEX  721 *****\n",
            "Sum --> 42.68981596853535 /Len --> 10\n",
            "Result --> 4.268981596853535 /Element --> 1003040411\n",
            "720\n",
            "720\n",
            "720\n",
            "******INDEX  722 *****\n",
            "Sum --> 32.87994186880884 /Len --> 10\n",
            "Result --> 3.2879941868808844 /Element --> 0707070201\n",
            "721\n",
            "721\n",
            "721\n",
            "******INDEX  723 *****\n",
            "Sum --> 28.585814486631534 /Len --> 10\n",
            "Result --> 2.8585814486631533 /Element --> 0106051103\n",
            "722\n",
            "722\n",
            "722\n",
            "******INDEX  724 *****\n",
            "Sum --> 26.0 /Len --> 10\n",
            "Result --> 2.6 /Element --> 0101080808\n",
            "723\n",
            "723\n",
            "723\n",
            "******INDEX  725 *****\n",
            "Sum --> 31.169267463764307 /Len --> 10\n",
            "Result --> 3.1169267463764307 /Element --> 0605050405\n",
            "724\n",
            "724\n",
            "724\n",
            "******INDEX  726 *****\n",
            "Sum --> 38.125732532083184 /Len --> 10\n",
            "Result --> 3.8125732532083183 /Element --> 0108111301\n",
            "725\n",
            "725\n",
            "725\n",
            "******INDEX  727 *****\n",
            "Sum --> 7.759685536683902 /Len --> 10\n",
            "Result --> 0.7759685536683902 /Element --> 0202010101\n",
            "726\n",
            "726\n",
            "726\n",
            "******INDEX  728 *****\n",
            "Sum --> 25.297396709994068 /Len --> 10\n",
            "Result --> 2.529739670999407 /Element --> 0102100606\n",
            "727\n",
            "727\n",
            "727\n",
            "******INDEX  729 *****\n",
            "Sum --> 32.67526226973017 /Len --> 10\n",
            "Result --> 3.267526226973017 /Element --> 0708030302\n",
            "728\n",
            "728\n",
            "728\n",
            "******INDEX  730 *****\n",
            "Sum --> 32.4685642209418 /Len --> 10\n",
            "Result --> 3.2468564220941802 /Element --> 0302061208\n",
            "729\n",
            "729\n",
            "729\n",
            "******INDEX  731 *****\n",
            "Sum --> 28.898648307306075 /Len --> 10\n",
            "Result --> 2.8898648307306076 /Element --> 0105100902\n",
            "730\n",
            "730\n",
            "730\n",
            "******INDEX  732 *****\n",
            "Sum --> 42.0698158182538 /Len --> 10\n",
            "Result --> 4.20698158182538 /Element --> 0305051412\n",
            "731\n",
            "******INDEX  733 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "732\n",
            "732\n",
            "******INDEX  734 *****\n",
            "Sum --> 15.360262976035663 /Len --> 8\n",
            "Result --> 1.920032872004458 /Element --> 04020205\n",
            "736\n",
            "736\n",
            "736\n",
            "******INDEX  738 *****\n",
            "Sum --> 20.462288826689832 /Len --> 10\n",
            "Result --> 2.0462288826689834 /Element --> 0201020906\n",
            "737\n",
            "737\n",
            "737\n",
            "******INDEX  739 *****\n",
            "Sum --> 39.16926746376431 /Len --> 10\n",
            "Result --> 3.9169267463764306 /Element --> 0605061006\n",
            "738\n",
            "738\n",
            "738\n",
            "******INDEX  740 *****\n",
            "Sum --> 24.759685536683904 /Len --> 10\n",
            "Result --> 2.47596855366839 /Element --> 0202080309\n",
            "739\n",
            "739\n",
            "739\n",
            "******INDEX  741 *****\n",
            "Sum --> 28.588021358773016 /Len --> 10\n",
            "Result --> 2.8588021358773017 /Element --> 0208080402\n",
            "740\n",
            "740\n",
            "740\n",
            "******INDEX  742 *****\n",
            "Sum --> 15.737192818846552 /Len --> 10\n",
            "Result --> 1.5737192818846553 /Element --> 0103010505\n",
            "741\n",
            "741\n",
            "741\n",
            "******INDEX  743 *****\n",
            "Sum --> 44.51434235122045 /Len --> 10\n",
            "Result --> 4.451434235122045 /Element --> 0806060510\n",
            "742\n",
            "742\n",
            "******INDEX  744 *****\n",
            "Sum --> 35.311220751300965 /Len --> 8\n",
            "Result --> 4.413902593912621 /Element --> 02101007\n",
            "743\n",
            "******INDEX  745 *****\n",
            "Sum --> 3 /Len --> 2\n",
            "Result --> 1.5 /Element --> 03\n",
            "746\n",
            "746\n",
            "746\n",
            "******INDEX  748 *****\n",
            "Sum --> 44.229015515547 /Len --> 10\n",
            "Result --> 4.4229015515547 /Element --> 0508080808\n",
            "747\n",
            "747\n",
            "******INDEX  749 *****\n",
            "Sum --> 19 /Len --> 4\n",
            "Result --> 4.75 /Element --> 0217\n",
            "748\n",
            "748\n",
            "******INDEX  750 *****\n",
            "Sum --> 21 /Len --> 4\n",
            "Result --> 5.25 /Element --> 1110\n",
            "749\n",
            "******INDEX  751 *****\n",
            "Sum --> 6 /Len --> 2\n",
            "Result --> 3.0 /Element --> 06\n",
            "750\n",
            "750\n",
            "750\n",
            "******INDEX  752 *****\n",
            "Sum --> 11.737192818846552 /Len --> 10\n",
            "Result --> 1.1737192818846551 /Element --> 0103020203\n",
            "751\n",
            "751\n",
            "751\n",
            "******INDEX  753 *****\n",
            "Sum --> 25.393278397203456 /Len --> 10\n",
            "Result --> 2.539327839720346 /Element --> 0407010602\n",
            "752\n",
            "752\n",
            "752\n",
            "******INDEX  754 *****\n",
            "Sum --> 16.73719281884655 /Len --> 10\n",
            "Result --> 1.6737192818846551 /Element --> 0103030603\n",
            "753\n",
            "753\n",
            "753\n",
            "******INDEX  755 *****\n",
            "Sum --> 37.43369511462568 /Len --> 10\n",
            "Result --> 3.743369511462568 /Element --> 0507030412\n",
            "754\n",
            "754\n",
            "754\n",
            "******INDEX  756 *****\n",
            "Sum --> 27.188598798124776 /Len --> 10\n",
            "Result --> 2.7188598798124777 /Element --> 0408010701\n",
            "755\n",
            "******INDEX  757 *****\n",
            "Sum --> 16.548650799549808 /Len --> 6\n",
            "Result --> 2.7581084665916347 /Element --> 060401\n",
            "756\n",
            "756\n",
            "756\n",
            "******INDEX  758 *****\n",
            "Sum --> 17.34089790913317 /Len --> 10\n",
            "Result --> 1.7340897909133168 /Element --> 0404030201\n"
          ]
        }
      ],
      "source": [
        "def count_lastRaces(arrayLast):\n",
        "  index = 0\n",
        "  for element in arrayLast:\n",
        "    element = element.replace(',', '')\n",
        "    element = element.replace('[', '')\n",
        "    element = element[:len(element)-1]\n",
        "    i = 0\n",
        "    j = 0\n",
        "    sum = 0\n",
        "    if 'Debutante' in element:\n",
        "      sum = 10\n",
        "      element = []\n",
        "    else:\n",
        "      element = element.replace('NP', '08')\n",
        "      element = element.replace('Desc', '08')\n",
        "      element = element.replace(' ', '')\n",
        "      while(i < len(element)):\n",
        "        if(i == 0 and len(element)>4):\n",
        "          j = j+2\n",
        "          sum += int(element[i:j])**1.3\n",
        "          i = i+2 \n",
        "        elif (i == 2 and len(element)>4):\n",
        "          j = j+2\n",
        "          sum += int(element[i:j])**1.2\n",
        "          i = i+2 \n",
        "        else:\n",
        "          j = j+2\n",
        "          print(index)\n",
        "          sum += int(element[i:j])\n",
        "          i = i+2\n",
        "      print('******INDEX ', index+2, '*****')\n",
        "      print('Sum -->', sum, '/Len -->', len(element))\n",
        "      sum = sum / len(element)\n",
        "      print('Result -->', sum, '/Element -->' ,element)\n",
        "    train.at[index, 'MediaUltimasActuaciones'] = sum\n",
        "    train.at[index, 'CantidadActuaciones'] = (len(element)/2)\n",
        "    index += 1\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "t_array = train[\"UltimasActuaciones\"]\n",
        "count_lastRaces(t_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "a6D-jazfZK_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb5472e-d4f1-4549-c100-669be8ba1f8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1.456720\n",
              "1     2.500000\n",
              "2     0.500000\n",
              "3     5.750000\n",
              "4     5.000000\n",
              "5     3.000000\n",
              "6     2.250000\n",
              "7     1.000000\n",
              "8     2.896886\n",
              "9     4.428258\n",
              "10    2.500000\n",
              "11    2.900193\n",
              "12    2.500000\n",
              "13    6.000000\n",
              "14    1.500000\n",
              "15    2.000000\n",
              "16    2.000000\n",
              "17    2.000000\n",
              "18    2.574867\n",
              "19    5.000000\n",
              "Name: MediaUltimasActuaciones, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ],
      "source": [
        " train['MediaUltimasActuaciones'].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO3y7XmtVGKF"
      },
      "source": [
        "## **Tratamiento (Hora)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "tEq7CD02wR-0"
      },
      "outputs": [],
      "source": [
        "train['Hora'] = train['Hora'].str.replace(':', '') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSBaoWTwVMGs"
      },
      "source": [
        "## **Tratamiento (Terreno)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "atUehBWViH6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674486f6-5712-40a7-c73e-a6fd3ff23e5f"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 757 entries, 0 to 756\n",
            "Data columns (total 42 columns):\n",
            " #   Column                   Non-Null Count  Dtype         \n",
            "---  ------                   --------------  -----         \n",
            " 0   Puesto                   757 non-null    int64         \n",
            " 1   NombreCaballo            757 non-null    object        \n",
            " 2   Peso                     757 non-null    object        \n",
            " 3   Edad                     757 non-null    int64         \n",
            " 4   Mantilla                 757 non-null    int64         \n",
            " 5   Propietario              757 non-null    object        \n",
            " 6   Preparador               757 non-null    object        \n",
            " 7   Jinete                   757 non-null    object        \n",
            " 8   Problemas                757 non-null    object        \n",
            " 9   UltimasActuaciones       757 non-null    object        \n",
            " 10  Fecha                    757 non-null    object        \n",
            " 11  Hora                     757 non-null    object        \n",
            " 12  Terreno                  757 non-null    object        \n",
            " 13  Distancia                757 non-null    int64         \n",
            " 14  Tipo                     756 non-null    object        \n",
            " 15  Categoría                757 non-null    object        \n",
            " 16  SentidoHipodromo         757 non-null    int64         \n",
            " 17  Meteorología             757 non-null    object        \n",
            " 18  LLuvia                   757 non-null    float64       \n",
            " 19  Viento                   757 non-null    int64         \n",
            " 20  Temperatura              757 non-null    float64       \n",
            " 21  Hipodromo                757 non-null    object        \n",
            " 22  FechaAux                 757 non-null    datetime64[ns]\n",
            " 23  year                     757 non-null    float64       \n",
            " 24  month                    757 non-null    float64       \n",
            " 25  day                      757 non-null    float64       \n",
            " 26  Otoño                    757 non-null    int64         \n",
            " 27  DiasDesdeCarrera         757 non-null    float64       \n",
            " 28  DaysSincePreviousRace    757 non-null    float64       \n",
            " 29  Contrincantes            757 non-null    float64       \n",
            " 30  DestrezaDistancia        757 non-null    float64       \n",
            " 31  Problema_Nulo            757 non-null    int64         \n",
            " 32  Problema_1               757 non-null    int64         \n",
            " 33  Problema_2               757 non-null    int64         \n",
            " 34  Problema_3               757 non-null    int64         \n",
            " 35  Problema_4               757 non-null    int64         \n",
            " 36  Problema_5               757 non-null    int64         \n",
            " 37  Problema_6               757 non-null    int64         \n",
            " 38  Problema_7               757 non-null    int64         \n",
            " 39  Problema_8               757 non-null    int64         \n",
            " 40  MediaUltimasActuaciones  757 non-null    float64       \n",
            " 41  CantidadActuaciones      757 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(11), int64(16), object(14)\n",
            "memory usage: 248.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT3WFAY1VV6m"
      },
      "source": [
        "## **Tratamiento (Distancia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIoYoCBVZvz"
      },
      "source": [
        "## **Tratamiento (Tipo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnYcVwMOVcSq"
      },
      "source": [
        "## **Tratamiento (Categoria)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmxe8YbuVe9c"
      },
      "source": [
        "## **Tratamiento (SentidoHipodromo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXmLbU8sViiz"
      },
      "source": [
        "## **Tratamiento (Meteorologia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0nFs5yJVk-E"
      },
      "source": [
        "## **Tratamiento (Lluvia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2C6KoQNVnSN"
      },
      "source": [
        "## **Tratamiento (Viento)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVGceRFMVpSQ"
      },
      "source": [
        "## **Tratamiento (TemperaturaMax)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMTgV1HiVsvL"
      },
      "source": [
        "## **Tratamiento (TemperaturaMin)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUiYNaafVuSf"
      },
      "source": [
        "## **Tratamiento (Hipodromo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1PqUKx7cm1H"
      },
      "source": [
        "# Visualizacion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe()"
      ],
      "metadata": {
        "id": "ksvsSNTP8R63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "ef41c0e1-218f-4a68-b5e7-3bde59f058e1"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Puesto        Edad    Mantilla    Distancia  SentidoHipodromo  \\\n",
              "count  757.000000  757.000000  757.000000   757.000000             757.0   \n",
              "mean     2.145310    3.686922    5.966975  1682.034346               0.0   \n",
              "std      1.533582    1.525272    3.781425   392.582797               0.0   \n",
              "min      0.000000    2.000000    1.000000  1100.000000               0.0   \n",
              "25%      1.000000    2.000000    3.000000  1400.000000               0.0   \n",
              "50%      2.000000    3.000000    5.000000  1600.000000               0.0   \n",
              "75%      3.000000    5.000000    8.000000  1800.000000               0.0   \n",
              "max      6.000000   10.000000   18.000000  3200.000000               0.0   \n",
              "\n",
              "           LLuvia      Viento  Temperatura    year       month         day  \\\n",
              "count  757.000000  757.000000   757.000000   757.0  757.000000  757.000000   \n",
              "mean     0.001849   10.179657    21.968956  2022.0    9.690885   14.371202   \n",
              "std      0.013482    5.088056     5.461690     0.0    0.874134    8.094244   \n",
              "min      0.000000    5.000000    15.000000  2022.0    8.000000    2.000000   \n",
              "25%      0.000000    7.000000    17.500000  2022.0    9.000000    9.000000   \n",
              "50%      0.000000   10.000000    20.000000  2022.0   10.000000   12.000000   \n",
              "75%      0.000000   10.000000    26.000000  2022.0   10.000000   18.000000   \n",
              "max      0.100000   28.000000    33.000000  2022.0   11.000000   30.000000   \n",
              "\n",
              "       Otoño  DiasDesdeCarrera  DaysSincePreviousRace  Contrincantes  \\\n",
              "count  757.0        757.000000             757.000000     757.000000   \n",
              "mean     0.0         74.902246              35.577279      10.706737   \n",
              "std      0.0         27.204953              13.997767       3.666828   \n",
              "min      0.0         37.000000               5.000000       4.000000   \n",
              "25%      0.0         50.000000              21.000000       8.000000   \n",
              "50%      0.0         71.000000              45.000000      10.000000   \n",
              "75%      0.0         92.000000              45.000000      13.000000   \n",
              "max      0.0        136.000000              94.000000      18.000000   \n",
              "\n",
              "       DestrezaDistancia  Problema_Nulo  Problema_1  Problema_2  Problema_3  \\\n",
              "count         757.000000     757.000000       757.0  757.000000  757.000000   \n",
              "mean            0.001321       0.453104         0.0    0.047556    0.140026   \n",
              "std             0.036346       0.498125         0.0    0.212966    0.347244   \n",
              "min             0.000000       0.000000         0.0    0.000000    0.000000   \n",
              "25%             0.000000       0.000000         0.0    0.000000    0.000000   \n",
              "50%             0.000000       0.000000         0.0    0.000000    0.000000   \n",
              "75%             0.000000       1.000000         0.0    0.000000    0.000000   \n",
              "max             1.000000       1.000000         0.0    1.000000    1.000000   \n",
              "\n",
              "       Problema_4  Problema_5  Problema_6  Problema_7  Problema_8  \\\n",
              "count  757.000000  757.000000  757.000000       757.0  757.000000   \n",
              "mean     0.079260    0.110964    0.011889         0.0    0.350066   \n",
              "std      0.270323    0.314296    0.108458         0.0    0.477306   \n",
              "min      0.000000    0.000000    0.000000         0.0    0.000000   \n",
              "25%      0.000000    0.000000    0.000000         0.0    0.000000   \n",
              "50%      0.000000    0.000000    0.000000         0.0    0.000000   \n",
              "75%      0.000000    0.000000    0.000000         0.0    1.000000   \n",
              "max      1.000000    1.000000    1.000000         0.0    1.000000   \n",
              "\n",
              "       MediaUltimasActuaciones  CantidadActuaciones  \n",
              "count               757.000000           757.000000  \n",
              "mean                  3.398371             3.981506  \n",
              "std                   2.404985             1.681577  \n",
              "min                   0.500000             0.000000  \n",
              "25%                   2.000000             3.000000  \n",
              "50%                   2.738131             5.000000  \n",
              "75%                   3.839635             5.000000  \n",
              "max                  26.544382             5.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3c2a5cb-f3f7-48ca-a157-9849583396b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Puesto</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "      <th>DaysSincePreviousRace</th>\n",
              "      <th>Contrincantes</th>\n",
              "      <th>DestrezaDistancia</th>\n",
              "      <th>Problema_Nulo</th>\n",
              "      <th>Problema_1</th>\n",
              "      <th>Problema_2</th>\n",
              "      <th>Problema_3</th>\n",
              "      <th>Problema_4</th>\n",
              "      <th>Problema_5</th>\n",
              "      <th>Problema_6</th>\n",
              "      <th>Problema_7</th>\n",
              "      <th>Problema_8</th>\n",
              "      <th>MediaUltimasActuaciones</th>\n",
              "      <th>CantidadActuaciones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.0</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.0</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.0</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.0</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.0</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "      <td>757.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.145310</td>\n",
              "      <td>3.686922</td>\n",
              "      <td>5.966975</td>\n",
              "      <td>1682.034346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001849</td>\n",
              "      <td>10.179657</td>\n",
              "      <td>21.968956</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>9.690885</td>\n",
              "      <td>14.371202</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.902246</td>\n",
              "      <td>35.577279</td>\n",
              "      <td>10.706737</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.453104</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.047556</td>\n",
              "      <td>0.140026</td>\n",
              "      <td>0.079260</td>\n",
              "      <td>0.110964</td>\n",
              "      <td>0.011889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.350066</td>\n",
              "      <td>3.398371</td>\n",
              "      <td>3.981506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.533582</td>\n",
              "      <td>1.525272</td>\n",
              "      <td>3.781425</td>\n",
              "      <td>392.582797</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013482</td>\n",
              "      <td>5.088056</td>\n",
              "      <td>5.461690</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.874134</td>\n",
              "      <td>8.094244</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.204953</td>\n",
              "      <td>13.997767</td>\n",
              "      <td>3.666828</td>\n",
              "      <td>0.036346</td>\n",
              "      <td>0.498125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.212966</td>\n",
              "      <td>0.347244</td>\n",
              "      <td>0.270323</td>\n",
              "      <td>0.314296</td>\n",
              "      <td>0.108458</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.477306</td>\n",
              "      <td>2.404985</td>\n",
              "      <td>1.681577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1400.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>17.500000</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.738131</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1800.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.839635</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>3200.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>136.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>26.544382</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3c2a5cb-f3f7-48ca-a157-9849583396b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3c2a5cb-f3f7-48ca-a157-9849583396b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3c2a5cb-f3f7-48ca-a157-9849583396b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.hist(figsize = (40,30))"
      ],
      "metadata": {
        "id": "wpX8azW_E7iY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5638008-baa6-447e-e5dd-21372acc8a0e"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f96588430>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f96304dc0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88e50520>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88e04910>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88e24d00>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88dd6070>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88dd6160>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88d7b5b0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88d48d00>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88cfe130>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88d25520>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88cd0910>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88c7ad00>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88cb3130>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88c57be0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88c0c340>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88c32a60>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88be91c0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f9951b340>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f96452b50>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f961a2d00>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f88d06790>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f96470070>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f95f2f3a0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f960bdaf0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f3f96044040>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f95fad910>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f95f3d280>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f95f335b0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f3f95f09b50>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2880x2160 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "C_mat = train.corr()\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "\n",
        "#Correlation ranges from -1 to +1. Values closer to zero means there is no linear trend between the two variables. \n",
        "#The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. \n",
        "#A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases.\n",
        "\n",
        "sns.heatmap(C_mat, vmax = .8, square = True, cmap=\"PiYG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D4AxCeH5FHSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "outputId": "a6d2be19-72fe-4c3e-e70b-8e929ce79254"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "8I-8VFtocpqj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "1e77d81a-ea3c-4ee0-cfdf-a48b23a3f2c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(data=train, x='Distancia', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train[(train['Distancia'] < 1600).tolist()]))\n",
        "\n",
        "print(len(train[((train['Distancia'] >= 1600) & (train['Distancia'] <= 2400) ).tolist()]))\n",
        "\n",
        "print(len(train[(train['Distancia'] >= 3200).tolist()]))"
      ],
      "metadata": {
        "id": "jZuBN_M-l4Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473b68b1-f913-47dc-d053-0eaaea5dbef7"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "330\n",
            "409\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "M3o8h0QOcraE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "5d4f5652-6bc7-4672-e053-e9900534baf8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(data=train, x='Hora', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "avFzL6E4gnb_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "78c946c8-8c41-4b83-b282-cc23ba709cbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(data=train, x='Categoría', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "rcz5Q6pOgqOq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "e7ef9dbb-6c97-455e-d700-c3584169b811"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3600x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(50,8))\n",
        "sns.boxplot(data=train, x='Tipo', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "HLw83GVVgs2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6e887850-11df-4452-816d-c8ab180c0804"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2880x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(40,8))\n",
        "sns.boxplot(data=train, x='Meteorología', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "pCO2kvGFgvU2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "88769277-6fb9-455c-e125-6ff437c80803"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2880x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(40,8))\n",
        "sns.boxplot(data=train, x='LLuvia', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "ol6nFvIWqivn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "6a60b58f-63c0-4583-f343-709dbe75b4ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(data=train, x='Viento', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "uPRY73NArcFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "6156ad4f-62cf-4b8c-ae9e-4711616850ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(data=train, x='Temperatura', y='Puesto')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCyC0Ia-eqj7"
      },
      "source": [
        "# Preprocesamiento Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "bqepCaJ7pQT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ea11e2-788b-4df8-e6b5-8f5df908ab42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 757 entries, 0 to 756\n",
            "Data columns (total 42 columns):\n",
            " #   Column                   Non-Null Count  Dtype         \n",
            "---  ------                   --------------  -----         \n",
            " 0   Puesto                   757 non-null    int64         \n",
            " 1   NombreCaballo            757 non-null    object        \n",
            " 2   Peso                     757 non-null    object        \n",
            " 3   Edad                     757 non-null    int64         \n",
            " 4   Mantilla                 757 non-null    int64         \n",
            " 5   Propietario              757 non-null    object        \n",
            " 6   Preparador               757 non-null    object        \n",
            " 7   Jinete                   757 non-null    object        \n",
            " 8   Problemas                757 non-null    object        \n",
            " 9   UltimasActuaciones       757 non-null    object        \n",
            " 10  Fecha                    757 non-null    object        \n",
            " 11  Hora                     757 non-null    object        \n",
            " 12  Terreno                  757 non-null    object        \n",
            " 13  Distancia                757 non-null    int64         \n",
            " 14  Tipo                     756 non-null    object        \n",
            " 15  Categoría                757 non-null    object        \n",
            " 16  SentidoHipodromo         757 non-null    int64         \n",
            " 17  Meteorología             757 non-null    object        \n",
            " 18  LLuvia                   757 non-null    float64       \n",
            " 19  Viento                   757 non-null    int64         \n",
            " 20  Temperatura              757 non-null    float64       \n",
            " 21  Hipodromo                757 non-null    object        \n",
            " 22  FechaAux                 757 non-null    datetime64[ns]\n",
            " 23  year                     757 non-null    float64       \n",
            " 24  month                    757 non-null    float64       \n",
            " 25  day                      757 non-null    float64       \n",
            " 26  Otoño                    757 non-null    int64         \n",
            " 27  DiasDesdeCarrera         757 non-null    float64       \n",
            " 28  DaysSincePreviousRace    757 non-null    float64       \n",
            " 29  Contrincantes            757 non-null    float64       \n",
            " 30  DestrezaDistancia        757 non-null    float64       \n",
            " 31  Problema_Nulo            757 non-null    int64         \n",
            " 32  Problema_1               757 non-null    int64         \n",
            " 33  Problema_2               757 non-null    int64         \n",
            " 34  Problema_3               757 non-null    int64         \n",
            " 35  Problema_4               757 non-null    int64         \n",
            " 36  Problema_5               757 non-null    int64         \n",
            " 37  Problema_6               757 non-null    int64         \n",
            " 38  Problema_7               757 non-null    int64         \n",
            " 39  Problema_8               757 non-null    int64         \n",
            " 40  MediaUltimasActuaciones  757 non-null    float64       \n",
            " 41  CantidadActuaciones      757 non-null    float64       \n",
            "dtypes: datetime64[ns](1), float64(11), int64(16), object(14)\n",
            "memory usage: 248.5+ KB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "P-rFC62tnTJ5"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import sklearn.compose\n",
        "#Solo ejecutar previo a esto product para separar las palabras\n",
        "column_transformer_train = sklearn.compose.ColumnTransformer(transformers=[\n",
        "    (\"drop\", \"drop\", ['UltimasActuaciones','Fecha','year','day', 'Problemas','Hora', 'FechaAux', 'Mantilla', 'Terreno', 'Tipo','Categoría', 'Meteorología', 'LLuvia', 'Viento', 'Temperatura', 'Hipodromo', 'Contrincantes']),\n",
        "    (\"scale\", sklearn.preprocessing.StandardScaler(), []),\n",
        "    # (\"num\", sklearn.preprocessing.MinMaxScaler(), ['Distancia', 'Edad', 'Peso']),\n",
        "    (\"one-hot\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), ['NombreCaballo','Propietario', 'Preparador', 'Jinete']),\n",
        "], remainder='passthrough');\n",
        "\n",
        "# column_transformer = sklearn.compose.ColumnTransformer(transformers=[\n",
        "#     (\"drop\", \"drop\", ['UltimasActuaciones','Fecha','year','day', 'Problemas','Hora']),\n",
        "#     (\"scale\", sklearn.preprocessing.StandardScaler(), []),\n",
        "#     (\"num\", sklearn.preprocessing.MinMaxScaler(), ['Distancia', 'Edad', 'Peso']),\n",
        "#     (\"one-hot\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), ['NombreCaballo', 'Mantilla','Propietario', 'Preparador', 'Jinete', 'FechaAux', 'Terreno', 'Tipo','Categoría', 'Meteorología', 'LLuvia', 'Viento', 'TemperaturaMax', 'TemperaturaMin', 'Hipodromo']),\n",
        "# ], remainder='passthrough');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "K8Pil183vZ-_"
      },
      "outputs": [],
      "source": [
        "train = train.reset_index()\n",
        "X=train\n",
        "X = X.drop([\"Puesto\"], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "Zv-MOXk6SQ9i"
      },
      "outputs": [],
      "source": [
        "X_transform = column_transformer_train.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_transformer_train.get_feature_names()"
      ],
      "metadata": {
        "id": "NqI3k89KY-u3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848d76e8-a392-4f86-b12c-fa98031e6b9e"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one-hot__x0_AMERICANO',\n",
              " 'one-hot__x0_ARETHA',\n",
              " 'one-hot__x0_ASTRAL',\n",
              " 'one-hot__x0_ASTURIAS',\n",
              " 'one-hot__x0_ATLANTICO',\n",
              " 'one-hot__x0_AUSTRALIA CAPE',\n",
              " 'one-hot__x0_CAROLINA WEST',\n",
              " 'one-hot__x0_CHUSQUEZ',\n",
              " 'one-hot__x0_EL CANEY',\n",
              " 'one-hot__x0_EMBAT',\n",
              " 'one-hot__x0_FAITH ROSE',\n",
              " 'one-hot__x0_FINELY TUNED',\n",
              " 'one-hot__x0_FORTUNATO',\n",
              " 'one-hot__x0_HIGHLAND MARKET',\n",
              " 'one-hot__x0_IZAMAL',\n",
              " 'one-hot__x0_KANE ORE',\n",
              " 'one-hot__x0_LA MAL AMADA',\n",
              " 'one-hot__x0_LADY RAZALMA',\n",
              " 'one-hot__x0_MAITRE YODA',\n",
              " 'one-hot__x0_MEDICEAN BLUE',\n",
              " 'one-hot__x0_MONTERREDONDO',\n",
              " 'one-hot__x0_ORBAYO',\n",
              " 'one-hot__x0_OtherHorse',\n",
              " 'one-hot__x0_PIU BIRCH',\n",
              " 'one-hot__x0_ROBAYERA',\n",
              " 'one-hot__x0_ROBLON',\n",
              " 'one-hot__x0_RUMBERA',\n",
              " 'one-hot__x0_SANCTI PETRI',\n",
              " 'one-hot__x0_SANS ATTENDRE',\n",
              " 'one-hot__x0_SEVERUS',\n",
              " 'one-hot__x0_SHELBY',\n",
              " 'one-hot__x0_SOFUNNY',\n",
              " 'one-hot__x0_SOGALINDA',\n",
              " 'one-hot__x0_STARSHADOW',\n",
              " 'one-hot__x0_THE GAME',\n",
              " 'one-hot__x0_TRES DE TREBOL',\n",
              " 'one-hot__x0_UPSDAWN',\n",
              " 'one-hot__x0_UPSILON',\n",
              " 'one-hot__x0_VIKING CITY',\n",
              " 'one-hot__x0_VITA BARELIERE',\n",
              " 'one-hot__x0_WALKING TO GLORY',\n",
              " \"one-hot__x0_WARRIOR'S REVENGE\",\n",
              " 'one-hot__x0_WHITE WINE',\n",
              " 'one-hot__x0_WINTON',\n",
              " 'one-hot__x0_XILADO',\n",
              " 'one-hot__x1_4 C',\n",
              " 'one-hot__x1_AMAZING TURF',\n",
              " 'one-hot__x1_ANNUA RACING, S....',\n",
              " 'one-hot__x1_ARTEMIS',\n",
              " 'one-hot__x1_ATERPE',\n",
              " 'one-hot__x1_BERNIE',\n",
              " 'one-hot__x1_CANARIAS',\n",
              " 'one-hot__x1_CARAL',\n",
              " 'one-hot__x1_CELSO MENDEZ',\n",
              " 'one-hot__x1_CENTURION',\n",
              " 'one-hot__x1_CIELO DE MADRID',\n",
              " 'one-hot__x1_COCHETEUX',\n",
              " 'one-hot__x1_CUADRA A.F.F S.L',\n",
              " 'one-hot__x1_DELTA',\n",
              " 'one-hot__x1_DONALFA',\n",
              " 'one-hot__x1_DUQUE DE ALBURQU...',\n",
              " 'one-hot__x1_ECURIE DES MOUET...',\n",
              " 'one-hot__x1_EL GENTIO',\n",
              " 'one-hot__x1_ELEVAGE LA REVER...',\n",
              " 'one-hot__x1_ENRIQUE FERNANDE...',\n",
              " 'one-hot__x1_EUROPA',\n",
              " 'one-hot__x1_FORRAJES Y CEREA...',\n",
              " 'one-hot__x1_GISPERT',\n",
              " 'one-hot__x1_IGUELDO',\n",
              " 'one-hot__x1_JACAL',\n",
              " 'one-hot__x1_JAVIER MALDONADO...',\n",
              " 'one-hot__x1_LA TOLEDANA',\n",
              " 'one-hot__x1_LAS AGUILAS',\n",
              " 'one-hot__x1_MARTUL',\n",
              " 'one-hot__x1_MEDITERRANEO',\n",
              " 'one-hot__x1_OtherOwner',\n",
              " 'one-hot__x1_PAOLO NERI',\n",
              " 'one-hot__x1_QUINTO REAL',\n",
              " 'one-hot__x1_REAPERTURA',\n",
              " 'one-hot__x1_SALVADOR MARQUEZ',\n",
              " 'one-hot__x1_SANTA BARBARA',\n",
              " 'one-hot__x1_TEN FE',\n",
              " 'one-hot__x1_TIGRES',\n",
              " 'one-hot__x1_TIZIANO',\n",
              " 'one-hot__x1_TRILOGIA',\n",
              " 'one-hot__x1_VALLADOLID',\n",
              " 'one-hot__x1_YEGUADA AGF',\n",
              " 'one-hot__x1_YEGUADA ARANJUEZ',\n",
              " 'one-hot__x1_YEGUADA ROCIO',\n",
              " 'one-hot__x1_ZEZINHO',\n",
              " 'one-hot__x1_ZUL',\n",
              " 'one-hot__x1_ZURRAQUIN',\n",
              " 'one-hot__x2_A.CARRASCO',\n",
              " 'one-hot__x2_A.IMAZ,B.',\n",
              " 'one-hot__x2_A.NUÑEZ',\n",
              " 'one-hot__x2_A.SOTO',\n",
              " 'one-hot__x2_A.TSERETELI',\n",
              " 'one-hot__x2_B.MORENO',\n",
              " 'one-hot__x2_B.RAMA',\n",
              " 'one-hot__x2_B.VALENTI',\n",
              " 'one-hot__x2_C.FERNANDEZ',\n",
              " 'one-hot__x2_D.DIEZ',\n",
              " 'one-hot__x2_E.ARGUINZONES',\n",
              " 'one-hot__x2_F.RODRIGUEZ',\n",
              " 'one-hot__x2_G.ARIZKORRETA',\n",
              " 'one-hot__x2_J.A.RODRIGUEZ',\n",
              " 'one-hot__x2_J.C.CERQUEIRA',\n",
              " 'one-hot__x2_J.C.ROSELL',\n",
              " 'one-hot__x2_J.L.MAROTO',\n",
              " 'one-hot__x2_J.LOPEZ',\n",
              " 'one-hot__x2_J.M.OSORIO',\n",
              " 'one-hot__x2_M&M RACING',\n",
              " 'one-hot__x2_M.A.MARIN',\n",
              " 'one-hot__x2_M.ALONSO R.',\n",
              " 'one-hot__x2_M.ALVAREZ',\n",
              " 'one-hot__x2_M.J.PEREZ',\n",
              " 'one-hot__x2_OtherTrainer',\n",
              " 'one-hot__x2_P.OLAVE',\n",
              " 'one-hot__x2_R.MARTIN V.',\n",
              " 'one-hot__x2_T.MARTINS',\n",
              " 'one-hot__x3_A.MARTINEZ',\n",
              " 'one-hot__x3_B. FAYOS',\n",
              " 'one-hot__x3_B.ESTUPIÑAN',\n",
              " 'one-hot__x3_C. CADEL',\n",
              " 'one-hot__x3_C.HAZEN',\n",
              " 'one-hot__x3_C.PEREZ',\n",
              " 'one-hot__x3_D. FERREIRA',\n",
              " 'one-hot__x3_D.SIKOROVÁ',\n",
              " 'one-hot__x3_F.MARTINEZ',\n",
              " 'one-hot__x3_G.TROLLEY DE PRE...',\n",
              " 'one-hot__x3_J.GELABERT',\n",
              " 'one-hot__x3_J.L. BORREGO',\n",
              " 'one-hot__x3_J.L. MARTINEZ',\n",
              " 'one-hot__x3_L.FONSECA',\n",
              " 'one-hot__x3_N. DE JULIAN',\n",
              " 'one-hot__x3_N. GARCIA',\n",
              " 'one-hot__x3_N.SACCU',\n",
              " 'one-hot__x3_OtherJockey',\n",
              " 'one-hot__x3_R.N.VALLE',\n",
              " 'one-hot__x3_SRTA. TENA, C.',\n",
              " 'one-hot__x3_STA. BUESA,C.',\n",
              " 'one-hot__x3_V. JANACEK',\n",
              " 'one-hot__x3_V.ALONSO V.',\n",
              " 'one-hot__x3_Y.RODRIGUEZ',\n",
              " 'index',\n",
              " 'Peso',\n",
              " 'Edad',\n",
              " 'Distancia',\n",
              " 'SentidoHipodromo',\n",
              " 'month',\n",
              " 'Otoño',\n",
              " 'DiasDesdeCarrera',\n",
              " 'DaysSincePreviousRace',\n",
              " 'DestrezaDistancia',\n",
              " 'Problema_Nulo',\n",
              " 'Problema_1',\n",
              " 'Problema_2',\n",
              " 'Problema_3',\n",
              " 'Problema_4',\n",
              " 'Problema_5',\n",
              " 'Problema_6',\n",
              " 'Problema_7',\n",
              " 'Problema_8',\n",
              " 'MediaUltimasActuaciones',\n",
              " 'CantidadActuaciones']"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "wDcx2HPswpdr"
      },
      "outputs": [],
      "source": [
        "train \n",
        "X_transform\n",
        "X = X_transform       #Nos quedamos con todo menos la columna objetivo, en este caso 'price'\n",
        "y = train[\"Puesto\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "QIaYVmOXyUhP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e910db8-90d3-4cdd-cbf7-e5aa5538ea13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     NombreCaballo  MediaUltimasActuaciones\n",
              "0       OtherHorse                 1.456720\n",
              "1       OtherHorse                 2.500000\n",
              "2       OtherHorse                 0.500000\n",
              "3       OtherHorse                 5.750000\n",
              "4       OtherHorse                 5.000000\n",
              "5       OtherHorse                 3.000000\n",
              "6       OtherHorse                 2.250000\n",
              "7       OtherHorse                 1.000000\n",
              "8       OtherHorse                 2.896886\n",
              "9       OtherHorse                 4.428258\n",
              "10      OtherHorse                 2.500000\n",
              "11      OtherHorse                 2.900193\n",
              "12      OtherHorse                 2.500000\n",
              "13      OtherHorse                 6.000000\n",
              "14      OtherHorse                 1.500000\n",
              "15      OtherHorse                 2.000000\n",
              "16      OtherHorse                 2.000000\n",
              "17      OtherHorse                 2.000000\n",
              "18      OtherHorse                 2.574867\n",
              "19      OtherHorse                 5.000000\n",
              "20      OtherHorse                 3.500000\n",
              "21      OtherHorse                 6.500000\n",
              "22      OtherHorse                 8.000000\n",
              "23      OtherHorse                 7.500000\n",
              "24      OtherHorse                 2.654865\n",
              "25      OtherHorse                 1.873719\n",
              "26      OtherHorse                 4.352437\n",
              "27      OtherHorse                 2.966572\n",
              "28      OtherHorse                 1.580006\n",
              "29      OtherHorse                 1.984048\n",
              "30      OtherHorse                 3.085643\n",
              "31      OtherHorse                 2.006287\n",
              "32   MONTERREDONDO                 2.982756\n",
              "33      OtherHorse                 2.969604\n",
              "34      OtherHorse                 1.846856\n",
              "35      OtherHorse                 1.810328\n",
              "36      OtherHorse                 1.546229\n",
              "37      OtherHorse                 2.027803\n",
              "38      OtherHorse                 1.736026\n",
              "39      OtherHorse                 0.946229\n",
              "40      OtherHorse                 2.929690\n",
              "41      OtherHorse                 2.180006\n",
              "42      OtherHorse                 2.173719\n",
              "43      OtherHorse                 1.844920\n",
              "44      OtherHorse                 2.068910\n",
              "45      OtherHorse                 1.427803\n",
              "46      OtherHorse                 2.429690\n",
              "47      OtherHorse                 3.402948\n",
              "48       SOGALINDA                 1.817117\n",
              "49      OtherHorse                 2.500193\n",
              "50      OtherHorse                 5.541496\n",
              "51      OtherHorse                 3.596151\n",
              "52       ATLANTICO                 3.129690\n",
              "53      OtherHorse                 3.044818\n",
              "54      OtherHorse                 5.302879\n",
              "55    LA MAL AMADA                 4.268982\n",
              "56      OtherHorse                 6.864831\n",
              "57      OtherHorse                 2.682756\n",
              "58      OtherHorse                 3.853844\n",
              "59      OtherHorse                 2.606287\n",
              "60      OtherHorse                 3.854865\n",
              "61      OtherHorse                 2.968910\n",
              "62       PIU BIRCH                 5.516798\n",
              "63      OtherHorse                 4.667526\n",
              "64      OtherHorse                 5.241496\n",
              "65      OtherHorse                 5.205426\n",
              "66      OtherHorse                 2.761336\n",
              "67      OtherHorse                 3.861545\n",
              "68      OtherHorse                 1.521396\n",
              "69      OtherHorse                 3.141154\n",
              "70      OtherHorse                 3.306989\n",
              "71      OtherHorse                 8.277705\n",
              "72      OtherHorse                 5.146657\n",
              "73      OtherHorse                 5.523485\n",
              "74      OtherHorse                 1.636026\n",
              "75      OtherHorse                 2.738131\n",
              "76      OtherHorse                 2.119948\n",
              "77      OtherHorse                 3.698445\n",
              "78      OtherHorse                 2.439328\n",
              "79      OtherHorse                 2.700781\n",
              "80      OtherHorse                 1.750000\n",
              "81      OtherHorse                 2.654953\n",
              "82      OtherHorse                 1.684048\n",
              "83      OtherHorse                 3.382718\n",
              "84      OtherHorse                 3.034090\n",
              "85      OtherHorse                 4.380156\n",
              "86      OtherHorse                 1.146229\n",
              "87      OtherHorse                 3.777746\n",
              "88      OtherHorse                 1.046856\n",
              "89      OtherHorse                 3.195221\n",
              "90      OtherHorse                 1.946856\n",
              "91      OtherHorse                 2.536094\n",
              "92      OtherHorse                 4.481689\n",
              "93      OtherHorse                 3.044818\n",
              "94      OtherHorse                 4.816798\n",
              "95      OtherHorse                 3.528672\n",
              "96      OtherHorse                 2.410328\n",
              "97      OtherHorse                 3.292853\n",
              "98      OtherHorse                 1.017117\n",
              "99      OtherHorse                 3.484048\n",
              "100     OtherHorse                 2.527803\n",
              "101     OtherHorse                 3.082718\n",
              "102     OtherHorse                 1.827803\n",
              "103     OtherHorse                 2.774032\n",
              "104     OtherHorse                 0.700000\n",
              "105     OtherHorse                 1.706287\n",
              "106     OtherHorse                 2.536094\n",
              "107     OtherHorse                 2.058802\n",
              "108     OtherHorse                 2.829729\n",
              "109     OtherHorse                 1.046856\n",
              "110     OtherHorse                 3.031122\n",
              "111     OtherHorse                 1.500000\n",
              "112     OtherHorse                 1.536094\n",
              "113     OtherHorse                 1.817117\n",
              "114     OtherHorse                 3.605426\n",
              "115     OtherHorse                 3.982756\n",
              "116     OtherHorse                 2.340068\n",
              "117     OtherHorse                 1.746229\n",
              "118     OtherHorse                 2.206982\n",
              "119       ASTURIAS                 1.875969\n",
              "120        UPSILON                 6.367121\n",
              "121     OtherHorse                 3.585127\n",
              "122     OtherHorse                 3.905426\n",
              "123     OtherHorse                 3.132169\n",
              "124     OtherHorse                10.000000\n",
              "125     OtherHorse                10.000000\n",
              "126   SANCTI PETRI                10.000000\n",
              "127     OtherHorse                10.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7431cf5-3dad-446f-b56d-c8346546c800\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>MediaUltimasActuaciones</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.456720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.896886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.428258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.900193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.574867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>6.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>7.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.654865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.873719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.352437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.966572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.580006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.984048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.085643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.006287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>MONTERREDONDO</td>\n",
              "      <td>2.982756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.969604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.846856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.810328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.546229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.027803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.736026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.946229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.929690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.180006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.173719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.844920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.068910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.427803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.429690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.402948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>1.817117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.500193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.541496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.596151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>3.129690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.044818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.302879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>4.268982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>6.864831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.682756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.853844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.606287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.854865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.968910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>5.516798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.667526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.241496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.205426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.761336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.861545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.521396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.141154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.306989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>8.277705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.146657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>5.523485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.636026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.738131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.119948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.698445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.439328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.700781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.654953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.684048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.382718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.034090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.380156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.146229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.777746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.046856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.195221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.946856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.536094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.481689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.044818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>4.816798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.528672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.410328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.292853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.017117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.484048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.527803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.082718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.827803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.774032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.706287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.536094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.058802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.829729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.046856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.031122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.536094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.817117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.605426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.982756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.340068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.746229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.206982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>1.875969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>UPSILON</td>\n",
              "      <td>6.367121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.585127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.905426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.132169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>SANCTI PETRI</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7431cf5-3dad-446f-b56d-c8346546c800')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7431cf5-3dad-446f-b56d-c8346546c800 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7431cf5-3dad-446f-b56d-c8346546c800');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "train[['NombreCaballo','MediaUltimasActuaciones']].head(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "iu9A7E_1w67d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1920)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "Lv303sRUw8h1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d23cff-e189-43bb-b212-20b68041ffa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1503: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "#Regresión lineal\n",
        "linear = LinearRegression()\n",
        "linear.fit(X_train, y_train)\n",
        "\n",
        "#Regresión de Ridge\n",
        "\n",
        "ridge = Ridge()\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "#LASSO\n",
        "\n",
        "lasso = linear_model.Lasso()\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "#ElasticNet\n",
        "\n",
        "elasticnet = ElasticNet()\n",
        "elasticnet.fit(X_train, y_train);\n",
        "\n",
        "\n",
        "reg = SGDRegressor(max_iter=50, random_state=43)\n",
        "reg.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "rXV9YRpcw-Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3234953b-95ab-48d1-e66c-a22dc7031c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 0 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 1 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 2 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 3 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 4 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 5 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 6 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 7 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 8 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "<ipython-input-195-a43deb5d10a7>:10: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  elasticnet.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 9 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335.96592913662266, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9028421342012\n",
            "0 10 1.337854470890544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481.71516347357425, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1278902947311038\n",
            "1 0 1.2588753532092933\n",
            "ElasticNet (train): 1.1586242277216674\n",
            "1 1 1.2703886504288764\n",
            "ElasticNet (train): 1.172747685633271\n",
            "1 2 1.279803035542373\n",
            "ElasticNet (train): 1.179515617955745\n",
            "1 3 1.2862085445407676\n",
            "ElasticNet (train): 1.1835580951617048\n",
            "1 4 1.2896456948117856\n",
            "ElasticNet (train): 1.1864827023057116\n",
            "1 5 1.2916861282045562\n",
            "ElasticNet (train): 1.1889347949131392\n",
            "1 6 1.294728858604231\n",
            "ElasticNet (train): 1.1905292531170053\n",
            "1 7 1.2966178482749586\n",
            "ElasticNet (train): 1.1908313908588244\n",
            "1 8 1.2972420079716211\n",
            "ElasticNet (train): 1.1911352950059337\n",
            "1 9 1.2978992462309085\n",
            "ElasticNet (train): 1.191453632575042\n",
            "1 10 1.2984353402719253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494.2958901863979, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1490109332072187\n",
            "2 0 1.2707036308483781\n",
            "ElasticNet (train): 1.177554578190977\n",
            "2 1 1.2853444849448505\n",
            "ElasticNet (train): 1.185534514762547\n",
            "2 2 1.2922748768418524\n",
            "ElasticNet (train): 1.1898756724625537\n",
            "2 3 1.2958516486265208\n",
            "ElasticNet (train): 1.1912708341682496\n",
            "2 4 1.297906304858549\n",
            "ElasticNet (train): 1.191906001666409\n",
            "2 5 1.2990287662978266\n",
            "ElasticNet (train): 1.1925534176492483\n",
            "2 6 1.3000184740560738\n",
            "ElasticNet (train): 1.1932683254246774\n",
            "2 7 1.3010680307354323\n",
            "ElasticNet (train): 1.1941175811849396\n",
            "2 8 1.3022792404028065\n",
            "ElasticNet (train): 1.1949759944614637\n",
            "2 9 1.3034994251260488\n",
            "ElasticNet (train): 1.195858534252991\n",
            "2 10 1.3047782620770245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500.2738086253512, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.159309534776906\n",
            "3 0 1.2769302962424458\n",
            "ElasticNet (train): 1.1841809869534867\n",
            "3 1 1.291935964291172\n",
            "ElasticNet (train): 1.1905150416843113\n",
            "3 2 1.2966605207637456\n",
            "ElasticNet (train): 1.1920352211337515\n",
            "3 3 1.2991150970815748\n",
            "ElasticNet (train): 1.1929975575271043\n",
            "3 4 1.3005737820753138\n",
            "ElasticNet (train): 1.1942009820906392\n",
            "3 5 1.3023155148506882\n",
            "ElasticNet (train): 1.1954623681348502\n",
            "3 6 1.304129980480337\n",
            "ElasticNet (train): 1.1967860160824\n",
            "3 7 1.3060947972556627\n",
            "ElasticNet (train): 1.1981873375956085\n",
            "3 8 1.3081354228628863\n",
            "ElasticNet (train): 1.1996663234491016\n",
            "3 9 1.3103114668182445\n",
            "ElasticNet (train): 1.2011915427797994\n",
            "3 10 1.3125114039255228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503.9716209366382, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1654966275989118\n",
            "4 0 1.281022640493458\n",
            "ElasticNet (train): 1.1879004236719728\n",
            "4 1 1.2949596059998358\n",
            "ElasticNet (train): 1.1921604427668164\n",
            "4 2 1.2991513420893008\n",
            "ElasticNet (train): 1.193476768385327\n",
            "4 3 1.3011839111045864\n",
            "ElasticNet (train): 1.1951025209966673\n",
            "4 4 1.3035218030727964\n",
            "ElasticNet (train): 1.1968063411547223\n",
            "4 5 1.3060457040076354\n",
            "ElasticNet (train): 1.198657306145054\n",
            "4 6 1.3087487697091478\n",
            "ElasticNet (train): 1.200609026042133\n",
            "4 7 1.3116079983207714\n",
            "ElasticNet (train): 1.2026567179961198\n",
            "4 8 1.3152385800703141\n",
            "ElasticNet (train): 1.204746411083866\n",
            "4 9 1.3196405789204542\n",
            "ElasticNet (train): 1.2068711346873633\n",
            "4 10 1.324108223978451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506.5745472061813, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1696892065834272\n",
            "5 0 1.2839796879924814\n",
            "ElasticNet (train): 1.190508283432879\n",
            "5 1 1.2968236408914644\n",
            "ElasticNet (train): 1.1932120338190613\n",
            "5 2 1.30067842261657\n",
            "ElasticNet (train): 1.195163152025307\n",
            "5 3 1.30353001875958\n",
            "ElasticNet (train): 1.1972661675771363\n",
            "5 4 1.306643903370641\n",
            "ElasticNet (train): 1.199594565922231\n",
            "5 5 1.3100386690008987\n",
            "ElasticNet (train): 1.2020411063630208\n",
            "5 6 1.3138520986803814\n",
            "ElasticNet (train): 1.2045883638125445\n",
            "5 7 1.3192329090834771\n",
            "ElasticNet (train): 1.2071891797672425\n",
            "5 8 1.3247119003535086\n",
            "ElasticNet (train): 1.209949400011215\n",
            "5 9 1.3302921550794566\n",
            "ElasticNet (train): 1.2128394828020956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 508.55411028990767, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 10 1.3361326845607444\n",
            "ElasticNet (train): 1.1727485285760766\n",
            "6 0 1.286281868937618\n",
            "ElasticNet (train): 1.192049163076692\n",
            "6 1 1.2987521157023147\n",
            "ElasticNet (train): 1.1944360001364152\n",
            "6 2 1.3024068025238895\n",
            "ElasticNet (train): 1.1968465242047335\n",
            "6 3 1.3059475482257508\n",
            "ElasticNet (train): 1.1995624432909617\n",
            "6 4 1.3099074344814525\n",
            "ElasticNet (train): 1.2024416099527098\n",
            "6 5 1.314612862867672\n",
            "ElasticNet (train): 1.2054478822653945\n",
            "6 6 1.320975306094185\n",
            "ElasticNet (train): 1.2085800249438334\n",
            "6 7 1.3274748149136135\n",
            "ElasticNet (train): 1.2118869791056097\n",
            "6 8 1.3341759167395832\n",
            "ElasticNet (train): 1.2155265677961276\n",
            "6 9 1.341344276326653\n",
            "ElasticNet (train): 1.2193817626681192\n",
            "6 10 1.3486772715584012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 510.13885500122456, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1751244119987474\n",
            "7 0 1.2881600736613434\n",
            "ElasticNet (train): 1.1931004821801232\n",
            "7 1 1.3002545671113446\n",
            "ElasticNet (train): 1.1956667432205654\n",
            "7 2 1.3041123454407906\n",
            "ElasticNet (train): 1.1986234749786553\n",
            "7 3 1.308434152391767\n",
            "ElasticNet (train): 1.2018585094899945\n",
            "7 4 1.3132877768941666\n",
            "ElasticNet (train): 1.205281953165349\n",
            "7 5 1.3205531281795129\n",
            "ElasticNet (train): 1.2088623162174454\n",
            "7 6 1.327997501950038\n",
            "ElasticNet (train): 1.2126967763766399\n",
            "7 7 1.3357794322977288\n",
            "ElasticNet (train): 1.2169520654205894\n",
            "7 8 1.3440389926817466\n",
            "ElasticNet (train): 1.2214042630141873\n",
            "7 9 1.3525170787820704\n",
            "ElasticNet (train): 1.2241678099142028\n",
            "7 10 1.3581450773500072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 511.45444172318395, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.177040212996122\n",
            "8 0 1.2897431274517053\n",
            "ElasticNet (train): 1.1938731394107138\n",
            "8 1 1.3013722585587157\n",
            "ElasticNet (train): 1.1968861050783555\n",
            "8 2 1.3058495744039817\n",
            "ElasticNet (train): 1.2004017184122524\n",
            "8 3 1.310983751883517\n",
            "ElasticNet (train): 1.2041572921369454\n",
            "8 4 1.3180919262025765\n",
            "ElasticNet (train): 1.2080967558832298\n",
            "8 5 1.3263905464601689\n",
            "ElasticNet (train): 1.2123489963066847\n",
            "8 6 1.3350411083545328\n",
            "ElasticNet (train): 1.2170972443909458\n",
            "8 7 1.3442980370893614\n",
            "ElasticNet (train): 1.2220932637192163\n",
            "8 8 1.353829093422731\n",
            "ElasticNet (train): 1.224064301307732\n",
            "8 9 1.358256025843164\n",
            "ElasticNet (train): 1.2236684212196274\n",
            "8 10 1.3586912694922098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512.5763510083278, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 1.1786755878996111\n",
            "9 0 1.2911039643797344\n",
            "ElasticNet (train): 1.194662859782762\n",
            "9 1 1.3024754000474856\n",
            "ElasticNet (train): 1.1981745971461921\n",
            "9 2 1.307611932244056\n",
            "ElasticNet (train): 1.2021564524921724\n",
            "9 3 1.3137465603479979\n",
            "ElasticNet (train): 1.2064040326395755\n",
            "9 4 1.3227987326391935\n",
            "ElasticNet (train): 1.210938044163165\n",
            "9 5 1.3321278462838824\n",
            "ElasticNet (train): 1.2160372470545102\n",
            "9 6 1.3422485684844632\n",
            "ElasticNet (train): 1.2215126840244936\n",
            "9 7 1.3527238898596439\n",
            "ElasticNet (train): 1.2240597501904662\n",
            "9 8 1.3582581762865815\n",
            "ElasticNet (train): 1.2236152177163633\n",
            "9 9 1.358747360638933\n",
            "ElasticNet (train): 1.2231689891211421\n",
            "9 10 1.3592374915507583\n",
            "ElasticNet (train): 1.1801175065982115\n",
            "10 0 1.2923145970232963\n",
            "ElasticNet (train): 1.1954436021347041\n",
            "10 1 1.3035463329093873\n",
            "ElasticNet (train): 1.1994556039268982\n",
            "10 2 1.3094123354481815\n",
            "ElasticNet (train): 1.2039015590953113\n",
            "10 3 1.3173919920335317\n",
            "ElasticNet (train): 1.208647128699597\n",
            "10 4 1.3274110661286733\n",
            "ElasticNet (train): 1.213868669218418\n",
            "10 5 1.3380567377140593\n",
            "ElasticNet (train): 1.2197525696332376\n",
            "10 6 1.3493541273342338\n",
            "ElasticNet (train): 1.2241535722831136\n",
            "10 7 1.3581518510140052\n",
            "ElasticNet (train): 1.223660899245464\n",
            "10 8 1.3586947010839086\n",
            "ElasticNet (train): 1.2231662174541202\n",
            "10 9 1.3592386631912436\n",
            "ElasticNet (train): 1.2226978883151942\n",
            "10 10 1.3596810188568054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513.5530055560673, tolerance: 0.1099412228796844\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "for i in range(0, 11):\n",
        "  for j in range(0, 11):\n",
        "    #     xgb_model = xgb.XGBRegressor(objective = 'reg:squarederror', reg_alpha=i*0.1, reg_lambda=j*0.1, verbosity=0, random_state=42, eta=0.2, tree_method=\"exact\") 148\n",
        "\n",
        "    elasticnet = ElasticNet(alpha=i*0.1, l1_ratio=j*0.1)\n",
        "    elasticnet.fit(X_train, y_train)\n",
        "    print('ElasticNet (train): ' + str(mean_absolute_error(y_train, elasticnet.predict(X_train))))\n",
        "    print(f'{i} {j} ' + str(mean_absolute_error(y_test, elasticnet.predict(X_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "1vD8jvFqw_S8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffae51d4-e9f0-4e7c-9f0b-9a672e30cfe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet (train): 0.9103998002215955\n",
            "0 0 1.2951698546409607\n",
            "ElasticNet (train): 0.9121766239698587\n",
            "0 1 1.300767377614975\n",
            "ElasticNet (train): 0.9104476308916682\n",
            "0 2 1.2975009741783141\n",
            "ElasticNet (train): 0.9131557360673562\n",
            "0 3 1.29533113861084\n",
            "ElasticNet (train): 0.9130028991774459\n",
            "0 4 1.3036385104656218\n",
            "ElasticNet (train): 0.9096947264154047\n",
            "0 5 1.2999710421562194\n",
            "ElasticNet (train): 0.9183729937560929\n",
            "0 6 1.2921058421134948\n",
            "ElasticNet (train): 0.9220379071357923\n",
            "0 7 1.2988560318946838\n",
            "ElasticNet (train): 0.9077043289968953\n",
            "0 8 1.2981850728988646\n",
            "ElasticNet (train): 0.9166100664721908\n",
            "0 9 1.2962614097595215\n",
            "ElasticNet (train): 0.9164038267596468\n",
            "0 10 1.2974017872810364\n",
            "ElasticNet (train): 0.9038803937167106\n",
            "1 0 1.3136758151054382\n",
            "ElasticNet (train): 0.9049653834139807\n",
            "1 1 1.3047720527648925\n",
            "ElasticNet (train): 0.9133949045833987\n",
            "1 2 1.3001749293804168\n",
            "ElasticNet (train): 0.9124090573961683\n",
            "1 3 1.31005406165123\n",
            "ElasticNet (train): 0.9047669987001362\n",
            "1 4 1.2986258039474488\n",
            "ElasticNet (train): 0.9142049419339122\n",
            "1 5 1.2987323300838471\n",
            "ElasticNet (train): 0.9147584913280118\n",
            "1 6 1.3007784843444825\n",
            "ElasticNet (train): 0.9025505429659134\n",
            "1 7 1.3011116843223571\n",
            "ElasticNet (train): 0.9027474669544889\n",
            "1 8 1.3002429904937745\n",
            "ElasticNet (train): 0.9137279406572\n",
            "1 9 1.305863706111908\n",
            "ElasticNet (train): 0.9086225733484272\n",
            "1 10 1.3048543643951416\n",
            "ElasticNet (train): 0.9040314525777301\n",
            "2 0 1.2952302753925324\n",
            "ElasticNet (train): 0.9004278338167089\n",
            "2 1 1.2942678093910218\n",
            "ElasticNet (train): 0.9074280547672475\n",
            "2 2 1.2950281791687013\n",
            "ElasticNet (train): 0.9084278099400522\n",
            "2 3 1.3067516655921936\n",
            "ElasticNet (train): 0.9056849606643768\n",
            "2 4 1.3036857171058656\n",
            "ElasticNet (train): 0.9195399443073385\n",
            "2 5 1.3008996920585632\n",
            "ElasticNet (train): 0.9114600923169529\n",
            "2 6 1.3104949522018432\n",
            "ElasticNet (train): 0.9156922688615863\n",
            "2 7 1.2981106958389281\n",
            "ElasticNet (train): 0.9079031195396032\n",
            "2 8 1.295571307659149\n",
            "ElasticNet (train): 0.9071889830058848\n",
            "2 9 1.2980266075134277\n",
            "ElasticNet (train): 0.9175044097138579\n",
            "2 10 1.3002158994674682\n",
            "ElasticNet (train): 0.9042041804898655\n",
            "3 0 1.293726257801056\n",
            "ElasticNet (train): 0.8994704279203621\n",
            "3 1 1.2939410581588746\n",
            "ElasticNet (train): 0.907811100783903\n",
            "3 2 1.2931349053382875\n",
            "ElasticNet (train): 0.9017410852029714\n",
            "3 3 1.3029623064994813\n",
            "ElasticNet (train): 0.9116002095521555\n",
            "3 4 1.2956390790939332\n",
            "ElasticNet (train): 0.9042998787449191\n",
            "3 5 1.3094667377471925\n",
            "ElasticNet (train): 0.9118367065574525\n",
            "3 6 1.3062304329872132\n",
            "ElasticNet (train): 0.9043024747799604\n",
            "3 7 1.2974452896118165\n",
            "ElasticNet (train): 0.9088534496474784\n",
            "3 8 1.2998893857002258\n",
            "ElasticNet (train): 0.9129164141310743\n",
            "3 9 1.2993826262950898\n",
            "ElasticNet (train): 0.9148782653921455\n",
            "3 10 1.3041829614639282\n",
            "ElasticNet (train): 0.896444235562808\n",
            "4 0 1.2954256114959717\n",
            "ElasticNet (train): 0.9109280174065388\n",
            "4 1 1.2841384654045105\n",
            "ElasticNet (train): 0.9063224136476686\n",
            "4 2 1.2986977519989014\n",
            "ElasticNet (train): 0.9029241418227171\n",
            "4 3 1.3006833324432372\n",
            "ElasticNet (train): 0.8960156679388569\n",
            "4 4 1.3058210096359253\n",
            "ElasticNet (train): 0.9092522939516477\n",
            "4 5 1.2982395839691163\n",
            "ElasticNet (train): 0.9058368785611741\n",
            "4 6 1.2989533171653747\n",
            "ElasticNet (train): 0.9144371015550588\n",
            "4 7 1.3035352792739867\n",
            "ElasticNet (train): 0.9069612096752641\n",
            "4 8 1.2984018177986145\n",
            "ElasticNet (train): 0.9137039256283987\n",
            "4 9 1.2898703017234803\n",
            "ElasticNet (train): 0.9156675383419209\n",
            "4 10 1.3023279252052307\n",
            "ElasticNet (train): 0.904295936961616\n",
            "5 0 1.2964172849655151\n",
            "ElasticNet (train): 0.9024566792877469\n",
            "5 1 1.293303560256958\n",
            "ElasticNet (train): 0.9106102713936649\n",
            "5 2 1.2979483122825624\n",
            "ElasticNet (train): 0.9108917467457773\n",
            "5 3 1.2952998180389403\n",
            "ElasticNet (train): 0.8960678241191766\n",
            "5 4 1.304694589614868\n",
            "ElasticNet (train): 0.8998568121498153\n",
            "5 5 1.3096574959754943\n",
            "ElasticNet (train): 0.9139960013197724\n",
            "5 6 1.2981054549217224\n",
            "ElasticNet (train): 0.9078562840437278\n",
            "5 7 1.2859028968811035\n",
            "ElasticNet (train): 0.9085686385513998\n",
            "5 8 1.3042278337478637\n",
            "ElasticNet (train): 0.9100523188974731\n",
            "5 9 1.302355839252472\n",
            "ElasticNet (train): 0.9117104096055266\n",
            "5 10 1.2999613218307495\n",
            "ElasticNet (train): 0.9018387960259026\n",
            "6 0 1.291466656446457\n",
            "ElasticNet (train): 0.8997912137701196\n",
            "6 1 1.2976857416629792\n",
            "ElasticNet (train): 0.9028066195679839\n",
            "6 2 1.298002697944641\n",
            "ElasticNet (train): 0.9012142911230083\n",
            "6 3 1.3032856540679931\n",
            "ElasticNet (train): 0.9039050770934517\n",
            "6 4 1.3014040417671204\n",
            "ElasticNet (train): 0.9051844195501338\n",
            "6 5 1.3027008876800537\n",
            "ElasticNet (train): 0.9088363283014391\n",
            "6 6 1.3009606285095214\n",
            "ElasticNet (train): 0.9031651518518873\n",
            "6 7 1.3040702123641967\n",
            "ElasticNet (train): 0.9019379076167676\n",
            "6 8 1.3000024929046632\n",
            "ElasticNet (train): 0.9066108966014794\n",
            "6 9 1.2965848546028138\n",
            "ElasticNet (train): 0.9129615152140811\n",
            "6 10 1.2913054842948914\n",
            "ElasticNet (train): 0.8991429895103801\n",
            "7 0 1.3006668219566344\n",
            "ElasticNet (train): 0.8972089961905921\n",
            "7 1 1.3010400898456573\n",
            "ElasticNet (train): 0.8978384801386846\n",
            "7 2 1.2919144270420075\n",
            "ElasticNet (train): 0.9093990795005708\n",
            "7 3 1.2946177439689637\n",
            "ElasticNet (train): 0.9007606166589425\n",
            "7 4 1.2942085366249085\n",
            "ElasticNet (train): 0.9014304624741835\n",
            "7 5 1.2873669757843018\n",
            "ElasticNet (train): 0.9036552553346171\n",
            "7 6 1.3034534163475036\n",
            "ElasticNet (train): 0.9012933744951583\n",
            "7 7 1.2997230100631714\n",
            "ElasticNet (train): 0.912081887148306\n",
            "7 8 1.2987899222373962\n",
            "ElasticNet (train): 0.9095575296902327\n",
            "7 9 1.2959020681381226\n",
            "ElasticNet (train): 0.9161691886668609\n",
            "7 10 1.2955090279579162\n",
            "ElasticNet (train): 0.9017540007183067\n",
            "8 0 1.2873359825611115\n",
            "ElasticNet (train): 0.8967223829301386\n",
            "8 1 1.291710572719574\n",
            "ElasticNet (train): 0.9041044184206976\n",
            "8 2 1.299543474674225\n",
            "ElasticNet (train): 0.902768828342182\n",
            "8 3 1.2872023100852967\n",
            "ElasticNet (train): 0.8982160974301295\n",
            "8 4 1.2970449342727661\n",
            "ElasticNet (train): 0.8935836018660129\n",
            "8 5 1.300418426990509\n",
            "ElasticNet (train): 0.8987724856276954\n",
            "8 6 1.2899846873283387\n",
            "ElasticNet (train): 0.9116628295806268\n",
            "8 7 1.3073162388801576\n",
            "ElasticNet (train): 0.909304333273946\n",
            "8 8 1.2904376354217528\n",
            "ElasticNet (train): 0.9034749178961654\n",
            "8 9 1.2957355303764344\n",
            "ElasticNet (train): 0.9199560352801336\n",
            "8 10 1.2975201210975646\n",
            "ElasticNet (train): 0.9038079618467146\n",
            "9 0 1.2887379474639893\n",
            "ElasticNet (train): 0.9051025239438465\n",
            "9 1 1.2912139008045196\n",
            "ElasticNet (train): 0.902280335360495\n",
            "9 2 1.2970013990402223\n",
            "ElasticNet (train): 0.9054520019647873\n",
            "9 3 1.2970890884399413\n",
            "ElasticNet (train): 0.8959470925011344\n",
            "9 4 1.2951171436309814\n",
            "ElasticNet (train): 0.9047191498077363\n",
            "9 5 1.3039341130256652\n",
            "ElasticNet (train): 0.903801027604579\n",
            "9 6 1.2995102334022521\n",
            "ElasticNet (train): 0.910279574243745\n",
            "9 7 1.3008949089050292\n",
            "ElasticNet (train): 0.9074481897335316\n",
            "9 8 1.2866010475158691\n",
            "ElasticNet (train): 0.912719119347529\n",
            "9 9 1.2976964702606202\n",
            "ElasticNet (train): 0.9085136375718803\n",
            "9 10 1.2943480515480041\n",
            "ElasticNet (train): 0.9021522526204939\n",
            "10 0 1.308073257446289\n",
            "ElasticNet (train): 0.895361533884466\n",
            "10 1 1.294717613220215\n",
            "ElasticNet (train): 0.9048404367952892\n",
            "10 2 1.2936308040618896\n",
            "ElasticNet (train): 0.8886562409485586\n",
            "10 3 1.283225781917572\n",
            "ElasticNet (train): 0.896424501370161\n",
            "10 4 1.288433879137039\n",
            "ElasticNet (train): 0.9034397261147433\n",
            "10 5 1.2953373913764954\n",
            "ElasticNet (train): 0.9032069635109083\n",
            "10 6 1.3028435850143432\n",
            "ElasticNet (train): 0.9008031826047502\n",
            "10 7 1.2826478571891784\n",
            "ElasticNet (train): 0.9053895867317621\n",
            "10 8 1.284021426677704\n",
            "ElasticNet (train): 0.9061284662468664\n",
            "10 9 1.2937927699089051\n",
            "ElasticNet (train): 0.9073781855477854\n",
            "10 10 1.2931113753318786\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "for i in range(0, 11):\n",
        "  for j in range(0, 11):\n",
        "    #     xgb_model = xgb.XGBRegressor(objective = 'reg:squarederror', reg_alpha=i*0.1, reg_lambda=j*0.1, verbosity=0, random_state=42, eta=0.2, tree_method=\"exact\") 148\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(objective = 'reg:squarederror', reg_alpha=i*0.1, reg_lambda=j*0.1, verbosity=0, random_state=42, eta=0.2, tree_method=\"exact\") #subsample ojo\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    print('ElasticNet (train): ' + str(mean_absolute_error(y_train, xgb_model.predict(X_train))))\n",
        "    print(f'{i} {j} ' + str(mean_absolute_error(y_test, xgb_model.predict(X_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "VcXpNFGq4cV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c899a3-2b47-4d79-a570-5f1d8b65ae27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(eta=0.2, objective='reg:squarederror', random_state=42,\n",
              "             reg_alpha=0.7000000000000001, reg_lambda=0.4, tree_method='exact',\n",
              "             verbosity=0)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective = 'reg:squarederror', reg_alpha=7*0.1, reg_lambda=4*0.1, verbosity=0, random_state=42, eta=0.2, tree_method=\"exact\")\n",
        "\n",
        "xgb_model.fit(X, y)\n",
        "#print('Lineal (train): ' + str(mean_absolute_error(y_train, xgb_model.predict(X))))\n",
        "#print('Lineal (test): ' + str(mean_absolute_error(y_test, xgb_model.predict(X_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "ynw-X-hr4e1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554b87cf-18fa-45aa-b3c3-ae9a86ae2572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from catboost) (1.7.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from catboost) (1.21.6)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->catboost) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->catboost) (8.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "import catboost as cb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "a1gcUS2H4gnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fafa90-cca2-48c1-d032-12089a217f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 1.8448877\ttotal: 84.4ms\tremaining: 4.14s\n",
            "1:\tlearn: 1.7682381\ttotal: 159ms\tremaining: 3.81s\n",
            "2:\tlearn: 1.6979468\ttotal: 244ms\tremaining: 3.83s\n",
            "3:\tlearn: 1.6421566\ttotal: 328ms\tremaining: 3.78s\n",
            "4:\tlearn: 1.5845647\ttotal: 402ms\tremaining: 3.62s\n",
            "5:\tlearn: 1.5369004\ttotal: 501ms\tremaining: 3.67s\n",
            "6:\tlearn: 1.4925118\ttotal: 684ms\tremaining: 4.2s\n",
            "7:\tlearn: 1.4578697\ttotal: 846ms\tremaining: 4.44s\n",
            "8:\tlearn: 1.4174278\ttotal: 1.04s\tremaining: 4.75s\n",
            "9:\tlearn: 1.3965480\ttotal: 1.22s\tremaining: 4.87s\n",
            "10:\tlearn: 1.3572059\ttotal: 1.34s\tremaining: 4.74s\n",
            "11:\tlearn: 1.3159674\ttotal: 1.42s\tremaining: 4.5s\n",
            "12:\tlearn: 1.2748090\ttotal: 1.51s\tremaining: 4.31s\n",
            "13:\tlearn: 1.2392224\ttotal: 1.6s\tremaining: 4.11s\n",
            "14:\tlearn: 1.2006381\ttotal: 1.68s\tremaining: 3.93s\n",
            "15:\tlearn: 1.1650875\ttotal: 1.77s\tremaining: 3.76s\n",
            "16:\tlearn: 1.1294140\ttotal: 1.85s\tremaining: 3.59s\n",
            "17:\tlearn: 1.1007956\ttotal: 1.92s\tremaining: 3.42s\n",
            "18:\tlearn: 1.0658284\ttotal: 2.01s\tremaining: 3.27s\n",
            "19:\tlearn: 1.0343983\ttotal: 2.15s\tremaining: 3.22s\n",
            "20:\tlearn: 1.0048667\ttotal: 2.3s\tremaining: 3.18s\n",
            "21:\tlearn: 0.9751001\ttotal: 2.45s\tremaining: 3.11s\n",
            "22:\tlearn: 0.9484930\ttotal: 2.53s\tremaining: 2.97s\n",
            "23:\tlearn: 0.9201593\ttotal: 2.63s\tremaining: 2.84s\n",
            "24:\tlearn: 0.8925679\ttotal: 2.72s\tremaining: 2.72s\n",
            "25:\tlearn: 0.8656103\ttotal: 2.8s\tremaining: 2.58s\n",
            "26:\tlearn: 0.8433013\ttotal: 2.87s\tremaining: 2.44s\n",
            "27:\tlearn: 0.8198308\ttotal: 2.96s\tremaining: 2.32s\n",
            "28:\tlearn: 0.8007679\ttotal: 3.03s\tremaining: 2.19s\n",
            "29:\tlearn: 0.7809390\ttotal: 3.11s\tremaining: 2.07s\n",
            "30:\tlearn: 0.7647544\ttotal: 3.2s\tremaining: 1.96s\n",
            "31:\tlearn: 0.7426204\ttotal: 3.29s\tremaining: 1.85s\n",
            "32:\tlearn: 0.7328620\ttotal: 3.37s\tremaining: 1.73s\n",
            "33:\tlearn: 0.7115143\ttotal: 3.45s\tremaining: 1.62s\n",
            "34:\tlearn: 0.6954036\ttotal: 3.54s\tremaining: 1.51s\n",
            "35:\tlearn: 0.6812025\ttotal: 3.63s\tremaining: 1.41s\n",
            "36:\tlearn: 0.6622740\ttotal: 3.83s\tremaining: 1.35s\n",
            "37:\tlearn: 0.6446679\ttotal: 3.97s\tremaining: 1.25s\n",
            "38:\tlearn: 0.6299466\ttotal: 4.13s\tremaining: 1.16s\n",
            "39:\tlearn: 0.6204608\ttotal: 4.25s\tremaining: 1.06s\n",
            "40:\tlearn: 0.6050361\ttotal: 4.42s\tremaining: 970ms\n",
            "41:\tlearn: 0.5911874\ttotal: 4.55s\tremaining: 867ms\n",
            "42:\tlearn: 0.5793251\ttotal: 4.74s\tremaining: 772ms\n",
            "43:\tlearn: 0.5651039\ttotal: 4.92s\tremaining: 670ms\n",
            "44:\tlearn: 0.5514710\ttotal: 5.09s\tremaining: 566ms\n",
            "45:\tlearn: 0.5400603\ttotal: 5.25s\tremaining: 456ms\n",
            "46:\tlearn: 0.5280011\ttotal: 5.43s\tremaining: 346ms\n",
            "47:\tlearn: 0.5185552\ttotal: 5.61s\tremaining: 234ms\n",
            "48:\tlearn: 0.5109338\ttotal: 5.77s\tremaining: 118ms\n",
            "49:\tlearn: 0.4995026\ttotal: 5.93s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7f3f88b02580>"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "params = {\"depth\": 10, \"learning_rate\": 0.5, \"iterations\": 50, \"l2_leaf_reg\": 5} #Iterations CatBoost\n",
        "model_cat_tun = cb.CatBoostClassifier(**params)\n",
        "model_cat_tun.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "jSm-yQLI4j7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921e03e0-508f-4f71-d321-86b3bf9971ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lineal (train): 0.9028850274850805\n",
            "Lineal (test): 1.335001565668233\n",
            "---\n",
            "Ridge (train): 0.9753792057581869\n",
            "Ridge (test): 1.2815609680885665\n",
            "---\n",
            "LASSO (train): 1.2226978883151942\n",
            "LASSO (test): 1.3596810188568054\n",
            "---\n",
            "ElasticNet (train): 1.2226978883151942\n",
            "ElasticNet (test): 1.3596810188568054\n",
            "---\n",
            "SGD (train): 1956110423856417.2\n",
            "SGD (test): 2002481948676912.8\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "print('Lineal (train): ' + str(mean_absolute_error(y_train, linear.predict(X_train))))\n",
        "print('Lineal (test): ' + str(mean_absolute_error(y_test, linear.predict(X_test))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('Ridge (train): ' + str(mean_absolute_error(y_train, ridge.predict(X_train))))\n",
        "print('Ridge (test): ' + str(mean_absolute_error(y_test, ridge.predict(X_test))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('LASSO (train): ' + str(mean_absolute_error(y_train, lasso.predict(X_train))))\n",
        "print('LASSO (test): ' + str(mean_absolute_error(y_test, lasso.predict(X_test))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('ElasticNet (train): ' + str(mean_absolute_error(y_train, elasticnet.predict(X_train))))\n",
        "print('ElasticNet (test): ' + str(mean_absolute_error(y_test, elasticnet.predict(X_test))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('SGD (train): ' + str(mean_absolute_error(y_train, reg.predict(X_train))))\n",
        "print('SGD (test): ' + str(mean_absolute_error(y_test, reg.predict(X_test))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "IcRcAhfc4lme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea38561-6205-41e7-8172-847a637247d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lineal (train): 0.3888262108880687\n",
            "Lineal (test): -0.07716943367481188\n",
            "---\n",
            "Ridge (train): 0.33214287841514645\n",
            "Ridge (test): 0.05001608973712701\n",
            "---\n",
            "LASSO (train): 0.007251217692580081\n",
            "LASSO (test): 0.0048035086549249595\n",
            "---\n",
            "ElasticNet (train): 0.007251217692580081\n",
            "ElasticNet (test): 0.0048035086549249595\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Lineal (train): ' + str(linear.score(X_train, y_train)))\n",
        "print('Lineal (test): ' + str(linear.score(X_test, y_test)))\n",
        "print('---')\n",
        "print('Ridge (train): ' + str(ridge.score(X_train, y_train)))\n",
        "print('Ridge (test): ' + str(ridge.score(X_test, y_test)))\n",
        "print('---')\n",
        "print('LASSO (train): ' + str(lasso.score(X_train, y_train)))\n",
        "print('LASSO (test): ' + str(lasso.score(X_test, y_test)))\n",
        "print('---')\n",
        "print('ElasticNet (train): ' + str(elasticnet.score(X_train, y_train)))\n",
        "print('ElasticNet (test): ' + str(elasticnet.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "09rf5dY04muJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dabfe4b-d638-4beb-ceaa-349f74c9b53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lineal (train): 1.1512208918165758\n",
            "Lineal (test): 1.7092821396878295\n",
            "---\n",
            "Ridge (train): 1.2034223769221757\n",
            "Ridge (test): 1.6052026536318584\n",
            "---\n",
            "LASSO (train): 1.4672225175345628\n",
            "LASSO (test): 1.6429568681340774\n",
            "---\n",
            "ElasticNet (train): 1.4672225175345628\n",
            "ElasticNet (test): 1.6429568681340774\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "print('Lineal (train): ' + str(sqrt(mean_squared_error(y_train, linear.predict(X_train)))))\n",
        "print('Lineal (test): ' + str(sqrt(mean_squared_error(y_test, linear.predict(X_test)))))\n",
        "print('---')\n",
        "print('Ridge (train): ' + str(sqrt(mean_squared_error(y_train, ridge.predict(X_train)))))\n",
        "print('Ridge (test): ' + str(sqrt(mean_squared_error(y_test, ridge.predict(X_test)))))\n",
        "print('---')\n",
        "print('LASSO (train): ' + str(sqrt(mean_squared_error(y_train, lasso.predict(X_train)))))\n",
        "print('LASSO (test): ' + str(sqrt(mean_squared_error(y_test, lasso.predict(X_test)))))\n",
        "print('---')\n",
        "print('ElasticNet (train): ' + str(sqrt(mean_squared_error(y_train, elasticnet.predict(X_train)))))\n",
        "print('ElasticNet (test): ' + str(sqrt(mean_squared_error(y_test, elasticnet.predict(X_test)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQU4pBF6XZaV"
      },
      "source": [
        "# Optimización\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1VOY56rAXmq"
      },
      "source": [
        "Linear\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "mY8EIhB7_gVL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "AzNSozol_20c"
      },
      "outputs": [],
      "source": [
        "fit_intercept = [True, False]\n",
        "copy_x = [True, False]\n",
        "normalize = [True, False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "PgQ2RTUi-GCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ff97ce-b9e1-4e14-88bf-ad7a1a10d860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LinearRegression(),\n",
              "             param_grid={'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'normalize': [True, False]})"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ],
      "source": [
        "linear_Grid = GridSearchCV(\n",
        "  estimator = LinearRegression(),\n",
        "  param_grid = dict(fit_intercept = fit_intercept, copy_X = copy_x, normalize = normalize),\n",
        "  cv = 5\n",
        ")\n",
        "linear_Grid.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "0a0YfqJPh_zr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f17102-a6f7-4fd8-fa95-fb60f290e135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(normalize=True)"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "linear_Grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "3EruRja-AIst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9438b851-8d50-4c71-8fdd-dbfe122a3149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'copy_X': True, 'fit_intercept': True, 'normalize': True}"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "linear_Grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "25Nb13pTG_y6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee72797f-20de-4873-cff9-628ea34346b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.3477893754513567"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "linear_Grid.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "300m24n9AqOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db397d48-1ea2-43a4-9565-543633362e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(normalize=False)"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ],
      "source": [
        "linear = LinearRegression(fit_intercept=True, normalize=False, copy_X=True)\n",
        "linear.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "gTpG2tRlCMVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5204f6-f82d-43aa-87ce-2853f4134757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: \n",
            "Train: 0.9028850274850805\n",
            "Test: 1.335001565668233\n",
            "---------------------------------------\n",
            "R^2: \n",
            "Train: 0.3888262108880687\n",
            "Test: -0.07716943367481188\n",
            "---------------------------------------\n",
            "RMST: \n",
            "Train: 1.1512208918165758\n",
            "Test: 1.1512208918165758\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE: \")\n",
        "print('Train: ' + str(mean_absolute_error(y_train, linear.predict(X_train))))\n",
        "print('Test: ' + str(mean_absolute_error(y_test, linear.predict(X_test))))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"R^2: \")\n",
        "print(\"Train: \" + str(linear.score(X_train, y_train)))\n",
        "print(\"Test: \" + str(linear.score(X_test, y_test)))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"RMST: \")\n",
        "print('Train: ' + str(sqrt(mean_squared_error(y_train, linear.predict(X_train)))))\n",
        "print('Test: ' + str(sqrt(mean_squared_error(y_train, linear.predict(X_train)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laYT9xJwDQeC"
      },
      "source": [
        "Se puede observar que no mejora ya que los parámetros que nos da el GridSearch son los mismos que tiene de base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqJKZLOSDsyB"
      },
      "source": [
        "Ridge\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "D8RFc_ufDxdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a587df69-d6c3-4d4e-e285-c3d392fe8afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Ridge(),\n",
              "             param_grid={'alpha': array([0.        , 0.02040816, 0.04081633, 0.06122449, 0.08163265,\n",
              "       0.10204082, 0.12244898, 0.14285714, 0.16326531, 0.18367347,\n",
              "       0.20408163, 0.2244898 , 0.24489796, 0.26530612, 0.28571429,\n",
              "       0.30612245, 0.32653061, 0.34693878, 0.36734694, 0.3877551 ,\n",
              "       0.40816327, 0.42857143, 0.44897959, 0.46938776, 0.48979592,\n",
              "       0.51020408, 0.53061224, 0.55102041, 0.57142857, 0.59183673,\n",
              "       0.6122449 , 0.63265306, 0.65306122, 0.67346939, 0.69387755,\n",
              "       0.71428571, 0.73469388, 0.75510204, 0.7755102 , 0.79591837,\n",
              "       0.81632653, 0.83673469, 0.85714286, 0.87755102, 0.89795918,\n",
              "       0.91836735, 0.93877551, 0.95918367, 0.97959184, 1.        ]),\n",
              "                         'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'normalize': [True, False]})"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ],
      "source": [
        "ridge_Grid = GridSearchCV(\n",
        "  estimator = Ridge(),\n",
        "  param_grid = dict(alpha= np.linspace(0, 1, 50), copy_X = copy_x, fit_intercept=fit_intercept, normalize = normalize),\n",
        "  cv = 5\n",
        "\n",
        ")\n",
        "ridge_Grid.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "uVTNrcR3l3w7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfaafe1d-1952-4c12-a740-c92945052ad1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge(normalize=True)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "ridge_Grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "RUYsSh3kDxf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1298ff37-b5e5-426d-900e-33aa899daa50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'normalize': True}"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "ridge_Grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "KLxVAk5jl6pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43df636d-b9e0-4298-c2e0-1612e73c36c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.022362520492124727"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "ridge_Grid.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "BPfYc8maDxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9635176-a4c9-4e50-bebc-38a7e1df6a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ridge(alpha=0.32653061224489793, normalize=True, random_state=1337)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "ridge = Ridge(alpha=0.32653061224489793, copy_X=True, fit_intercept=True,\n",
        "      normalize=True, random_state=1337, solver='auto', tol=0.001)\n",
        "ridge.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "gGPOsyq5D10p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3168392c-a66a-4d9e-fe44-c5e13e5052b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: \n",
            "Train: 0.9710771736393949\n",
            "Test: 1.298536304868284\n",
            "---------------------------------------\n",
            "R^2: \n",
            "Train: 0.33277573799376514\n",
            "Test: 0.03003585643963369\n",
            "---------------------------------------\n",
            "RMST: \n",
            "Train: 1.2028520619093006\n",
            "Test: 1.6219952728932197\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE: \")\n",
        "print('Train: ' + str(mean_absolute_error(y_train, ridge.predict(X_train))))\n",
        "print('Test: ' + str(mean_absolute_error(y_test, ridge.predict(X_test))))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"R^2: \")\n",
        "print(\"Train: \" + str(ridge.score(X_train, y_train)))\n",
        "print(\"Test: \" + str(ridge.score(X_test, y_test)))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"RMST: \")\n",
        "print('Train: ' + str(sqrt(mean_squared_error(y_train, ridge.predict(X_train)))))\n",
        "print('Test: ' + str(sqrt(mean_squared_error(y_test, ridge.predict(X_test)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81swjhxYklmc"
      },
      "source": [
        "Parece que empeora ligeramente con respecto a los valores por defecto que tenía antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoNlsctKMr42"
      },
      "source": [
        "LASSO\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "0iM6cHwP2ldQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd9c6e2-3c09-422d-f733-81934dacfa14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=Lasso(),\n",
              "             param_grid={'alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
              "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])})"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "lasso_Grid = GridSearchCV(estimator = linear_model.Lasso(), param_grid = dict(alpha= np.linspace(0.05,1,20)), cv = 5)\n",
        "lasso_Grid.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "alIQtheS2llV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4dd23a2-db44-4815-eec9-cfd573756bae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.35}"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "lasso_Grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "wQ6NsKWS2ls1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b415b61-4fe4-4da2-a70a-b4123a073296"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=1, random_state=1337)"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ],
      "source": [
        "lasso = linear_model.Lasso(alpha=1, random_state=1337)\n",
        "lasso.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "BK4GQtLL2l0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa16604-5c21-4cd5-a568-cd83e9a91c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: \n",
            "Train: 1.2226978883151942\n",
            "Test: 1.3596810188568054\n",
            "---------------------------------------\n",
            "R^2: \n",
            "Train: 0.007251217692580081\n",
            "Test: 0.0048035086549249595\n",
            "---------------------------------------\n",
            "RMST: \n",
            "Train: 1.4672225175345628\n",
            "Test: 1.4672225175345628\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE: \")\n",
        "print('Train: ' + str(mean_absolute_error(y_train, lasso.predict(X_train))))\n",
        "print('Test: ' + str(mean_absolute_error(y_test, lasso.predict(X_test))))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"R^2: \")\n",
        "print(\"Train: \" + str(lasso.score(X_train, y_train)))\n",
        "print(\"Test: \" + str(lasso.score(X_test, y_test)))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"RMST: \")\n",
        "print('Train: ' + str(sqrt(mean_squared_error(y_train, lasso.predict(X_train)))))\n",
        "print('Test: ' + str(sqrt(mean_squared_error(y_train, lasso.predict(X_train)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyydY4zBIIaC"
      },
      "source": [
        "Se queda igual que con los valores de base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtAIFzIZOx93"
      },
      "source": [
        "ElasticNet\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9eChANL709v"
      },
      "source": [
        "Este regresor a veces funciona y a veces explota, dejamos guardado los resultados de cuando ha funcionado. De todos modos nos sale el siguiente error:<br>\n",
        "\n",
        "\n",
        "```\n",
        "# /usr/local/lib/python3.6/dist-packages/sklearn linear_model_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "F8Qp6_727e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355c645f-0663-4f8d-82ca-78e97ef2328d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 597.9366180338466, tolerance: 0.13841917355371908\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 619.1500923918794, tolerance: 0.14444892561983472\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 613.0933531965605, tolerance: 0.1382963696369637\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 593.6336872300827, tolerance: 0.14137953795379535\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 649.2613423031122, tolerance: 0.1484237623762377\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 636.3048449267998, tolerance: 0.13841917355371908\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 662.8190854421795, tolerance: 0.14444892561983472\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 647.4876622414037, tolerance: 0.1382963696369637\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 636.5819655727646, tolerance: 0.14137953795379535\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691.6142847579152, tolerance: 0.1484237623762377\n",
            "  model = cd_fast.sparse_enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=ElasticNet(),\n",
              "             param_grid={'alpha': array([0.1, 1. ]),\n",
              "                         'l1_ratio': array([0., 1.])})"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ],
      "source": [
        "elastic_Grid = GridSearchCV(\n",
        "  estimator = ElasticNet(),\n",
        "  param_grid = dict(alpha=np.linspace(0.1,1,2), l1_ratio=np.linspace(0,1,2)),\n",
        "  cv = 5\n",
        ")\n",
        "elastic_Grid.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "kDpl5-JtmrlO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25551fa-1495-4298-f9a3-d972e6b90dfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 1.0, 'l1_ratio': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ],
      "source": [
        "elastic_Grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "PhiAH0PEmrGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7ae1bb-1df1-4770-dae6-500d2df4b96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElasticNet(l1_ratio=1, normalize=False)"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "elasticnet = ElasticNet(alpha = 1.0, l1_ratio = 1, fit_intercept=True, normalize=False, copy_X=True)\n",
        "elasticnet.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "N6QpoZnWmqB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240398d2-0be7-4317-9d53-b00786c8d7b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: \n",
            "Train: 1.2226978883151942\n",
            "Test: 1.3596810188568054\n",
            "---------------------------------------\n",
            "R^2: \n",
            "Train: 0.007251217692580081\n",
            "Test: 0.0048035086549249595\n",
            "---------------------------------------\n",
            "RMST: \n",
            "Train: 1.4672225175345628\n",
            "Test: 1.4672225175345628\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE: \")\n",
        "print('Train: ' + str(mean_absolute_error(y_train, elasticnet.predict(X_train))))\n",
        "print('Test: ' + str(mean_absolute_error(y_test, elasticnet.predict(X_test))))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"R^2: \")\n",
        "print(\"Train: \" + str(lasso.score(X_train, y_train)))\n",
        "print(\"Test: \" + str(lasso.score(X_test, y_test)))\n",
        "print(\"---------------------------------------\")\n",
        "print(\"RMST: \")\n",
        "print('Train: ' + str(sqrt(mean_squared_error(y_train, elasticnet.predict(X_train)))))\n",
        "print('Test: ' + str(sqrt(mean_squared_error(y_train, elasticnet.predict(X_train)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RikP5HVUSW"
      },
      "source": [
        "# Validación Cruzada\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJkv475twgAc"
      },
      "source": [
        "## Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV_DNK0Cwu29"
      },
      "source": [
        "MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "N_57qnnBvv7A"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "XOPNmINIv5fu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e8e1ca-842a-4496-d98c-39e9843da7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-1.36044628 -1.25558364 -1.26022313 -1.46536687 -1.59005947]\n",
            "Média métricas de validación cruzada: -1.3863358779003492\n"
          ]
        }
      ],
      "source": [
        "cv_scores_linearMAE = cross_val_score(\n",
        "                estimator = linear,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_absolute_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_linearMAE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_linearMAE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhfmfvvwyP8"
      },
      "source": [
        "R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "F6Q4gC0bLXNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2856bb75-39b2-40c7-9f8c-3e841d675b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-0.0579797  -0.25618733  0.02609191 -0.38587288 -1.10204429]\n",
            "Média métricas de validación cruzada: -0.35519845716217163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_linearR = cross_val_score(\n",
        "                estimator = linear,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'r2',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_linearR}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_linearR.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCEc01CHw1ZE"
      },
      "source": [
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "4QPkbFjhLXlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266d4a7e-78cf-46c8-b7bc-d9209f159485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-2.72842818 -2.75633728 -2.53743186 -3.34248154 -3.97213632]\n",
            "Média métricas de validación cruzada: -3.067363034192203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_linearRMSE = cross_val_score(\n",
        "                estimator = linear,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_linearRMSE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_linearRMSE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX7A5lFxKi1Z"
      },
      "source": [
        "## Ridge\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7T6L4eLK5cY"
      },
      "source": [
        "MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "eP244yZaL6OY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf28b54-f608-47be-eee3-c47cb6b063ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-1.32778081 -1.21040626 -1.2488882  -1.30913972 -1.15525738]\n",
            "Média métricas de validación cruzada: -1.250294473480984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_ridgeMAE = cross_val_score(\n",
        "                estimator = ridge,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_absolute_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_ridgeMAE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_ridgeMAE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyasqzbTK7nH"
      },
      "source": [
        "R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "1qB7zRVCL663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd54fdbf-b39b-46e2-9163-c0e68d18098a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [ 0.00951156 -0.05911228  0.08809371 -0.10380153 -0.05896605]\n",
            "Média métricas de validación cruzada: -0.024854914825443063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_ridgeR = cross_val_score(\n",
        "                estimator = ridge,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'r2',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_ridgeR}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_ridgeR.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vm-JkDoK9DX"
      },
      "source": [
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "i4gq6Dc6L7W3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a91a3-bcd1-44d1-f849-bb73c215a0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-2.55437468 -2.32391347 -2.37589162 -2.66217506 -2.00107939]\n",
            "Média métricas de validación cruzada: -2.3834868437349415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * n_samples. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_ridgeRMSE = cross_val_score(\n",
        "                estimator = ridge,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_ridgeRMSE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_ridgeRMSE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hIBAw3uKm1f"
      },
      "source": [
        "## LASSO\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5XQ5xWFK_IW"
      },
      "source": [
        "MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "ihKKRPvOMj04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c6fc19-8a12-4781-f3ff-8139fc4202b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-1.35388683 -1.23502737 -1.33767429 -1.29706447 -1.18485417]\n",
            "Média métricas de validación cruzada: -1.2817014287884163\n"
          ]
        }
      ],
      "source": [
        "cv_scores_lassoMAE = cross_val_score(\n",
        "                estimator = lasso,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_absolute_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_lassoMAE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_lassoMAE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7RkvicIK_8e"
      },
      "source": [
        "R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "J40hMq5CMkR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbae7243-d256-4818-d7e9-97bb319df185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [ 0.00380696 -0.00518962 -0.00204061 -0.00228847 -0.0441597 ]\n",
            "Média métricas de validación cruzada: -0.009974286362579554\n"
          ]
        }
      ],
      "source": [
        "cv_scores_lassoR = cross_val_score(\n",
        "                estimator = lasso,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'r2',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_lassoR}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_lassoR.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8LuPfVZLBGP"
      },
      "source": [
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "brqdCkmVMk5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5199384-bf1a-47d4-8717-affa90054f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-2.56908629 -2.20559589 -2.61072867 -2.41734343 -1.97310052]\n",
            "Média métricas de validación cruzada: -2.3551709604389495\n"
          ]
        }
      ],
      "source": [
        "cv_scores_lassoRMSE = cross_val_score(\n",
        "                estimator = lasso,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_lassoRMSE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_lassoRMSE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWEb9BGDKp53"
      },
      "source": [
        "## ElasticNet\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mYny4-cLMV_"
      },
      "source": [
        "MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "-2NeCduoNWdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc77bb5-32ce-49ef-b4a8-ad95d17ba3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-1.35388683 -1.23502737 -1.33767429 -1.29706447 -1.18485417]\n",
            "Média métricas de validación cruzada: -1.2817014287884163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netMAE = cross_val_score(\n",
        "                estimator = elasticnet,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_absolute_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netMAE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netMAE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiPNTcHYLN-3"
      },
      "source": [
        "R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "_hcKpKQ-NW-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac17b1c-f129-4632-eabf-757c5cc54d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [ 0.00380696 -0.00518962 -0.00204061 -0.00228847 -0.0441597 ]\n",
            "Média métricas de validación cruzada: -0.009974286362579554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netR = cross_val_score(\n",
        "                estimator = elasticnet,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'r2',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netR}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netR.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOePOV1eLPdX"
      },
      "source": [
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "8B0Snmf9NXdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79563cdc-db1f-47f0-f7b3-407a8cf82654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-2.56908629 -2.20559589 -2.61072867 -2.41734343 -1.97310052]\n",
            "Média métricas de validación cruzada: -2.3551709604389495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:148: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2. Please leave the normalize parameter to its default value to silence this warning. The default behavior of this estimator is to not do any normalization. If normalization is needed please use sklearn.preprocessing.StandardScaler instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netRMSE = cross_val_score(\n",
        "                estimator = elasticnet,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netRMSE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netRMSE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9hxJma87Flr"
      },
      "source": [
        "## XGBoost\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNX75HM7Flr"
      },
      "source": [
        "MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "XK6AW8JC7Flr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a9cdb7-94c8-45a0-fe5d-074cf5eea5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-1.37468563 -1.21935455 -1.26773854 -1.29919305 -1.119071  ]\n",
            "Média métricas de validación cruzada: -1.256008555438615\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netMAE = cross_val_score(\n",
        "                estimator = xgb_model,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_absolute_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netMAE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netMAE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAbfYdSF7Flr"
      },
      "source": [
        "R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "7Rh2k2Na7Flr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249f65c5-3acf-4544-a3cf-51232f77a55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-0.01096933 -0.02669672  0.1046385  -0.07463144  0.06573556]\n",
            "Média métricas de validación cruzada: 0.011615313821944584\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netR = cross_val_score(\n",
        "                estimator = xgb_model,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'r2',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netR}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netR.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcwsbHnQ7Fls"
      },
      "source": [
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "wI7FR0yp7Fls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee3a274-f5e3-4fdd-8042-e4caf59bd7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Métricas validación cruzada: [-2.60719295 -2.25278698 -2.33278563 -2.59182194 -1.76543649]\n",
            "Média métricas de validación cruzada: -2.3100047986455507\n"
          ]
        }
      ],
      "source": [
        "cv_scores_netRMSE = cross_val_score(\n",
        "                estimator = xgb_model,\n",
        "                X         = X,\n",
        "                y         = y,\n",
        "                scoring   = 'neg_mean_squared_error',\n",
        "                cv        = 5\n",
        "             )\n",
        "print(f\"Métricas validación cruzada: {cv_scores_netRMSE}\")\n",
        "print(f\"Média métricas de validación cruzada: {cv_scores_netRMSE.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFPEBuWcJ5pj"
      },
      "source": [
        "# Prueba\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "ERJ-q09uKMpM"
      },
      "outputs": [],
      "source": [
        "# train = train.drop([\"index\"], axis = 1)\n",
        "train = test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['NombreCaballo'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ],
      "metadata": {
        "id": "_kY3HQxsnz5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a22efa6-0d51-43a2-cb0d-679289868ec2"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PHILIPPO            1\n",
              "HIGHLAND MARKET     1\n",
              "LA MAL AMADA        1\n",
              "ATLANTICO           1\n",
              "CAROLINA WEST       1\n",
              "JERY SMAIH          1\n",
              "RUMBERA             1\n",
              "SANS ATTENDRE       1\n",
              "PIU BIRCH           1\n",
              "CHUSQUEZ            1\n",
              "ARETHA              1\n",
              "IZAMAL              1\n",
              "UPSDAWN             1\n",
              "ASTURIAS            1\n",
              "MAITRE YODA         1\n",
              "AUSTRALIA CAPE      1\n",
              "SOFUNNY             1\n",
              "SOGALINDA           1\n",
              "PONCE DE LEON       1\n",
              "HADES               1\n",
              "SEVERUS             1\n",
              "ROBAYERA            1\n",
              "VIKING CITY         1\n",
              "KANE ORE            1\n",
              "EMBAT               1\n",
              "MONTERREDONDO       1\n",
              "USI DE U            1\n",
              "TRES DE TREBOL      1\n",
              "MEDICEAN BLUE       1\n",
              "EL CANEY            1\n",
              "FINELY TUNED        1\n",
              "THE GAME            1\n",
              "ORBAYO              1\n",
              "ASTRAL              1\n",
              "LADY RAZALMA        1\n",
              "Name: NombreCaballo, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajuste Preprocesamiento\n",
        "- Borrar Puesto\n",
        "- Ajuste Listas Apariciones"
      ],
      "metadata": {
        "id": "pb5yf_8zjZ5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['NombreCaballo'] = train['NombreCaballo'].str.partition('(')[0]\n",
        "print(listAparicionesHorse)\n",
        "listTestHorse= eraseBlank(listaNombreCaballo)\n",
        "\n",
        "\n",
        "\n",
        "train['Propietario'] = train['Propietario'].str.strip()\n",
        "train['NombreCaballo'] = train['NombreCaballo'].str.strip()\n",
        "train['Jinete'] = train['Jinete'].str.strip()\n",
        "train['Preparador'] = train['Preparador'].str.strip()\n",
        "\n",
        "train.loc[~train[\"Propietario\"].isin(listAparicionesOwner), \"Propietario\"] = \"OtherOwner\"\n",
        "# train.loc[~train[\"NombreCaballo\"].isin(listAparicionesHorse), \"NombreCaballo\"] = \"OtherHorse\" #Lo hacemos mas tarde\n",
        "train.loc[~train[\"Jinete\"].isin(listAparicionesJockey), \"Jinete\"] = \"OtherJockey\"\n",
        "train.loc[~train[\"Preparador\"].isin(listAparicionesTrainer), \"Preparador\"] = \"OtherTrainer\"\n",
        "\n",
        "# if 'OtherOwner' not in train: // No necesario\n",
        "#   train['OtherOwner'] = 0\n",
        "\n",
        "# if 'OtherHorse' not in train: // Mas tarde /No necesario\n",
        "#   train['OtherHorse'] = 0\n",
        "\n",
        "if 'OtherJockey' not in train:\n",
        "  train['OtherJockey'] = 0\n",
        "\n",
        "if 'OtherTrainer' not in train:\n",
        "  train['OtherTrainer'] = 0\n"
      ],
      "metadata": {
        "id": "L1pulPHSx-yS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c8184c-3812-47cc-ad40-2a55f4683166"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['MONTERREDONDO', 'SOGALINDA', 'ATLANTICO', 'LA MAL AMADA', 'PIU BIRCH', 'ASTURIAS', 'UPSILON', 'SANCTI PETRI', 'ATLANTICO', 'UPSDAWN', 'LADY RAZALMA', 'FAITH ROSE', 'ROBLON', 'STARSHADOW', 'SHELBY', 'WINTON', 'XILADO', \"WARRIOR'S REVENGE\", 'WALKING TO GLORY', 'FORTUNATO', 'ASTRAL', 'ORBAYO', 'TRES DE TREBOL', 'FINELY TUNED', 'ASTURIAS', 'SEVERUS', 'MAITRE YODA', 'EMBAT', 'THE GAME', 'FINELY TUNED', 'EL CANEY', 'ASTRAL', 'KANE ORE', 'VIKING CITY', 'SOFUNNY', 'SOGALINDA', 'RUMBERA', 'PIU BIRCH', 'CAROLINA WEST', 'ROBAYERA', 'SANS ATTENDRE', 'HIGHLAND MARKET', 'IZAMAL', 'ORBAYO', 'ASTURIAS', 'ARETHA', 'LA MAL AMADA', 'AMERICANO', 'WINTON', 'STARSHADOW', 'UPSDAWN', 'SOGALINDA', 'ATLANTICO', 'PIU BIRCH', 'UPSILON', 'FORTUNATO', 'MAITRE YODA', 'SHELBY', 'STARSHADOW', 'ROBLON', 'UPSDAWN', 'THE GAME', 'FINELY TUNED', 'AUSTRALIA CAPE', 'SEVERUS', 'HIGHLAND MARKET', 'FORTUNATO', 'WALKING TO GLORY', 'LADY RAZALMA', 'CAROLINA WEST', 'SOFUNNY', 'VIKING CITY', 'ARETHA', 'PIU BIRCH', 'TRES DE TREBOL', 'EMBAT', 'ORBAYO', 'ASTURIAS', 'MEDICEAN BLUE', 'ASTRAL', 'ROBLON', 'WHITE WINE', 'VITA BARELIERE', 'KANE ORE', 'CHUSQUEZ', 'PIU BIRCH', 'SOFUNNY', 'SOGALINDA', 'ORBAYO', 'FORTUNATO', 'VIKING CITY', 'PIU BIRCH']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1hevyW7lRwj"
      },
      "source": [
        "# Preprocesamiento\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "p5kdoX88lRwk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "fc57bcda-295c-4273-83fd-83d2a6098084"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NombreCaballo         Peso  Edad  Mantilla       Propietario Preparador  \\\n",
              "0       PHILIPPO           57     2         4        OtherOwner  T.MARTINS   \n",
              "1  PONCE DE LEON           57     2         5  SALVADOR MARQUEZ  B.VALENTI   \n",
              "2       EL PATER           57     2         3           GISPERT    P.OLAVE   \n",
              "3     BRAGNOSERA  57,00-56,00     2         2         ZURRAQUIN     B.RAMA   \n",
              "4         BERTIZ           57     2         1       QUINTO REAL    P.OLAVE   \n",
              "\n",
              "         Jinete Problemas UltimasActuaciones                    Fecha   Hora  \\\n",
              "0      B. FAYOS        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "1    V. JANACEK        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "2  J.L. BORREGO        -4      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "3     L.FONSECA       NaN      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "4      C. CADEL       NaN      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "\n",
              "      Terreno  Distancia  Tipo Categoría  SentidoHipodromo  \\\n",
              "0  H - Blando       1600  Liso         D                 0   \n",
              "1  H - Blando       1600  Liso         D                 0   \n",
              "2  H - Blando       1600  Liso         D                 0   \n",
              "3  H - Blando       1600  Liso         D                 0   \n",
              "4  H - Blando       1600  Liso         D                 0   \n",
              "\n",
              "           Meteorología  LLuvia  Viento  Temperatura    Hipodromo  \\\n",
              "0  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "1  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "2  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "3  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "4  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "\n",
              "   OtherJockey  OtherTrainer  \n",
              "0            0             0  \n",
              "1            0             0  \n",
              "2            0             0  \n",
              "3            0             0  \n",
              "4            0             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-913f9ed3-da4a-4816-b9d3-8b58a3e505df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>OtherJockey</th>\n",
              "      <th>OtherTrainer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PHILIPPO</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>T.MARTINS</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PONCE DE LEON</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>SALVADOR MARQUEZ</td>\n",
              "      <td>B.VALENTI</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EL PATER</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>GISPERT</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BRAGNOSERA</td>\n",
              "      <td>57,00-56,00</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ZURRAQUIN</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BERTIZ</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>QUINTO REAL</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-913f9ed3-da4a-4816-b9d3-8b58a3e505df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-913f9ed3-da4a-4816-b9d3-8b58a3e505df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-913f9ed3-da4a-4816-b9d3-8b58a3e505df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh3WSnxElRwm"
      },
      "source": [
        "## **Tratamiento (Peso)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "ySrMtSGYlRwn"
      },
      "outputs": [],
      "source": [
        "train['Peso'] = train['Peso'].str.partition('-')[0]\n",
        "train['Peso'] = train['Peso'].str[:2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "JXAJNsvdlRwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d71415-b978-4e3b-d73a-c0fdd561fc29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56    23\n",
              "57     9\n",
              "54     9\n",
              "58     7\n",
              "59     5\n",
              "61     4\n",
              "60     4\n",
              "63     2\n",
              "64     1\n",
              "Name: Peso, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ],
      "source": [
        "train['Peso'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62zkqi9-lRwr"
      },
      "source": [
        "## **Tratamiento (Fecha)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "ueTnN4gelRwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc71f368-7678-48c1-9864-214e2bfbad50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2022-11-20\n",
              "1    2022-11-20\n",
              "2    2022-11-20\n",
              "3    2022-11-20\n",
              "4    2022-11-20\n",
              "5    2022-11-20\n",
              "6    2022-11-20\n",
              "7    2022-11-20\n",
              "8    2022-11-20\n",
              "9    2022-11-20\n",
              "10   2022-11-20\n",
              "11   2022-11-20\n",
              "12   2022-11-20\n",
              "13   2022-11-20\n",
              "14   2022-11-20\n",
              "15   2022-11-20\n",
              "16   2022-11-20\n",
              "17   2022-11-20\n",
              "18   2022-11-20\n",
              "19   2022-11-20\n",
              "20   2022-11-20\n",
              "21   2022-11-20\n",
              "22   2022-11-20\n",
              "23   2022-11-20\n",
              "24   2022-11-20\n",
              "25   2022-11-20\n",
              "26   2022-11-20\n",
              "27   2022-11-20\n",
              "28   2022-11-20\n",
              "29   2022-11-20\n",
              "30   2022-11-20\n",
              "31   2022-11-20\n",
              "32   2022-11-20\n",
              "33   2022-11-20\n",
              "34   2022-11-20\n",
              "35   2022-11-20\n",
              "36   2022-11-20\n",
              "37   2022-11-20\n",
              "38   2022-11-20\n",
              "39   2022-11-20\n",
              "40   2022-11-20\n",
              "41   2022-11-20\n",
              "42   2022-11-20\n",
              "43   2022-11-20\n",
              "44   2022-11-20\n",
              "45   2022-11-20\n",
              "46   2022-11-20\n",
              "47   2022-11-20\n",
              "48   2022-11-20\n",
              "49   2022-11-20\n",
              "50   2022-11-20\n",
              "51   2022-11-20\n",
              "52   2022-11-20\n",
              "53   2022-11-20\n",
              "54   2022-11-20\n",
              "55   2022-11-20\n",
              "56   2022-11-20\n",
              "57   2022-11-20\n",
              "58   2022-11-20\n",
              "59   2022-11-20\n",
              "60   2022-11-20\n",
              "61   2022-11-20\n",
              "62   2022-11-20\n",
              "63   2022-11-20\n",
              "Name: FechaAux, dtype: datetime64[ns]"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ],
      "source": [
        "import datetime as dt\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "def switchMonth(month):\n",
        "  if month == 'ENERO':  \n",
        "   return '01'\n",
        "  elif month == 'FEBRERO':\n",
        "    return '02'            \n",
        "  elif month == 'MARZO':\n",
        "   return '03'\n",
        "  elif month == 'ABRIL':\n",
        "    return '04' \n",
        "  elif month == 'MAYO':\n",
        "    return '05'                  \n",
        "  elif month == 'JUNIO':\n",
        "    return '06'\n",
        "  elif month == 'JULIO':\n",
        "    return '07'\n",
        "  elif month == 'AGOSTO':\n",
        "    return '08'\n",
        "  elif month == 'SEPTIEMBRE':\n",
        "    return '09'\n",
        "  elif month == 'OCTUBRE':\n",
        "     return '10'\n",
        "  elif month == 'NOVIEMBRE':\n",
        "    return '11'\n",
        "  elif month == 'DICIEMBRE':\n",
        "    return '12'\n",
        "\n",
        "def convertDate(fechas):\n",
        "  index = 0\n",
        "  for element in fechas:\n",
        "    sum = 0\n",
        "    year = element[-4:]\n",
        "    dateAux = year\n",
        "    day = element[:2]\n",
        "    if(int(day) < 10):\n",
        "      day = \"0\" + day[:1]\n",
        "    month = element[5:]\n",
        "    month = ''.join(month.split())[:-6].upper()\n",
        "    month = switchMonth(month)\n",
        "    dateAux = dateAux + \"-\" + month + \"-\" + day\n",
        "\n",
        "    today = date.today()\n",
        "    yearToday = int(str(today)[:4])\n",
        "    monthToday = int(str(today)[5:7])\n",
        "    dayToday = int(str(today)[8:10])\n",
        "    if(yearToday > int(year)):\n",
        "      sum += (yearToday - int(year))*365\n",
        "    sum += (monthToday - int(month))*30\n",
        "    sum += (dayToday - int(day))\n",
        "    datetimeAux = dt.datetime(int(year), int(month), int(day))\n",
        "    train.at[index, 'FechaAux'] = datetimeAux\n",
        "\n",
        "    train.at[index, 'year'] = int(year)\n",
        "    train.at[index, 'month'] = int(month)\n",
        "    train.at[index, 'day'] = int(day)\n",
        "    calculateSeason(int(month), index)\n",
        "\n",
        "    train.at[index, 'DiasDesdeCarrera'] = sum\n",
        "    index += 1\n",
        "   \n",
        "def calculateSeason(month, index):\n",
        "  # train['Invierno'] = 0\n",
        "  # train['Primavera'] = 0\n",
        "  # train['Verano'] = 0\n",
        "  train['Otoño'] = 0\n",
        "  # if(month >= 12 and month <=2):\n",
        "  #    train.at[index, 'Invierno'] = month\n",
        "  # if(month >= 3 and month <=5):\n",
        "  #    train.at[index, 'Primavera'] = month\n",
        "  # if(month >= 6 and month <=8):\n",
        "  #    train.at[index, 'Verano'] = month\n",
        "  if(month >= 9 and month <=11):\n",
        "     train.at[index, 'Otoño'] = month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "t_array = train[\"Fecha\"] \n",
        "convertDate(t_array)\n",
        "train['FechaAux'].head(70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "UdUYBQpplRws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "63df2da4-7977-4a42-e183-9c3120a937fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NombreCaballo Peso  Edad  Mantilla       Propietario Preparador  \\\n",
              "0       PHILIPPO   57     2         4        OtherOwner  T.MARTINS   \n",
              "1  PONCE DE LEON   57     2         5  SALVADOR MARQUEZ  B.VALENTI   \n",
              "2       EL PATER   57     2         3           GISPERT    P.OLAVE   \n",
              "3     BRAGNOSERA   57     2         2         ZURRAQUIN     B.RAMA   \n",
              "4         BERTIZ   57     2         1       QUINTO REAL    P.OLAVE   \n",
              "\n",
              "         Jinete Problemas UltimasActuaciones                    Fecha   Hora  \\\n",
              "0      B. FAYOS        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "1    V. JANACEK        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "2  J.L. BORREGO        -4      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "3     L.FONSECA       NaN      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "4      C. CADEL       NaN      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "\n",
              "      Terreno  Distancia  Tipo Categoría  SentidoHipodromo  \\\n",
              "0  H - Blando       1600  Liso         D                 0   \n",
              "1  H - Blando       1600  Liso         D                 0   \n",
              "2  H - Blando       1600  Liso         D                 0   \n",
              "3  H - Blando       1600  Liso         D                 0   \n",
              "4  H - Blando       1600  Liso         D                 0   \n",
              "\n",
              "           Meteorología  LLuvia  Viento  Temperatura    Hipodromo  \\\n",
              "0  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "1  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "2  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "3  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "4  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "\n",
              "   OtherJockey  OtherTrainer   FechaAux    year  month   day  Otoño  \\\n",
              "0            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "1            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "2            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "3            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "4            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "\n",
              "   DiasDesdeCarrera  \n",
              "0              30.0  \n",
              "1              30.0  \n",
              "2              30.0  \n",
              "3              30.0  \n",
              "4              30.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d850c69-a546-4d08-b8fa-5634d73a6abb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>OtherJockey</th>\n",
              "      <th>OtherTrainer</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PHILIPPO</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>T.MARTINS</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PONCE DE LEON</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>SALVADOR MARQUEZ</td>\n",
              "      <td>B.VALENTI</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EL PATER</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>GISPERT</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BRAGNOSERA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ZURRAQUIN</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BERTIZ</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>QUINTO REAL</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d850c69-a546-4d08-b8fa-5634d73a6abb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d850c69-a546-4d08-b8fa-5634d73a6abb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d850c69-a546-4d08-b8fa-5634d73a6abb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ],
      "source": [
        "train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "69cMMbqulRws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418ba83f-17f5-42ce-b8de-c91ad95ebe95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     45.0\n",
              "1     45.0\n",
              "2     45.0\n",
              "3     45.0\n",
              "4     45.0\n",
              "5     45.0\n",
              "6     45.0\n",
              "7     45.0\n",
              "8     45.0\n",
              "9     45.0\n",
              "10    45.0\n",
              "11    45.0\n",
              "12    45.0\n",
              "13    45.0\n",
              "14    45.0\n",
              "15    45.0\n",
              "16    45.0\n",
              "17    45.0\n",
              "18    45.0\n",
              "19    45.0\n",
              "20    45.0\n",
              "21    45.0\n",
              "22    45.0\n",
              "23    45.0\n",
              "24    45.0\n",
              "25    45.0\n",
              "26    45.0\n",
              "27    45.0\n",
              "28    45.0\n",
              "29    45.0\n",
              "30    45.0\n",
              "31    45.0\n",
              "32    45.0\n",
              "33    45.0\n",
              "34    45.0\n",
              "Name: DaysSincePreviousRace, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "def daysSincePreviousRace(nombres):\n",
        "  index = 0\n",
        "  for caballo in nombres:\n",
        "    indexNombre = 0\n",
        "    nombresAparece = []\n",
        "    for otroCaballo in nombres:\n",
        "      if(otroCaballo == caballo):\n",
        "        nombresAparece.append(indexNombre)\n",
        "      indexNombre += 1\n",
        "\n",
        "    day_actual = train.at[index, 'day']\n",
        "    month_actual = train.at[index, 'month']\n",
        "    year_actual = train.at[index, 'year']\n",
        "\n",
        "    fechaCaballoActual = dt.datetime(int(year_actual), int(month_actual), int(day_actual))\n",
        "    fechaMasReciente = dt.datetime(2020, 1, 1)\n",
        "    indexAux2 = 0\n",
        "    fechaCambiada = False\n",
        "    for aparicion in nombresAparece:\n",
        "      if(train.at[aparicion, 'FechaAux'] < fechaCaballoActual): \n",
        "        if(indexAux2 == 0):\n",
        "          fechaMasReciente = train.at[aparicion, 'FechaAux']\n",
        "          fechaCambiada = True\n",
        "          indexAux2 += 1\n",
        "        elif(train.at[aparicion, 'FechaAux'] > fechaMasReciente):\n",
        "          fechaMasReciente =  train.at[aparicion, 'FechaAux']\n",
        "    if(fechaCambiada):\n",
        "      diferenciaDias = (fechaCaballoActual - fechaMasReciente).days\n",
        "    else:\n",
        "      diferenciaDias = 45 # Penalización si no aparece en carrera anterior\n",
        "    train.at[index, 'DaysSincePreviousRace'] = diferenciaDias\n",
        "    index += 1\n",
        "\n",
        "\n",
        "eventos = train['NombreCaballo']\n",
        "daysSincePreviousRace(eventos)\n",
        "train['DaysSincePreviousRace'].head(35)\n",
        "# Recorro los nombres de caballos y guardo en un array los index de las columnas ligadas a un nombre de Caballo y despues recorrer las fechas y cuando coincida el array comprobar si ese dato\n",
        "# es mas antiguo que la fecha de la fila actual y en el caso de serlo compruebo si es la más antigua de entre las anteriores participaciones\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculo Contricantes\n",
        "---"
      ],
      "metadata": {
        "id": "xVd7IHIr49HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculoContrincantes(listaF, listaH, longitud):\n",
        "  for fecha in listaF:\n",
        "    for horario in listaH:\n",
        "      index = 0\n",
        "      listaIndex = []\n",
        "      sum = 0\n",
        "      while(index < longitud):\n",
        "        if((train.at[index, 'Fecha'] == fecha ) and (train.at[index, 'Hora'] == horario )):\n",
        "          listaIndex.append(index)\n",
        "          sum += 1\n",
        "        index += 1\n",
        "      for indice in listaIndex:\n",
        "        train.at[indice, 'Contrincantes'] = sum\n",
        "\n",
        "\n",
        "listaFechas = train['Fecha'].unique().tolist()\n",
        "listaHorarios = train['Hora'].unique().tolist()\n",
        "longitud = len(train.index)\n",
        "\n",
        "calculoContrincantes(listaFechas, listaHorarios, longitud)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2_4wnsGB49HG"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Busqueda Caballos Ganadores en ciertas condiciones**\n",
        "---"
      ],
      "metadata": {
        "id": "DSF7H_HP4-4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Busqueda Distancia\n",
        "---"
      ],
      "metadata": {
        "id": "dtcWgeS94-4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "train.head(70)"
      ],
      "metadata": {
        "id": "PUEEKuX_6G9-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d032ba48-38b5-4472-eab8-f096d266b737"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        NombreCaballo Peso  Edad  Mantilla          Propietario  \\\n",
              "0            PHILIPPO   57     2         4           OtherOwner   \n",
              "1       PONCE DE LEON   57     2         5     SALVADOR MARQUEZ   \n",
              "2            EL PATER   57     2         3              GISPERT   \n",
              "3          BRAGNOSERA   57     2         2            ZURRAQUIN   \n",
              "4              BERTIZ   57     2         1          QUINTO REAL   \n",
              "5              SANDRO   57     2         6           OtherOwner   \n",
              "6           GRAZALEMA   56     2         2         MEDITERRANEO   \n",
              "7            NICOLASA   56     2         6           OtherOwner   \n",
              "8      LA PERLA NEGRA   56     2         4           OtherOwner   \n",
              "9               WHITY   56     2         9          YEGUADA AGF   \n",
              "10           AFRODITA   56     2         1                CARAL   \n",
              "11            VADALEW   56     2         7  ELEVAGE LA REVER...   \n",
              "12               HEBE   56     2         3  DUQUE DE ALBURQU...   \n",
              "13              WENDY   56     2         8          YEGUADA AGF   \n",
              "14          MADRE MIA   56     2         5         MEDITERRANEO   \n",
              "15       LADY RAZALMA   54     2        12            CENTURION   \n",
              "16         FAITH ROSE   56     2         2               TEN FE   \n",
              "17  WARRIOR'S REVENGE   56     2         8        YEGUADA ROCIO   \n",
              "18             SHELBY   56     2         7               MARTUL   \n",
              "19         WHITE WINE   56     2         9          YEGUADA AGF   \n",
              "20     VITA BARELIERE   54     2        15                JACAL   \n",
              "21             XILADO   56     2        11           PAOLO NERI   \n",
              "22             WINTON   56     2        10               TIGRES   \n",
              "23          AMERICANO   56     2         1  ECURIE DES MOUET...   \n",
              "24    SUPERIOR BEAUTY   54     2        14  JAVIER MALDONADO...   \n",
              "25         STARSHADOW   54     2        13         CELSO MENDEZ   \n",
              "26        FUOCO GRECO   56     2         4           OtherOwner   \n",
              "27       SANCTI PETRI   56     2         6               BERNIE   \n",
              "28             ROBLON   56     2         5               MARTUL   \n",
              "29   WALKING TO GLORY   54     2        16         AMAZING TURF   \n",
              "30          FORTUNATO   56     2         3                  4 C   \n",
              "31     AUSTRALIA CAPE   63     3         2         MEDITERRANEO   \n",
              "32        MAITRE YODA   61     4         3     CUADRA A.F.F S.L   \n",
              "33           ASTURIAS   56     4         9                  ZUL   \n",
              "34            SEVERUS   59     3         6         CELSO MENDEZ   \n",
              "35           ROBAYERA   54     3        10            EL GENTIO   \n",
              "36        VIKING CITY   54     3        11     CUADRA A.F.F S.L   \n",
              "37           KANE ORE   59     3         5  ANNUA RACING, S....   \n",
              "38              EMBAT   57     9         7          LA TOLEDANA   \n",
              "39      MONTERREDONDO   57     4         8             CANARIAS   \n",
              "40              HADES   64     4         1           OtherOwner   \n",
              "41           USI DE U   54     3        12           VALLADOLID   \n",
              "42      MEDICEAN BLUE   60     7         4              TIZIANO   \n",
              "43           EL CANEY   58     4         2        SANTA BARBARA   \n",
              "44       FINELY TUNED   58     5         3  ENRIQUE FERNANDE...   \n",
              "45           THE GAME   58     4         5      CIELO DE MADRID   \n",
              "46             ORBAYO   58     6         4          LAS AGUILAS   \n",
              "47             ASTRAL   58     4         1               ATERPE   \n",
              "48     TRES DE TREBOL   57     4         6  FORRAJES Y CEREA...   \n",
              "49          SOGALINDA   60     4         5              ZEZINHO   \n",
              "50           CHUSQUEZ   56     3        13           OtherOwner   \n",
              "51            SOFUNNY   59     3         9             TRILOGIA   \n",
              "52       LA MAL AMADA   60     5         7               EUROPA   \n",
              "53          ATLANTICO   59     7        10           REAPERTURA   \n",
              "54      CAROLINA WEST   59     4         8              TIZIANO   \n",
              "55         JERY SMAIH   58     4        11           OtherOwner   \n",
              "56            RUMBERA   58     5        12            COCHETEUX   \n",
              "57      SANS ATTENDRE   61     3         2              IGUELDO   \n",
              "58    HIGHLAND MARKET   63     3         1  ENRIQUE FERNANDE...   \n",
              "59          PIU BIRCH   54     3        15                DELTA   \n",
              "60             ARETHA   61     5         3              ARTEMIS   \n",
              "61             IZAMAL   56     3        14     YEGUADA ARANJUEZ   \n",
              "62            UPSDAWN   60     4         6              DONALFA   \n",
              "63            UPSILON   61     4         4  ANNUA RACING, S....   \n",
              "\n",
              "       Preparador               Jinete  Problemas UltimasActuaciones  \\\n",
              "0       T.MARTINS             B. FAYOS         -8      [ Debutante ]   \n",
              "1       B.VALENTI           V. JANACEK         -8      [ Debutante ]   \n",
              "2         P.OLAVE         J.L. BORREGO         -4      [ Debutante ]   \n",
              "3          B.RAMA            L.FONSECA        NaN      [ Debutante ]   \n",
              "4         P.OLAVE             C. CADEL        NaN      [ Debutante ]   \n",
              "5          A.SOTO        J.L. MARTINEZ        NaN      [ Debutante ]   \n",
              "6   G.ARIZKORRETA        J.L. MARTINEZ        NaN      [ Debutante ]   \n",
              "7      J.M.OSORIO             B. FAYOS         -4      [ Debutante ]   \n",
              "8          B.RAMA            L.FONSECA        NaN      [ Debutante ]   \n",
              "9   G.ARIZKORRETA           J.GELABERT        NaN      [ Debutante ]   \n",
              "10     J.L.MAROTO         N. DE JULIAN        NaN      [ Debutante ]   \n",
              "11      T.MARTINS             C. CADEL        NaN      [ Debutante ]   \n",
              "12     J.M.OSORIO          V.ALONSO V.        NaN      [ Debutante ]   \n",
              "13  G.ARIZKORRETA          D. FERREIRA        NaN      [ Debutante ]   \n",
              "14         A.SOTO           V. JANACEK         -4      [ Debutante ]   \n",
              "15     M&M RACING        J.L. MARTINEZ        NaN      [01,08,05,05]   \n",
              "16        P.OLAVE             B. FAYOS         -3            [02,07]   \n",
              "17  G.ARIZKORRETA           V. JANACEK        NaN   [10,03,05,03,03]   \n",
              "18  J.A.RODRIGUEZ         N. DE JULIAN        NaN      [06,05,05,06]   \n",
              "19  G.ARIZKORRETA           J.GELABERT        NaN            [05,10]   \n",
              "20    A.TSERETELI              N.SACCU         -4               [11]   \n",
              "21     M&M RACING          B.ESTUPIÑAN         -3            [08,06]   \n",
              "22        A.NUÑEZ          D. FERREIRA     (4)(8)            [07,12]   \n",
              "23      A.IMAZ,B.             C. CADEL  (4)(5)(8)   [05,04,07,02,04]   \n",
              "24    M.ALONSO R.  G.TROLLEY DE PRE...         -8          [ 01,03 ]   \n",
              "25      M.ALVAREZ            R.N.VALLE        NaN      [05,07,07,05]   \n",
              "26     M&M RACING          Y.RODRIGUEZ        NaN               [11]   \n",
              "27  J.C.CERQUEIRA          V.ALONSO V.        NaN               [03]   \n",
              "28    R.MARTIN V.         J.L. BORREGO         -8      [03,09,02,NP]   \n",
              "29  J.A.RODRIGUEZ              C.PEREZ        NaN   [13,07,05,05,04]   \n",
              "30  J.A.RODRIGUEZ            N. GARCIA     (3)(8)   [14,03,08,01,03]   \n",
              "31  G.ARIZKORRETA           D.SIKOROVÁ        NaN   [03,01,04,01,02]   \n",
              "32    F.RODRIGUEZ             B. FAYOS         -8   [06,01,03,03,03]   \n",
              "33    C.FERNANDEZ        STA. BUESA,C.         -3   [03,02,02,04,03]   \n",
              "34      M.ALVAREZ            R.N.VALLE        NaN   [04,06,03,05,02]   \n",
              "35         A.SOTO           J.GELABERT         -8   [02,04,02,03,11]   \n",
              "36    F.RODRIGUEZ              C.PEREZ     (3)(8)   [01,03,02,05,04]   \n",
              "37     J.C.ROSELL         J.L. BORREGO        NaN   [02,08,07,05,05]   \n",
              "38     J.L.MAROTO        J.L. MARTINEZ        NaN   [07,02,04,10,03]   \n",
              "39  E.ARGUINZONES              C.HAZEN         -8   [09,07,04,05,06]   \n",
              "40        A.NUÑEZ       SRTA. TENA, C.         -8   [06,01,06,01,01]   \n",
              "41      M.J.PEREZ          V.ALONSO V.        NaN   [09,04,01,05,07]   \n",
              "42     J.L.MAROTO           V. JANACEK        NaN   [06,02,02,04,02]   \n",
              "43     J.M.OSORIO             B. FAYOS         -8   [16,04,02,01,05]   \n",
              "44        J.LOPEZ  G.TROLLEY DE PRE...     (3)(8)   [07,10,04,04,12]   \n",
              "45        P.OLAVE         J.L. BORREGO         -5   [01,03,09,01,05]   \n",
              "46     A.CARRASCO           J.GELABERT         -8   [02,01,03,07,02]   \n",
              "47  G.ARIZKORRETA           V. JANACEK        NaN   [01,06,07,05,03]   \n",
              "48         D.DIEZ        J.L. MARTINEZ         -8   [03,01,06,03,04]   \n",
              "49  J.C.CERQUEIRA          V.ALONSO V.         -3   [01,03,01,05,02]   \n",
              "50  E.ARGUINZONES           J.GELABERT     (4)(8)   [09,06,09,16,06]   \n",
              "51        P.OLAVE             B. FAYOS        NaN   [02,02,03,02,05]   \n",
              "52  J.C.CERQUEIRA          B.ESTUPIÑAN     (3)(8)   [08,10,03,09,05]   \n",
              "53    A.TSERETELI              C.HAZEN        NaN   [05,03,08,07,05]   \n",
              "54     J.L.MAROTO           V. JANACEK        NaN   [10,01,02,02,11]   \n",
              "55  E.ARGUINZONES           F.MARTINEZ     (3)(8)   [02,04,04,04,06]   \n",
              "56      M.ALVAREZ         N. DE JULIAN        NaN   [07,05,06,08,03]   \n",
              "57       B.MORENO            R.N.VALLE         -3   [04,07,09,05,06]   \n",
              "58        J.LOPEZ            L.FONSECA         -5   [05,07,07,03,03]   \n",
              "59      M.A.MARIN           A.MARTINEZ         -8   [15,09,11,10,04]   \n",
              "60        J.LOPEZ              C.PEREZ        NaN   [NP,04,08,04,01]   \n",
              "61        J.LOPEZ              N.SACCU         -4      [09,07,06,09]   \n",
              "62      M.J.PEREZ          D. FERREIRA         -8   [14,06,16,03,01]   \n",
              "63     J.C.ROSELL         J.L. BORREGO         -5   [04,17,05,03,08]   \n",
              "\n",
              "                      Fecha   Hora     Terreno  Distancia  Tipo Categoría  \\\n",
              "0   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "1   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "2   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "3   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "4   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "5   20 de noviembre de 2022  11:45  H - Blando       1600  Liso         D   \n",
              "6   20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "7   20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "8   20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "9   20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "10  20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "11  20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "12  20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "13  20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "14  20 de noviembre de 2022  12:20  H - Blando       1600  Liso         D   \n",
              "15  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "16  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "17  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "18  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "19  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "20  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "21  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "22  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "23  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "24  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "25  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "26  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "27  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "28  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "29  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "30  20 de noviembre de 2022  12:55  H - Blando       2000  Liso         B   \n",
              "31  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "32  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "33  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "34  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "35  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "36  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "37  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "38  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "39  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "40  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "41  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "42  20 de noviembre de 2022  13:30  H - Blando       2100  Liso         D   \n",
              "43  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "44  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "45  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "46  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "47  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "48  20 de noviembre de 2022  14:05  H - Blando       4000  Liso         B   \n",
              "49  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "50  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "51  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "52  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "53  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "54  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "55  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "56  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "57  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "58  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "59  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "60  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "61  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "62  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "63  20 de noviembre de 2022  14:40  H - Blando       2100  Liso         D   \n",
              "\n",
              "    SentidoHipodromo          Meteorología  LLuvia  Viento  Temperatura  \\\n",
              "0                  0  Parcialmente nublado       0      15           10   \n",
              "1                  0  Parcialmente nublado       0      15           10   \n",
              "2                  0  Parcialmente nublado       0      15           10   \n",
              "3                  0  Parcialmente nublado       0      15           10   \n",
              "4                  0  Parcialmente nublado       0      15           10   \n",
              "5                  0  Parcialmente nublado       0      15           10   \n",
              "6                  0  Parcialmente nublado       0      15           10   \n",
              "7                  0  Parcialmente nublado       0      15           10   \n",
              "8                  0  Parcialmente nublado       0      15           10   \n",
              "9                  0  Parcialmente nublado       0      15           10   \n",
              "10                 0  Parcialmente nublado       0      15           10   \n",
              "11                 0  Parcialmente nublado       0      15           10   \n",
              "12                 0  Parcialmente nublado       0      15           10   \n",
              "13                 0  Parcialmente nublado       0      15           10   \n",
              "14                 0  Parcialmente nublado       0      15           10   \n",
              "15                 0  Parcialmente nublado       0      15           10   \n",
              "16                 0  Parcialmente nublado       0      15           10   \n",
              "17                 0  Parcialmente nublado       0      15           10   \n",
              "18                 0  Parcialmente nublado       0      15           10   \n",
              "19                 0  Parcialmente nublado       0      15           10   \n",
              "20                 0  Parcialmente nublado       0      15           10   \n",
              "21                 0  Parcialmente nublado       0      15           10   \n",
              "22                 0  Parcialmente nublado       0      15           10   \n",
              "23                 0  Parcialmente nublado       0      15           10   \n",
              "24                 0  Parcialmente nublado       0      15           10   \n",
              "25                 0  Parcialmente nublado       0      15           10   \n",
              "26                 0  Parcialmente nublado       0      15           10   \n",
              "27                 0  Parcialmente nublado       0      15           10   \n",
              "28                 0  Parcialmente nublado       0      15           10   \n",
              "29                 0  Parcialmente nublado       0      15           10   \n",
              "30                 0  Parcialmente nublado       0      15           10   \n",
              "31                 0  Parcialmente nublado       0      15           10   \n",
              "32                 0  Parcialmente nublado       0      15           10   \n",
              "33                 0  Parcialmente nublado       0      15           10   \n",
              "34                 0  Parcialmente nublado       0      15           10   \n",
              "35                 0  Parcialmente nublado       0      15           10   \n",
              "36                 0  Parcialmente nublado       0      15           10   \n",
              "37                 0  Parcialmente nublado       0      15           10   \n",
              "38                 0  Parcialmente nublado       0      15           10   \n",
              "39                 0  Parcialmente nublado       0      15           10   \n",
              "40                 0  Parcialmente nublado       0      15           10   \n",
              "41                 0  Parcialmente nublado       0      15           10   \n",
              "42                 0  Parcialmente nublado       0      15           10   \n",
              "43                 0  Parcialmente nublado       0      15           10   \n",
              "44                 0  Parcialmente nublado       0      15           10   \n",
              "45                 0  Parcialmente nublado       0      15           10   \n",
              "46                 0  Parcialmente nublado       0      15           10   \n",
              "47                 0  Parcialmente nublado       0      15           10   \n",
              "48                 0  Parcialmente nublado       0      15           10   \n",
              "49                 0  Parcialmente nublado       0      15           10   \n",
              "50                 0  Parcialmente nublado       0      15           10   \n",
              "51                 0  Parcialmente nublado       0      15           10   \n",
              "52                 0  Parcialmente nublado       0      15           10   \n",
              "53                 0  Parcialmente nublado       0      15           10   \n",
              "54                 0  Parcialmente nublado       0      15           10   \n",
              "55                 0  Parcialmente nublado       0      15           10   \n",
              "56                 0  Parcialmente nublado       0      15           10   \n",
              "57                 0  Parcialmente nublado       0      15           10   \n",
              "58                 0  Parcialmente nublado       0      15           10   \n",
              "59                 0  Parcialmente nublado       0      15           10   \n",
              "60                 0  Parcialmente nublado       0      15           10   \n",
              "61                 0  Parcialmente nublado       0      15           10   \n",
              "62                 0  Parcialmente nublado       0      15           10   \n",
              "63                 0  Parcialmente nublado       0      15           10   \n",
              "\n",
              "      Hipodromo  OtherJockey  OtherTrainer   FechaAux    year  month   day  \\\n",
              "0   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "1   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "2   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "3   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "4   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "5   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "6   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "7   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "8   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "9   La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "10  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "11  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "12  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "13  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "14  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "15  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "16  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "17  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "18  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "19  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "20  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "21  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "22  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "23  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "24  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "25  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "26  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "27  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "28  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "29  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "30  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "31  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "32  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "33  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "34  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "35  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "36  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "37  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "38  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "39  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "40  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "41  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "42  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "43  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "44  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "45  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "46  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "47  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "48  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "49  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "50  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "51  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "52  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "53  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "54  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "55  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "56  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "57  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "58  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "59  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "60  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "61  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "62  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "63  La Zarzuela            0             0 2022-11-20  2022.0   11.0  20.0   \n",
              "\n",
              "    Otoño  DiasDesdeCarrera  DaysSincePreviousRace  Contrincantes  \n",
              "0       0              30.0                   45.0            6.0  \n",
              "1       0              30.0                   45.0            6.0  \n",
              "2       0              30.0                   45.0            6.0  \n",
              "3       0              30.0                   45.0            6.0  \n",
              "4       0              30.0                   45.0            6.0  \n",
              "5       0              30.0                   45.0            6.0  \n",
              "6       0              30.0                   45.0            9.0  \n",
              "7       0              30.0                   45.0            9.0  \n",
              "8       0              30.0                   45.0            9.0  \n",
              "9       0              30.0                   45.0            9.0  \n",
              "10      0              30.0                   45.0            9.0  \n",
              "11      0              30.0                   45.0            9.0  \n",
              "12      0              30.0                   45.0            9.0  \n",
              "13      0              30.0                   45.0            9.0  \n",
              "14      0              30.0                   45.0            9.0  \n",
              "15      0              30.0                   45.0           16.0  \n",
              "16      0              30.0                   45.0           16.0  \n",
              "17      0              30.0                   45.0           16.0  \n",
              "18      0              30.0                   45.0           16.0  \n",
              "19      0              30.0                   45.0           16.0  \n",
              "20      0              30.0                   45.0           16.0  \n",
              "21      0              30.0                   45.0           16.0  \n",
              "22      0              30.0                   45.0           16.0  \n",
              "23      0              30.0                   45.0           16.0  \n",
              "24      0              30.0                   45.0           16.0  \n",
              "25      0              30.0                   45.0           16.0  \n",
              "26      0              30.0                   45.0           16.0  \n",
              "27      0              30.0                   45.0           16.0  \n",
              "28      0              30.0                   45.0           16.0  \n",
              "29      0              30.0                   45.0           16.0  \n",
              "30      0              30.0                   45.0           16.0  \n",
              "31      0              30.0                   45.0           12.0  \n",
              "32      0              30.0                   45.0           12.0  \n",
              "33      0              30.0                   45.0           12.0  \n",
              "34      0              30.0                   45.0           12.0  \n",
              "35      0              30.0                   45.0           12.0  \n",
              "36      0              30.0                   45.0           12.0  \n",
              "37      0              30.0                   45.0           12.0  \n",
              "38      0              30.0                   45.0           12.0  \n",
              "39      0              30.0                   45.0           12.0  \n",
              "40      0              30.0                   45.0           12.0  \n",
              "41      0              30.0                   45.0           12.0  \n",
              "42      0              30.0                   45.0           12.0  \n",
              "43      0              30.0                   45.0            6.0  \n",
              "44      0              30.0                   45.0            6.0  \n",
              "45      0              30.0                   45.0            6.0  \n",
              "46      0              30.0                   45.0            6.0  \n",
              "47      0              30.0                   45.0            6.0  \n",
              "48      0              30.0                   45.0            6.0  \n",
              "49      0              30.0                   45.0           15.0  \n",
              "50      0              30.0                   45.0           15.0  \n",
              "51      0              30.0                   45.0           15.0  \n",
              "52      0              30.0                   45.0           15.0  \n",
              "53      0              30.0                   45.0           15.0  \n",
              "54      0              30.0                   45.0           15.0  \n",
              "55      0              30.0                   45.0           15.0  \n",
              "56      0              30.0                   45.0           15.0  \n",
              "57      0              30.0                   45.0           15.0  \n",
              "58      0              30.0                   45.0           15.0  \n",
              "59      0              30.0                   45.0           15.0  \n",
              "60      0              30.0                   45.0           15.0  \n",
              "61      0              30.0                   45.0           15.0  \n",
              "62      0              30.0                   45.0           15.0  \n",
              "63     11              30.0                   45.0           15.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58c267ce-6530-44ac-be84-92ab09125c64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>OtherJockey</th>\n",
              "      <th>OtherTrainer</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "      <th>DaysSincePreviousRace</th>\n",
              "      <th>Contrincantes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PHILIPPO</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>T.MARTINS</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PONCE DE LEON</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>SALVADOR MARQUEZ</td>\n",
              "      <td>B.VALENTI</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EL PATER</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>GISPERT</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BRAGNOSERA</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ZURRAQUIN</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BERTIZ</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>QUINTO REAL</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SANDRO</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>A.SOTO</td>\n",
              "      <td>J.L. MARTINEZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GRAZALEMA</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>MEDITERRANEO</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>J.L. MARTINEZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NICOLASA</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LA PERLA NEGRA</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>WHITY</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>YEGUADA AGF</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>AFRODITA</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>CARAL</td>\n",
              "      <td>J.L.MAROTO</td>\n",
              "      <td>N. DE JULIAN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>VADALEW</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>ELEVAGE LA REVER...</td>\n",
              "      <td>T.MARTINS</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>HEBE</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>DUQUE DE ALBURQU...</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>V.ALONSO V.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WENDY</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>YEGUADA AGF</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>D. FERREIRA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>MADRE MIA</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>MEDITERRANEO</td>\n",
              "      <td>A.SOTO</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:20</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LADY RAZALMA</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>CENTURION</td>\n",
              "      <td>M&amp;M RACING</td>\n",
              "      <td>J.L. MARTINEZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[01,08,05,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>FAITH ROSE</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>TEN FE</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-3</td>\n",
              "      <td>[02,07]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>WARRIOR'S REVENGE</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>YEGUADA ROCIO</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[10,03,05,03,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SHELBY</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>MARTUL</td>\n",
              "      <td>J.A.RODRIGUEZ</td>\n",
              "      <td>N. DE JULIAN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[06,05,05,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>WHITE WINE</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>YEGUADA AGF</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[05,10]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>VITA BARELIERE</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>JACAL</td>\n",
              "      <td>A.TSERETELI</td>\n",
              "      <td>N.SACCU</td>\n",
              "      <td>-4</td>\n",
              "      <td>[11]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>XILADO</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>PAOLO NERI</td>\n",
              "      <td>M&amp;M RACING</td>\n",
              "      <td>B.ESTUPIÑAN</td>\n",
              "      <td>-3</td>\n",
              "      <td>[08,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>WINTON</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>TIGRES</td>\n",
              "      <td>A.NUÑEZ</td>\n",
              "      <td>D. FERREIRA</td>\n",
              "      <td>(4)(8)</td>\n",
              "      <td>[07,12]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AMERICANO</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>ECURIE DES MOUET...</td>\n",
              "      <td>A.IMAZ,B.</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>(4)(5)(8)</td>\n",
              "      <td>[05,04,07,02,04]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SUPERIOR BEAUTY</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>JAVIER MALDONADO...</td>\n",
              "      <td>M.ALONSO R.</td>\n",
              "      <td>G.TROLLEY DE PRE...</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ 01,03 ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>CELSO MENDEZ</td>\n",
              "      <td>M.ALVAREZ</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[05,07,07,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>FUOCO GRECO</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>M&amp;M RACING</td>\n",
              "      <td>Y.RODRIGUEZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[11]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SANCTI PETRI</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>BERNIE</td>\n",
              "      <td>J.C.CERQUEIRA</td>\n",
              "      <td>V.ALONSO V.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ROBLON</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>MARTUL</td>\n",
              "      <td>R.MARTIN V.</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-8</td>\n",
              "      <td>[03,09,02,NP]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>WALKING TO GLORY</td>\n",
              "      <td>54</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>AMAZING TURF</td>\n",
              "      <td>J.A.RODRIGUEZ</td>\n",
              "      <td>C.PEREZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[13,07,05,05,04]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>FORTUNATO</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4 C</td>\n",
              "      <td>J.A.RODRIGUEZ</td>\n",
              "      <td>N. GARCIA</td>\n",
              "      <td>(3)(8)</td>\n",
              "      <td>[14,03,08,01,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>12:55</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>AUSTRALIA CAPE</td>\n",
              "      <td>63</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>MEDITERRANEO</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>D.SIKOROVÁ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[03,01,04,01,02]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>MAITRE YODA</td>\n",
              "      <td>61</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>CUADRA A.F.F S.L</td>\n",
              "      <td>F.RODRIGUEZ</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[06,01,03,03,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>56</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>ZUL</td>\n",
              "      <td>C.FERNANDEZ</td>\n",
              "      <td>STA. BUESA,C.</td>\n",
              "      <td>-3</td>\n",
              "      <td>[03,02,02,04,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>SEVERUS</td>\n",
              "      <td>59</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>CELSO MENDEZ</td>\n",
              "      <td>M.ALVAREZ</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[04,06,03,05,02]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>ROBAYERA</td>\n",
              "      <td>54</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>EL GENTIO</td>\n",
              "      <td>A.SOTO</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>-8</td>\n",
              "      <td>[02,04,02,03,11]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>VIKING CITY</td>\n",
              "      <td>54</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>CUADRA A.F.F S.L</td>\n",
              "      <td>F.RODRIGUEZ</td>\n",
              "      <td>C.PEREZ</td>\n",
              "      <td>(3)(8)</td>\n",
              "      <td>[01,03,02,05,04]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>KANE ORE</td>\n",
              "      <td>59</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>ANNUA RACING, S....</td>\n",
              "      <td>J.C.ROSELL</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[02,08,07,05,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>EMBAT</td>\n",
              "      <td>57</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>LA TOLEDANA</td>\n",
              "      <td>J.L.MAROTO</td>\n",
              "      <td>J.L. MARTINEZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[07,02,04,10,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>MONTERREDONDO</td>\n",
              "      <td>57</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>CANARIAS</td>\n",
              "      <td>E.ARGUINZONES</td>\n",
              "      <td>C.HAZEN</td>\n",
              "      <td>-8</td>\n",
              "      <td>[09,07,04,05,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>HADES</td>\n",
              "      <td>64</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>A.NUÑEZ</td>\n",
              "      <td>SRTA. TENA, C.</td>\n",
              "      <td>-8</td>\n",
              "      <td>[06,01,06,01,01]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>USI DE U</td>\n",
              "      <td>54</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>VALLADOLID</td>\n",
              "      <td>M.J.PEREZ</td>\n",
              "      <td>V.ALONSO V.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[09,04,01,05,07]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>MEDICEAN BLUE</td>\n",
              "      <td>60</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>TIZIANO</td>\n",
              "      <td>J.L.MAROTO</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[06,02,02,04,02]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>13:30</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>EL CANEY</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>SANTA BARBARA</td>\n",
              "      <td>J.M.OSORIO</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[16,04,02,01,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>FINELY TUNED</td>\n",
              "      <td>58</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>ENRIQUE FERNANDE...</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>G.TROLLEY DE PRE...</td>\n",
              "      <td>(3)(8)</td>\n",
              "      <td>[07,10,04,04,12]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>THE GAME</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>CIELO DE MADRID</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-5</td>\n",
              "      <td>[01,03,09,01,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>ORBAYO</td>\n",
              "      <td>58</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>LAS AGUILAS</td>\n",
              "      <td>A.CARRASCO</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>-8</td>\n",
              "      <td>[02,01,03,07,02]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>ASTRAL</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>ATERPE</td>\n",
              "      <td>G.ARIZKORRETA</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[01,06,07,05,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>TRES DE TREBOL</td>\n",
              "      <td>57</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>FORRAJES Y CEREA...</td>\n",
              "      <td>D.DIEZ</td>\n",
              "      <td>J.L. MARTINEZ</td>\n",
              "      <td>-8</td>\n",
              "      <td>[03,01,06,03,04]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:05</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>4000</td>\n",
              "      <td>Liso</td>\n",
              "      <td>B</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>ZEZINHO</td>\n",
              "      <td>J.C.CERQUEIRA</td>\n",
              "      <td>V.ALONSO V.</td>\n",
              "      <td>-3</td>\n",
              "      <td>[01,03,01,05,02]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>CHUSQUEZ</td>\n",
              "      <td>56</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>E.ARGUINZONES</td>\n",
              "      <td>J.GELABERT</td>\n",
              "      <td>(4)(8)</td>\n",
              "      <td>[09,06,09,16,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>SOFUNNY</td>\n",
              "      <td>59</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>TRILOGIA</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[02,02,03,02,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>60</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>EUROPA</td>\n",
              "      <td>J.C.CERQUEIRA</td>\n",
              "      <td>B.ESTUPIÑAN</td>\n",
              "      <td>(3)(8)</td>\n",
              "      <td>[08,10,03,09,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>59</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>REAPERTURA</td>\n",
              "      <td>A.TSERETELI</td>\n",
              "      <td>C.HAZEN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[05,03,08,07,05]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>CAROLINA WEST</td>\n",
              "      <td>59</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>TIZIANO</td>\n",
              "      <td>J.L.MAROTO</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[10,01,02,02,11]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>JERY SMAIH</td>\n",
              "      <td>58</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>E.ARGUINZONES</td>\n",
              "      <td>F.MARTINEZ</td>\n",
              "      <td>(3)(8)</td>\n",
              "      <td>[02,04,04,04,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>RUMBERA</td>\n",
              "      <td>58</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>COCHETEUX</td>\n",
              "      <td>M.ALVAREZ</td>\n",
              "      <td>N. DE JULIAN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[07,05,06,08,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>SANS ATTENDRE</td>\n",
              "      <td>61</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>IGUELDO</td>\n",
              "      <td>B.MORENO</td>\n",
              "      <td>R.N.VALLE</td>\n",
              "      <td>-3</td>\n",
              "      <td>[04,07,09,05,06]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>HIGHLAND MARKET</td>\n",
              "      <td>63</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>ENRIQUE FERNANDE...</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>-5</td>\n",
              "      <td>[05,07,07,03,03]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>54</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>DELTA</td>\n",
              "      <td>M.A.MARIN</td>\n",
              "      <td>A.MARTINEZ</td>\n",
              "      <td>-8</td>\n",
              "      <td>[15,09,11,10,04]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>ARETHA</td>\n",
              "      <td>61</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>ARTEMIS</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>C.PEREZ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[NP,04,08,04,01]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>IZAMAL</td>\n",
              "      <td>56</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>YEGUADA ARANJUEZ</td>\n",
              "      <td>J.LOPEZ</td>\n",
              "      <td>N.SACCU</td>\n",
              "      <td>-4</td>\n",
              "      <td>[09,07,06,09]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>UPSDAWN</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>DONALFA</td>\n",
              "      <td>M.J.PEREZ</td>\n",
              "      <td>D. FERREIRA</td>\n",
              "      <td>-8</td>\n",
              "      <td>[14,06,16,03,01]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>UPSILON</td>\n",
              "      <td>61</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>ANNUA RACING, S....</td>\n",
              "      <td>J.C.ROSELL</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-5</td>\n",
              "      <td>[04,17,05,03,08]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>14:40</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>2100</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>11</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58c267ce-6530-44ac-be84-92ab09125c64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58c267ce-6530-44ac-be84-92ab09125c64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58c267ce-6530-44ac-be84-92ab09125c64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def comprobacionTipoDistanciaTrain(indice):\n",
        "  tipo = 0   # 0, 1, 2 / Corta, Media, Larga\n",
        "  val = trainOg.at[indice, 'Distancia']\n",
        "  if(val >= 3200):\n",
        "    tipo = 2\n",
        "  if((val >= 1600) & (val <= 2400)):\n",
        "    tipo = 1\n",
        "  return tipo;\n",
        "\n",
        "def comprobacionTipoDistanciaTest(indice):\n",
        "  tipo = 0   # 0, 1, 2 / Corta, Media, Larga\n",
        "  val = train.at[indice, 'Distancia']\n",
        "  if(val >= 3200):\n",
        "    tipo = 2\n",
        "  if((val >= 1600) & (val <= 2400)):\n",
        "    tipo = 1\n",
        "  return tipo;\n",
        "  \n",
        "def destrezaDistanciaTest(indices,indicesOg):                      \n",
        "  for i in range(indices):\n",
        "    tipo = comprobacionTipoDistanciaTest(i)\n",
        "    nombre = train.at[i, 'NombreCaballo']\n",
        "    fecha = train.at[i, 'FechaAux']\n",
        "    media = 0 # Media carreras\n",
        "    sum = 0 # Numero Carreras\n",
        "    mediaAux = 0 \n",
        "    sumAux = 0\n",
        "    for j in range(indicesOg):\n",
        "      fechaAux = trainOg.at[j, 'FechaAux']\n",
        "      tipoAux = comprobacionTipoDistanciaTrain(j)\n",
        "      nombreAux= trainOg.at[j, 'NombreCaballo']\n",
        "      if(nombreAux == nombre):\n",
        "        if((fechaAux < fecha) & (tipoAux == tipo)):\n",
        "          \n",
        "          media += (trainOg.at[j, 'Puesto'])\n",
        "          sum += 1\n",
        "        elif((fechaAux < fecha) & (tipoAux != tipo)):\n",
        "          mediaAux += (trainOg.at[j, 'Puesto'])\n",
        "          sumAux += 1\n",
        "    if(sum != 0):\n",
        "      media /= sum\n",
        "    if(sumAux != 0):\n",
        "      mediaAux /= sumAux\n",
        "    print(i, \"-->\", nombre, \" - Media->\", media, \"/MediaAux-->\", mediaAux, \"/Numero Destreza(\", sum, sumAux, \"):Numero Otros/\") # Si hay mas de 2 participaciones (en esas condiciones y resto ) y con una media al menos un 33% mas baja , es valido\n",
        "    if((sum >= 2) & (sumAux >= 2) & (((1.1* media) < mediaAux))): # Si hay mas de 2 participaciones en esas condiciones y con una media al menos un 33% mas baja , es valido\n",
        "      train.at[i, 'DestrezaDistancia'] = 1\n",
        "    else:\n",
        "      train.at[i, 'DestrezaDistancia'] = 0\n",
        "        \n",
        "    #IMP! Valorar si es buena idea crear una columna a partir de la variable \"media\" + \"mediaAux\" obtenida aqui.\n",
        "\n",
        "indices = len(train.index)\n",
        "indicesOg = len(trainOg.index)\n",
        "destrezaDistanciaTest(indices, indicesOg)\n",
        "\n",
        "\n",
        "train.loc[~train[\"NombreCaballo\"].isin(listAparicionesHorse), \"NombreCaballo\"] = \"OtherHorse\" # Lo posponemos a este momento(por ahora)\n",
        "\n",
        "#Len Intervalos\n",
        "print(len(train[(train['Distancia'] < 1600).tolist()]))\n",
        "print(len(train[((train['Distancia'] >= 1600) & (train['Distancia'] <= 2400) ).tolist()]))\n",
        "print(len(train[(train['Distancia'] >= 3200).tolist()]))"
      ],
      "metadata": {
        "id": "VRq4PK4b4-4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985e0277-f7f6-45f4-e92e-c59434c074f0"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 --> PHILIPPO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "1 --> PONCE DE LEON  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "2 --> EL PATER  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "3 --> BRAGNOSERA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "4 --> BERTIZ  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "5 --> SANDRO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "6 --> GRAZALEMA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "7 --> NICOLASA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "8 --> LA PERLA NEGRA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "9 --> WHITY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "10 --> AFRODITA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "11 --> VADALEW  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "12 --> HEBE  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "13 --> WENDY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "14 --> MADRE MIA  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "15 --> LADY RAZALMA  - Media-> 0.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "16 --> FAITH ROSE  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "17 --> WARRIOR'S REVENGE  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "18 --> SHELBY  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "19 --> WHITE WINE  - Media-> 0 /MediaAux--> 3.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "20 --> VITA BARELIERE  - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "21 --> XILADO  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "22 --> WINTON  - Media-> 2.0 /MediaAux--> 4.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "23 --> AMERICANO  - Media-> 0 /MediaAux--> 2.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "24 --> SUPERIOR BEAUTY  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "25 --> STARSHADOW  - Media-> 2.0 /MediaAux--> 2.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "26 --> FUOCO GRECO  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "27 --> SANCTI PETRI  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "28 --> ROBLON  - Media-> 2.0 /MediaAux--> 0.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "29 --> WALKING TO GLORY  - Media-> 5.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "30 --> FORTUNATO  - Media-> 5.0 /MediaAux--> 1.3333333333333333 /Numero Destreza( 1 3 ):Numero Otros/\n",
            "31 --> AUSTRALIA CAPE  - Media-> 0 /MediaAux--> 1.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "32 --> MAITRE YODA  - Media-> 1.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "33 --> ASTURIAS  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "34 --> SEVERUS  - Media-> 2.0 /MediaAux--> 2.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "35 --> ROBAYERA  - Media-> 0.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "36 --> VIKING CITY  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "37 --> KANE ORE  - Media-> 2.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "38 --> EMBAT  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "39 --> MONTERREDONDO  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "40 --> HADES  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "41 --> USI DE U  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "42 --> MEDICEAN BLUE  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "43 --> EL CANEY  - Media-> 0 /MediaAux--> 6.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "44 --> FINELY TUNED  - Media-> 3.0 /MediaAux--> 2.0 /Numero Destreza( 1 2 ):Numero Otros/\n",
            "45 --> THE GAME  - Media-> 0 /MediaAux--> 0.5 /Numero Destreza( 0 2 ):Numero Otros/\n",
            "46 --> ORBAYO  - Media-> 0.0 /MediaAux--> 1.3333333333333333 /Numero Destreza( 1 3 ):Numero Otros/\n",
            "47 --> ASTRAL  - Media-> 0.0 /MediaAux--> 3.0 /Numero Destreza( 1 2 ):Numero Otros/\n",
            "48 --> TRES DE TREBOL  - Media-> 1.0 /MediaAux--> 0.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "49 --> SOGALINDA  - Media-> 0.75 /MediaAux--> 0 /Numero Destreza( 4 0 ):Numero Otros/\n",
            "50 --> CHUSQUEZ  - Media-> 0 /MediaAux--> 4.0 /Numero Destreza( 0 1 ):Numero Otros/\n",
            "51 --> SOFUNNY  - Media-> 0.3333333333333333 /MediaAux--> 0 /Numero Destreza( 3 0 ):Numero Otros/\n",
            "52 --> LA MAL AMADA  - Media-> 3.5 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "53 --> ATLANTICO  - Media-> 2.0 /MediaAux--> 1.0 /Numero Destreza( 2 1 ):Numero Otros/\n",
            "54 --> CAROLINA WEST  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "55 --> JERY SMAIH  - Media-> 0 /MediaAux--> 0 /Numero Destreza( 0 0 ):Numero Otros/\n",
            "56 --> RUMBERA  - Media-> 3.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "57 --> SANS ATTENDRE  - Media-> 1.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "58 --> HIGHLAND MARKET  - Media-> 2.0 /MediaAux--> 3.0 /Numero Destreza( 1 1 ):Numero Otros/\n",
            "59 --> PIU BIRCH  - Media-> 3.6666666666666665 /MediaAux--> 0 /Numero Destreza( 6 0 ):Numero Otros/\n",
            "60 --> ARETHA  - Media-> 2.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "61 --> IZAMAL  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 1 0 ):Numero Otros/\n",
            "62 --> UPSDAWN  - Media-> 0 /MediaAux--> 4.333333333333333 /Numero Destreza( 0 3 ):Numero Otros/\n",
            "63 --> UPSILON  - Media-> 4.0 /MediaAux--> 0 /Numero Destreza( 2 0 ):Numero Otros/\n",
            "0\n",
            "58\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "train[['NombreCaballo','DestrezaDistancia']].head(64)"
      ],
      "metadata": {
        "id": "xSJojUpN4-4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82cbb8d2-8e2d-4e50-be1e-34ddb9f593cb"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        NombreCaballo  DestrezaDistancia\n",
              "0          OtherHorse                0.0\n",
              "1          OtherHorse                0.0\n",
              "2          OtherHorse                0.0\n",
              "3          OtherHorse                0.0\n",
              "4          OtherHorse                0.0\n",
              "5          OtherHorse                0.0\n",
              "6          OtherHorse                0.0\n",
              "7          OtherHorse                0.0\n",
              "8          OtherHorse                0.0\n",
              "9          OtherHorse                0.0\n",
              "10         OtherHorse                0.0\n",
              "11         OtherHorse                0.0\n",
              "12         OtherHorse                0.0\n",
              "13         OtherHorse                0.0\n",
              "14         OtherHorse                0.0\n",
              "15       LADY RAZALMA                0.0\n",
              "16         FAITH ROSE                0.0\n",
              "17  WARRIOR'S REVENGE                0.0\n",
              "18             SHELBY                0.0\n",
              "19         WHITE WINE                0.0\n",
              "20     VITA BARELIERE                0.0\n",
              "21             XILADO                0.0\n",
              "22             WINTON                0.0\n",
              "23          AMERICANO                0.0\n",
              "24         OtherHorse                0.0\n",
              "25         STARSHADOW                0.0\n",
              "26         OtherHorse                0.0\n",
              "27       SANCTI PETRI                0.0\n",
              "28             ROBLON                0.0\n",
              "29   WALKING TO GLORY                0.0\n",
              "30          FORTUNATO                0.0\n",
              "31     AUSTRALIA CAPE                0.0\n",
              "32        MAITRE YODA                0.0\n",
              "33           ASTURIAS                0.0\n",
              "34            SEVERUS                0.0\n",
              "35           ROBAYERA                0.0\n",
              "36        VIKING CITY                0.0\n",
              "37           KANE ORE                0.0\n",
              "38              EMBAT                0.0\n",
              "39      MONTERREDONDO                0.0\n",
              "40         OtherHorse                0.0\n",
              "41         OtherHorse                0.0\n",
              "42      MEDICEAN BLUE                0.0\n",
              "43           EL CANEY                0.0\n",
              "44       FINELY TUNED                0.0\n",
              "45           THE GAME                0.0\n",
              "46             ORBAYO                0.0\n",
              "47             ASTRAL                0.0\n",
              "48     TRES DE TREBOL                0.0\n",
              "49          SOGALINDA                0.0\n",
              "50           CHUSQUEZ                0.0\n",
              "51            SOFUNNY                0.0\n",
              "52       LA MAL AMADA                0.0\n",
              "53          ATLANTICO                0.0\n",
              "54      CAROLINA WEST                0.0\n",
              "55         OtherHorse                0.0\n",
              "56            RUMBERA                0.0\n",
              "57      SANS ATTENDRE                0.0\n",
              "58    HIGHLAND MARKET                0.0\n",
              "59          PIU BIRCH                0.0\n",
              "60             ARETHA                0.0\n",
              "61             IZAMAL                0.0\n",
              "62            UPSDAWN                0.0\n",
              "63            UPSILON                0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1db9d629-1184-4fd9-b099-ebfb669420c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>DestrezaDistancia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>LADY RAZALMA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>FAITH ROSE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>WARRIOR'S REVENGE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SHELBY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>WHITE WINE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>VITA BARELIERE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>XILADO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>WINTON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AMERICANO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>SANCTI PETRI</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ROBLON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>WALKING TO GLORY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>FORTUNATO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>AUSTRALIA CAPE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>MAITRE YODA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>SEVERUS</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>ROBAYERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>VIKING CITY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>KANE ORE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>EMBAT</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>MONTERREDONDO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>MEDICEAN BLUE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>EL CANEY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>FINELY TUNED</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>THE GAME</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>ORBAYO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>ASTRAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>TRES DE TREBOL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>CHUSQUEZ</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>SOFUNNY</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>CAROLINA WEST</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>RUMBERA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>SANS ATTENDRE</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>HIGHLAND MARKET</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>ARETHA</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>IZAMAL</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>UPSDAWN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>UPSILON</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1db9d629-1184-4fd9-b099-ebfb669420c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1db9d629-1184-4fd9-b099-ebfb669420c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1db9d629-1184-4fd9-b099-ebfb669420c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNcJzf5clRwn"
      },
      "source": [
        "## **Tratamiento (Edad)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "7drI0KXqlRwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347f647c-b20d-4778-cedf-6875eeb1c828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    31\n",
              "4    13\n",
              "3    12\n",
              "5     4\n",
              "7     2\n",
              "9     1\n",
              "6     1\n",
              "Name: Edad, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ],
      "source": [
        "train['Edad'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgzb2rAalRwn"
      },
      "source": [
        "## **Tratamiento (Mantilla)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "FK4j7b2LlRwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326fe4e5-f077-422e-d6ec-cef7c7d5b802"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4     6\n",
              "5     6\n",
              "3     6\n",
              "2     6\n",
              "1     6\n",
              "6     6\n",
              "9     4\n",
              "7     4\n",
              "8     4\n",
              "12    3\n",
              "11    3\n",
              "10    3\n",
              "15    2\n",
              "14    2\n",
              "13    2\n",
              "16    1\n",
              "Name: Mantilla, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "train['Mantilla'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLeBdZtqlRwp"
      },
      "source": [
        "## **Tratamiento (Problemas)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "MHy4cLmZlRwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112f4fe0-f2aa-4ce5-be4f-9091b0372d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-8           13\n",
              "-4            5\n",
              "-3            5\n",
              "(3)(8)        5\n",
              "-5            3\n",
              "(4)(8)        2\n",
              "(4)(5)(8)     1\n",
              "Name: Problemas, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ],
      "source": [
        "train['Problemas'].value_counts()[:35].sort_values(ascending=False) #90 primeras apariencias >= 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "RjpTdMkrlRwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a290f22e-daae-4a9d-e544-138ec482cbea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  NombreCaballo Peso  Edad  Mantilla       Propietario Preparador  \\\n",
              "0    OtherHorse   57     2         4        OtherOwner  T.MARTINS   \n",
              "1    OtherHorse   57     2         5  SALVADOR MARQUEZ  B.VALENTI   \n",
              "2    OtherHorse   57     2         3           GISPERT    P.OLAVE   \n",
              "3    OtherHorse   57     2         2         ZURRAQUIN     B.RAMA   \n",
              "4    OtherHorse   57     2         1       QUINTO REAL    P.OLAVE   \n",
              "\n",
              "         Jinete Problemas UltimasActuaciones                    Fecha   Hora  \\\n",
              "0      B. FAYOS        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "1    V. JANACEK        -8      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "2  J.L. BORREGO        -4      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "3     L.FONSECA         0      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "4      C. CADEL         0      [ Debutante ]  20 de noviembre de 2022  11:45   \n",
              "\n",
              "      Terreno  Distancia  Tipo Categoría  SentidoHipodromo  \\\n",
              "0  H - Blando       1600  Liso         D                 0   \n",
              "1  H - Blando       1600  Liso         D                 0   \n",
              "2  H - Blando       1600  Liso         D                 0   \n",
              "3  H - Blando       1600  Liso         D                 0   \n",
              "4  H - Blando       1600  Liso         D                 0   \n",
              "\n",
              "           Meteorología  LLuvia  Viento  Temperatura    Hipodromo  \\\n",
              "0  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "1  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "2  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "3  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "4  Parcialmente nublado       0      15           10  La Zarzuela   \n",
              "\n",
              "   OtherJockey  OtherTrainer   FechaAux    year  month   day  Otoño  \\\n",
              "0            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "1            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "2            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "3            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "4            0             0 2022-11-20  2022.0   11.0  20.0      0   \n",
              "\n",
              "   DiasDesdeCarrera  DaysSincePreviousRace  Contrincantes  DestrezaDistancia  \\\n",
              "0              30.0                   45.0            6.0                0.0   \n",
              "1              30.0                   45.0            6.0                0.0   \n",
              "2              30.0                   45.0            6.0                0.0   \n",
              "3              30.0                   45.0            6.0                0.0   \n",
              "4              30.0                   45.0            6.0                0.0   \n",
              "\n",
              "   Problema_Nulo  Problema_1  Problema_2  Problema_3  Problema_4  Problema_5  \\\n",
              "0              0           0           0           0           0           0   \n",
              "1              0           0           0           0           0           0   \n",
              "2              0           0           0           0           1           0   \n",
              "3              1           0           0           0           0           0   \n",
              "4              1           0           0           0           0           0   \n",
              "\n",
              "   Problema_6  Problema_7  Problema_8  \n",
              "0           0           0           1  \n",
              "1           0           0           1  \n",
              "2           0           0           0  \n",
              "3           0           0           0  \n",
              "4           0           0           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-708d52f7-ca9d-4f74-b178-9c64efa8d6d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Peso</th>\n",
              "      <th>Edad</th>\n",
              "      <th>Mantilla</th>\n",
              "      <th>Propietario</th>\n",
              "      <th>Preparador</th>\n",
              "      <th>Jinete</th>\n",
              "      <th>Problemas</th>\n",
              "      <th>UltimasActuaciones</th>\n",
              "      <th>Fecha</th>\n",
              "      <th>Hora</th>\n",
              "      <th>Terreno</th>\n",
              "      <th>Distancia</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>Categoría</th>\n",
              "      <th>SentidoHipodromo</th>\n",
              "      <th>Meteorología</th>\n",
              "      <th>LLuvia</th>\n",
              "      <th>Viento</th>\n",
              "      <th>Temperatura</th>\n",
              "      <th>Hipodromo</th>\n",
              "      <th>OtherJockey</th>\n",
              "      <th>OtherTrainer</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Otoño</th>\n",
              "      <th>DiasDesdeCarrera</th>\n",
              "      <th>DaysSincePreviousRace</th>\n",
              "      <th>Contrincantes</th>\n",
              "      <th>DestrezaDistancia</th>\n",
              "      <th>Problema_Nulo</th>\n",
              "      <th>Problema_1</th>\n",
              "      <th>Problema_2</th>\n",
              "      <th>Problema_3</th>\n",
              "      <th>Problema_4</th>\n",
              "      <th>Problema_5</th>\n",
              "      <th>Problema_6</th>\n",
              "      <th>Problema_7</th>\n",
              "      <th>Problema_8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>OtherOwner</td>\n",
              "      <td>T.MARTINS</td>\n",
              "      <td>B. FAYOS</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>SALVADOR MARQUEZ</td>\n",
              "      <td>B.VALENTI</td>\n",
              "      <td>V. JANACEK</td>\n",
              "      <td>-8</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>GISPERT</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>J.L. BORREGO</td>\n",
              "      <td>-4</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>ZURRAQUIN</td>\n",
              "      <td>B.RAMA</td>\n",
              "      <td>L.FONSECA</td>\n",
              "      <td>0</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>57</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>QUINTO REAL</td>\n",
              "      <td>P.OLAVE</td>\n",
              "      <td>C. CADEL</td>\n",
              "      <td>0</td>\n",
              "      <td>[ Debutante ]</td>\n",
              "      <td>20 de noviembre de 2022</td>\n",
              "      <td>11:45</td>\n",
              "      <td>H - Blando</td>\n",
              "      <td>1600</td>\n",
              "      <td>Liso</td>\n",
              "      <td>D</td>\n",
              "      <td>0</td>\n",
              "      <td>Parcialmente nublado</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>La Zarzuela</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-708d52f7-ca9d-4f74-b178-9c64efa8d6d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-708d52f7-ca9d-4f74-b178-9c64efa8d6d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-708d52f7-ca9d-4f74-b178-9c64efa8d6d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ],
      "source": [
        "train['Problemas'] = train['Problemas'].fillna('0')\n",
        "train['Problema_Nulo'] = np.where(train['Problemas'].str.contains('0'),1,0)\n",
        "train['Problema_1'] = np.where(train['Problemas'].str.contains('1'),1,0)\n",
        "train['Problema_2'] = np.where(train['Problemas'].str.contains('2'),1,0)\n",
        "train['Problema_3'] = np.where(train['Problemas'].str.contains('3'),1,0)\n",
        "train['Problema_4'] = np.where(train['Problemas'].str.contains('4'),1,0)\n",
        "train['Problema_5'] = np.where(train['Problemas'].str.contains('5'),1,0)\n",
        "train['Problema_6'] = np.where(train['Problemas'].str.contains('6'),1,0)\n",
        "train['Problema_7'] = np.where(train['Problemas'].str.contains('7'),1,0)\n",
        "train['Problema_8'] = np.where(train['Problemas'].str.contains('8'),1,0)\n",
        "train.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0hIuNADlRwq"
      },
      "source": [
        "## **Tratamiento (UltimasActuaciones)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "brpsU6QnlRwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300f77da-0ca4-4568-ef64-eefac5c8ddad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n",
            "1.0\n",
            "******INDEX  17 *****\n",
            "Sum --> 3.000241411346236 /Len --> 8\n",
            "Result --> 3.000241411346236 /Element --> 01080505\n",
            "******INDEX  18 *****\n",
            "Sum --> 3.711731611910263 /Len --> 4\n",
            "Result --> 3.711731611910263 /Element --> 0207\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  19 *****\n",
            "Sum --> 2.590836032979428 /Len --> 10\n",
            "Result --> 2.590836032979428 /Element --> 1003050303\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  20 *****\n",
            "Sum --> 3.5211584329705383 /Len --> 8\n",
            "Result --> 3.5211584329705383 /Element --> 06050506\n",
            "******INDEX  21 *****\n",
            "Sum --> 6.712817864248718 /Len --> 4\n",
            "Result --> 6.712817864248718 /Element --> 0510\n",
            "******INDEX  22 *****\n",
            "Sum --> 11.292250275123644 /Len --> 2\n",
            "Result --> 11.292250275123644 /Element --> 11\n",
            "******INDEX  23 *****\n",
            "Sum --> 5.599087922135354 /Len --> 4\n",
            "Result --> 5.599087922135354 /Element --> 0806\n",
            "******INDEX  24 *****\n",
            "Sum --> 8.90491073082388 /Len --> 4\n",
            "Result --> 8.90491073082388 /Element --> 0712\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  25 *****\n",
            "Sum --> 2.4360262976035663 /Len --> 10\n",
            "Result --> 2.4360262976035663 /Element --> 0504070204\n",
            "******INDEX  26 *****\n",
            "Sum --> 1.292791877736932 /Len --> 4\n",
            "Result --> 1.292791877736932 /Element --> 0103\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  27 *****\n",
            "Sum --> 3.8042118893282098 /Len --> 8\n",
            "Result --> 3.8042118893282098 /Element --> 05070705\n",
            "******INDEX  28 *****\n",
            "Sum --> 11.292250275123644 /Len --> 2\n",
            "Result --> 11.292250275123644 /Element --> 11\n",
            "******INDEX  29 *****\n",
            "Sum --> 2.085583755473864 /Len --> 2\n",
            "Result --> 2.085583755473864 /Element --> 03\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  30 *****\n",
            "Sum --> 3.6532405718228738 /Len --> 8\n",
            "Result --> 3.6532405718228738 /Element --> 03090208\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "******INDEX  31 *****\n",
            "Sum --> 3.796151457334767 /Len --> 10\n",
            "Result --> 3.796151457334767 /Element --> 1307050504\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n",
            "3.0\n",
            "2.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "def count_lastRaces(arrayLast):\n",
        "  index = 0\n",
        "  for element in arrayLast:\n",
        "    element = element.replace(',', '')\n",
        "    element = element.replace('[', '')\n",
        "    element = element[:len(element)-1]\n",
        "    sum = 0\n",
        "    if 'Debutante' in element:\n",
        "      sum = 10\n",
        "      element = []\n",
        "    else:\n",
        "      element = element.replace('NP', '08')\n",
        "      element = element.replace('Desc', '08')\n",
        "      element = element.replace(' ', '')\n",
        "      i = len(element)\n",
        "      j = len(element)\n",
        "      iter = (len(element)) / 2\n",
        "      count = 0\n",
        "      while(iter > 0):\n",
        "        if(count == 0):\n",
        "          i = i-2\n",
        "          sum += int(element[i:j])**1.3\n",
        "          j = j-2\n",
        "        elif(count == 1):\n",
        "          i = i-2\n",
        "          sum += int(element[i:j])**1.2\n",
        "          j = j-2\n",
        "        else:\n",
        "          print(iter)\n",
        "          i = i-2\n",
        "          sum += int(element[i:j])\n",
        "          j = j-2\n",
        "        count += 1\n",
        "        iter -= 1\n",
        "      sum = sum / len(element)\n",
        "      if(index < 30):\n",
        "        print('******INDEX ', index+2, '*****')\n",
        "        print('Sum -->', sum, '/Len -->', len(element))\n",
        "        print('Result -->', sum, '/Element -->' ,element)\n",
        "    train.at[index, 'MediaUltimasActuaciones'] = sum\n",
        "    train.at[index, 'CantidadActuaciones'] = len(element)\n",
        "    index += 1\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "t_array = train[\"UltimasActuaciones\"]\n",
        "count_lastRaces(t_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "YZ7UPavTlRwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf48150-8e96-432b-f48c-79ace5671080"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     10.000000\n",
              "1     10.000000\n",
              "2     10.000000\n",
              "3     10.000000\n",
              "4     10.000000\n",
              "5     10.000000\n",
              "6     10.000000\n",
              "7     10.000000\n",
              "8     10.000000\n",
              "9     10.000000\n",
              "10    10.000000\n",
              "11    10.000000\n",
              "12    10.000000\n",
              "13    10.000000\n",
              "14    10.000000\n",
              "15     3.000241\n",
              "16     3.711732\n",
              "17     2.590836\n",
              "18     3.521158\n",
              "19     6.712818\n",
              "20    11.292250\n",
              "21     5.599088\n",
              "22     8.904911\n",
              "23     2.436026\n",
              "24     1.292792\n",
              "Name: MediaUltimasActuaciones, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ],
      "source": [
        " train['MediaUltimasActuaciones'].head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dZmuPgIlRws"
      },
      "source": [
        "## **Tratamiento (Hora)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "BD5YDOnylRws"
      },
      "outputs": [],
      "source": [
        "train['Hora'] = train['Hora'].str.replace(':', '') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_92pJYndlRwt"
      },
      "source": [
        "## **Tratamiento (Terreno)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxCqIWailRwt"
      },
      "source": [
        "## **Tratamiento (Distancia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6UpYVOVlRwt"
      },
      "source": [
        "## **Tratamiento (Tipo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcaDp1-9lRwt"
      },
      "source": [
        "## **Tratamiento (Categoria)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNYSed3olRwt"
      },
      "source": [
        "## **Tratamiento (SentidoHipodromo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D0eOBFflRwt"
      },
      "source": [
        "## **Tratamiento (Meteorologia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD5cZkFclRwu"
      },
      "source": [
        "## **Tratamiento (Lluvia)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HouSXFB_lRwu"
      },
      "source": [
        "## **Tratamiento (Viento)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL0mqMmxlRwu"
      },
      "source": [
        "## **Tratamiento (TemperaturaMax)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA170c9RlRwu"
      },
      "source": [
        "## **Tratamiento (TemperaturaMin)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ-ZsvXDlRwx"
      },
      "source": [
        "## **Tratamiento (Hipodromo)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyOyCLzG4a-"
      },
      "source": [
        "#Continuacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "qr4DCNzkHSC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7452d67c-f7ee-4765-cbda-e86b488e9c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64 entries, 0 to 63\n",
            "Data columns (total 44 columns):\n",
            " #   Column                   Non-Null Count  Dtype         \n",
            "---  ------                   --------------  -----         \n",
            " 0   index                    64 non-null     int64         \n",
            " 1   NombreCaballo            64 non-null     object        \n",
            " 2   Peso                     64 non-null     object        \n",
            " 3   Edad                     64 non-null     int64         \n",
            " 4   Mantilla                 64 non-null     int64         \n",
            " 5   Propietario              64 non-null     object        \n",
            " 6   Preparador               64 non-null     object        \n",
            " 7   Jinete                   64 non-null     object        \n",
            " 8   Problemas                64 non-null     object        \n",
            " 9   UltimasActuaciones       64 non-null     object        \n",
            " 10  Fecha                    64 non-null     object        \n",
            " 11  Hora                     64 non-null     object        \n",
            " 12  Terreno                  64 non-null     object        \n",
            " 13  Distancia                64 non-null     int64         \n",
            " 14  Tipo                     64 non-null     object        \n",
            " 15  Categoría                64 non-null     object        \n",
            " 16  SentidoHipodromo         64 non-null     int64         \n",
            " 17  Meteorología             64 non-null     object        \n",
            " 18  LLuvia                   64 non-null     int64         \n",
            " 19  Viento                   64 non-null     int64         \n",
            " 20  Temperatura              64 non-null     int64         \n",
            " 21  Hipodromo                64 non-null     object        \n",
            " 22  OtherJockey              64 non-null     int64         \n",
            " 23  OtherTrainer             64 non-null     int64         \n",
            " 24  FechaAux                 64 non-null     datetime64[ns]\n",
            " 25  year                     64 non-null     float64       \n",
            " 26  month                    64 non-null     float64       \n",
            " 27  day                      64 non-null     float64       \n",
            " 28  Otoño                    64 non-null     int64         \n",
            " 29  DiasDesdeCarrera         64 non-null     float64       \n",
            " 30  DaysSincePreviousRace    64 non-null     float64       \n",
            " 31  Contrincantes            64 non-null     float64       \n",
            " 32  DestrezaDistancia        64 non-null     float64       \n",
            " 33  Problema_Nulo            64 non-null     int64         \n",
            " 34  Problema_1               64 non-null     int64         \n",
            " 35  Problema_2               64 non-null     int64         \n",
            " 36  Problema_3               64 non-null     int64         \n",
            " 37  Problema_4               64 non-null     int64         \n",
            " 38  Problema_5               64 non-null     int64         \n",
            " 39  Problema_6               64 non-null     int64         \n",
            " 40  Problema_7               64 non-null     int64         \n",
            " 41  Problema_8               64 non-null     int64         \n",
            " 42  MediaUltimasActuaciones  64 non-null     float64       \n",
            " 43  CantidadActuaciones      64 non-null     float64       \n",
            "dtypes: datetime64[ns](1), float64(9), int64(20), object(14)\n",
            "memory usage: 22.1+ KB\n"
          ]
        }
      ],
      "source": [
        "X_valu = train\n",
        "# X_val = X_val.reset_index()\n",
        "ids = train\n",
        "X_valu = X_valu.reset_index()\n",
        "X_valu.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "fnvAxxhxTGJh"
      },
      "outputs": [],
      "source": [
        "column_transformer_test = sklearn.compose.ColumnTransformer(transformers=[\n",
        " (\"drop\", \"drop\", ['UltimasActuaciones','Fecha','year','day', 'Problemas','Hora', 'FechaAux', 'Mantilla', 'Terreno', 'Tipo','Categoría', 'Meteorología', 'LLuvia', 'Viento', 'Temperatura', 'Hipodromo', 'Contrincantes']),\n",
        "    (\"scale\", sklearn.preprocessing.StandardScaler(), []),\n",
        "    # (\"num\", sklearn.preprocessing.MinMaxScaler(), ['Distancia', 'Edad', 'Peso']),\n",
        "    (\"one-hot\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), ['NombreCaballo','Propietario', 'Preparador', 'Jinete']),\n",
        "], remainder='passthrough');\n",
        "\n",
        "# column_transformer = sklearn.compose.ColumnTransformer(transformers=[\n",
        "#     (\"drop\", \"drop\", ['UltimasActuaciones','Fecha','year','day', 'Problemas']),\n",
        "#     (\"scale\", sklearn.preprocessing.StandardScaler(), []),\n",
        "#     (\"one-hot\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\"), ['NombreCaballo','Peso','Edad', 'Mantilla','Propietario', 'Preparador', 'Jinete', 'estacion', 'FechaAux', 'Terreno','Distancia', 'Tipo','Categoría', 'Meteorología', 'LLuvia', 'Viento', 'TemperaturaMax', 'TemperaturaMin', 'Hipodromo']),\n",
        "# ], remainder='passthrough');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "WZY_P1RQR1u4"
      },
      "outputs": [],
      "source": [
        "train = train.reset_index()\n",
        "\n",
        "X_val = column_transformer_test.fit_transform(X_valu)\n",
        "# X_test_transform = column_transformer.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_transformer_train.get_feature_names()"
      ],
      "metadata": {
        "id": "KeotIhAwzGNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8eab70-57a4-48df-fd4f-1896eeeb991e"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one-hot__x0_AMERICANO',\n",
              " 'one-hot__x0_ARETHA',\n",
              " 'one-hot__x0_ASTRAL',\n",
              " 'one-hot__x0_ASTURIAS',\n",
              " 'one-hot__x0_ATLANTICO',\n",
              " 'one-hot__x0_AUSTRALIA CAPE',\n",
              " 'one-hot__x0_CAROLINA WEST',\n",
              " 'one-hot__x0_CHUSQUEZ',\n",
              " 'one-hot__x0_EL CANEY',\n",
              " 'one-hot__x0_EMBAT',\n",
              " 'one-hot__x0_FAITH ROSE',\n",
              " 'one-hot__x0_FINELY TUNED',\n",
              " 'one-hot__x0_FORTUNATO',\n",
              " 'one-hot__x0_HIGHLAND MARKET',\n",
              " 'one-hot__x0_IZAMAL',\n",
              " 'one-hot__x0_KANE ORE',\n",
              " 'one-hot__x0_LA MAL AMADA',\n",
              " 'one-hot__x0_LADY RAZALMA',\n",
              " 'one-hot__x0_MAITRE YODA',\n",
              " 'one-hot__x0_MEDICEAN BLUE',\n",
              " 'one-hot__x0_MONTERREDONDO',\n",
              " 'one-hot__x0_ORBAYO',\n",
              " 'one-hot__x0_OtherHorse',\n",
              " 'one-hot__x0_PIU BIRCH',\n",
              " 'one-hot__x0_ROBAYERA',\n",
              " 'one-hot__x0_ROBLON',\n",
              " 'one-hot__x0_RUMBERA',\n",
              " 'one-hot__x0_SANCTI PETRI',\n",
              " 'one-hot__x0_SANS ATTENDRE',\n",
              " 'one-hot__x0_SEVERUS',\n",
              " 'one-hot__x0_SHELBY',\n",
              " 'one-hot__x0_SOFUNNY',\n",
              " 'one-hot__x0_SOGALINDA',\n",
              " 'one-hot__x0_STARSHADOW',\n",
              " 'one-hot__x0_THE GAME',\n",
              " 'one-hot__x0_TRES DE TREBOL',\n",
              " 'one-hot__x0_UPSDAWN',\n",
              " 'one-hot__x0_UPSILON',\n",
              " 'one-hot__x0_VIKING CITY',\n",
              " 'one-hot__x0_VITA BARELIERE',\n",
              " 'one-hot__x0_WALKING TO GLORY',\n",
              " \"one-hot__x0_WARRIOR'S REVENGE\",\n",
              " 'one-hot__x0_WHITE WINE',\n",
              " 'one-hot__x0_WINTON',\n",
              " 'one-hot__x0_XILADO',\n",
              " 'one-hot__x1_4 C',\n",
              " 'one-hot__x1_AMAZING TURF',\n",
              " 'one-hot__x1_ANNUA RACING, S....',\n",
              " 'one-hot__x1_ARTEMIS',\n",
              " 'one-hot__x1_ATERPE',\n",
              " 'one-hot__x1_BERNIE',\n",
              " 'one-hot__x1_CANARIAS',\n",
              " 'one-hot__x1_CARAL',\n",
              " 'one-hot__x1_CELSO MENDEZ',\n",
              " 'one-hot__x1_CENTURION',\n",
              " 'one-hot__x1_CIELO DE MADRID',\n",
              " 'one-hot__x1_COCHETEUX',\n",
              " 'one-hot__x1_CUADRA A.F.F S.L',\n",
              " 'one-hot__x1_DELTA',\n",
              " 'one-hot__x1_DONALFA',\n",
              " 'one-hot__x1_DUQUE DE ALBURQU...',\n",
              " 'one-hot__x1_ECURIE DES MOUET...',\n",
              " 'one-hot__x1_EL GENTIO',\n",
              " 'one-hot__x1_ELEVAGE LA REVER...',\n",
              " 'one-hot__x1_ENRIQUE FERNANDE...',\n",
              " 'one-hot__x1_EUROPA',\n",
              " 'one-hot__x1_FORRAJES Y CEREA...',\n",
              " 'one-hot__x1_GISPERT',\n",
              " 'one-hot__x1_IGUELDO',\n",
              " 'one-hot__x1_JACAL',\n",
              " 'one-hot__x1_JAVIER MALDONADO...',\n",
              " 'one-hot__x1_LA TOLEDANA',\n",
              " 'one-hot__x1_LAS AGUILAS',\n",
              " 'one-hot__x1_MARTUL',\n",
              " 'one-hot__x1_MEDITERRANEO',\n",
              " 'one-hot__x1_OtherOwner',\n",
              " 'one-hot__x1_PAOLO NERI',\n",
              " 'one-hot__x1_QUINTO REAL',\n",
              " 'one-hot__x1_REAPERTURA',\n",
              " 'one-hot__x1_SALVADOR MARQUEZ',\n",
              " 'one-hot__x1_SANTA BARBARA',\n",
              " 'one-hot__x1_TEN FE',\n",
              " 'one-hot__x1_TIGRES',\n",
              " 'one-hot__x1_TIZIANO',\n",
              " 'one-hot__x1_TRILOGIA',\n",
              " 'one-hot__x1_VALLADOLID',\n",
              " 'one-hot__x1_YEGUADA AGF',\n",
              " 'one-hot__x1_YEGUADA ARANJUEZ',\n",
              " 'one-hot__x1_YEGUADA ROCIO',\n",
              " 'one-hot__x1_ZEZINHO',\n",
              " 'one-hot__x1_ZUL',\n",
              " 'one-hot__x1_ZURRAQUIN',\n",
              " 'one-hot__x2_A.CARRASCO',\n",
              " 'one-hot__x2_A.IMAZ,B.',\n",
              " 'one-hot__x2_A.NUÑEZ',\n",
              " 'one-hot__x2_A.SOTO',\n",
              " 'one-hot__x2_A.TSERETELI',\n",
              " 'one-hot__x2_B.MORENO',\n",
              " 'one-hot__x2_B.RAMA',\n",
              " 'one-hot__x2_B.VALENTI',\n",
              " 'one-hot__x2_C.FERNANDEZ',\n",
              " 'one-hot__x2_D.DIEZ',\n",
              " 'one-hot__x2_E.ARGUINZONES',\n",
              " 'one-hot__x2_F.RODRIGUEZ',\n",
              " 'one-hot__x2_G.ARIZKORRETA',\n",
              " 'one-hot__x2_J.A.RODRIGUEZ',\n",
              " 'one-hot__x2_J.C.CERQUEIRA',\n",
              " 'one-hot__x2_J.C.ROSELL',\n",
              " 'one-hot__x2_J.L.MAROTO',\n",
              " 'one-hot__x2_J.LOPEZ',\n",
              " 'one-hot__x2_J.M.OSORIO',\n",
              " 'one-hot__x2_M&M RACING',\n",
              " 'one-hot__x2_M.A.MARIN',\n",
              " 'one-hot__x2_M.ALONSO R.',\n",
              " 'one-hot__x2_M.ALVAREZ',\n",
              " 'one-hot__x2_M.J.PEREZ',\n",
              " 'one-hot__x2_OtherTrainer',\n",
              " 'one-hot__x2_P.OLAVE',\n",
              " 'one-hot__x2_R.MARTIN V.',\n",
              " 'one-hot__x2_T.MARTINS',\n",
              " 'one-hot__x3_A.MARTINEZ',\n",
              " 'one-hot__x3_B. FAYOS',\n",
              " 'one-hot__x3_B.ESTUPIÑAN',\n",
              " 'one-hot__x3_C. CADEL',\n",
              " 'one-hot__x3_C.HAZEN',\n",
              " 'one-hot__x3_C.PEREZ',\n",
              " 'one-hot__x3_D. FERREIRA',\n",
              " 'one-hot__x3_D.SIKOROVÁ',\n",
              " 'one-hot__x3_F.MARTINEZ',\n",
              " 'one-hot__x3_G.TROLLEY DE PRE...',\n",
              " 'one-hot__x3_J.GELABERT',\n",
              " 'one-hot__x3_J.L. BORREGO',\n",
              " 'one-hot__x3_J.L. MARTINEZ',\n",
              " 'one-hot__x3_L.FONSECA',\n",
              " 'one-hot__x3_N. DE JULIAN',\n",
              " 'one-hot__x3_N. GARCIA',\n",
              " 'one-hot__x3_N.SACCU',\n",
              " 'one-hot__x3_OtherJockey',\n",
              " 'one-hot__x3_R.N.VALLE',\n",
              " 'one-hot__x3_SRTA. TENA, C.',\n",
              " 'one-hot__x3_STA. BUESA,C.',\n",
              " 'one-hot__x3_V. JANACEK',\n",
              " 'one-hot__x3_V.ALONSO V.',\n",
              " 'one-hot__x3_Y.RODRIGUEZ',\n",
              " 'index',\n",
              " 'Peso',\n",
              " 'Edad',\n",
              " 'Distancia',\n",
              " 'SentidoHipodromo',\n",
              " 'month',\n",
              " 'Otoño',\n",
              " 'DiasDesdeCarrera',\n",
              " 'DaysSincePreviousRace',\n",
              " 'DestrezaDistancia',\n",
              " 'Problema_Nulo',\n",
              " 'Problema_1',\n",
              " 'Problema_2',\n",
              " 'Problema_3',\n",
              " 'Problema_4',\n",
              " 'Problema_5',\n",
              " 'Problema_6',\n",
              " 'Problema_7',\n",
              " 'Problema_8',\n",
              " 'MediaUltimasActuaciones',\n",
              " 'CantidadActuaciones']"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_transformer_test.get_feature_names()"
      ],
      "metadata": {
        "id": "b2wOYxlvX5Qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2b5a45-3874-46e7-bde2-8ce210a83449"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one-hot__x0_AMERICANO',\n",
              " 'one-hot__x0_ARETHA',\n",
              " 'one-hot__x0_ASTRAL',\n",
              " 'one-hot__x0_ASTURIAS',\n",
              " 'one-hot__x0_ATLANTICO',\n",
              " 'one-hot__x0_AUSTRALIA CAPE',\n",
              " 'one-hot__x0_CAROLINA WEST',\n",
              " 'one-hot__x0_CHUSQUEZ',\n",
              " 'one-hot__x0_EL CANEY',\n",
              " 'one-hot__x0_EMBAT',\n",
              " 'one-hot__x0_FAITH ROSE',\n",
              " 'one-hot__x0_FINELY TUNED',\n",
              " 'one-hot__x0_FORTUNATO',\n",
              " 'one-hot__x0_HIGHLAND MARKET',\n",
              " 'one-hot__x0_IZAMAL',\n",
              " 'one-hot__x0_KANE ORE',\n",
              " 'one-hot__x0_LA MAL AMADA',\n",
              " 'one-hot__x0_LADY RAZALMA',\n",
              " 'one-hot__x0_MAITRE YODA',\n",
              " 'one-hot__x0_MEDICEAN BLUE',\n",
              " 'one-hot__x0_MONTERREDONDO',\n",
              " 'one-hot__x0_ORBAYO',\n",
              " 'one-hot__x0_OtherHorse',\n",
              " 'one-hot__x0_PIU BIRCH',\n",
              " 'one-hot__x0_ROBAYERA',\n",
              " 'one-hot__x0_ROBLON',\n",
              " 'one-hot__x0_RUMBERA',\n",
              " 'one-hot__x0_SANCTI PETRI',\n",
              " 'one-hot__x0_SANS ATTENDRE',\n",
              " 'one-hot__x0_SEVERUS',\n",
              " 'one-hot__x0_SHELBY',\n",
              " 'one-hot__x0_SOFUNNY',\n",
              " 'one-hot__x0_SOGALINDA',\n",
              " 'one-hot__x0_STARSHADOW',\n",
              " 'one-hot__x0_THE GAME',\n",
              " 'one-hot__x0_TRES DE TREBOL',\n",
              " 'one-hot__x0_UPSDAWN',\n",
              " 'one-hot__x0_UPSILON',\n",
              " 'one-hot__x0_VIKING CITY',\n",
              " 'one-hot__x0_VITA BARELIERE',\n",
              " 'one-hot__x0_WALKING TO GLORY',\n",
              " \"one-hot__x0_WARRIOR'S REVENGE\",\n",
              " 'one-hot__x0_WHITE WINE',\n",
              " 'one-hot__x0_WINTON',\n",
              " 'one-hot__x0_XILADO',\n",
              " 'one-hot__x1_4 C',\n",
              " 'one-hot__x1_AMAZING TURF',\n",
              " 'one-hot__x1_ANNUA RACING, S....',\n",
              " 'one-hot__x1_ARTEMIS',\n",
              " 'one-hot__x1_ATERPE',\n",
              " 'one-hot__x1_BERNIE',\n",
              " 'one-hot__x1_CANARIAS',\n",
              " 'one-hot__x1_CARAL',\n",
              " 'one-hot__x1_CELSO MENDEZ',\n",
              " 'one-hot__x1_CENTURION',\n",
              " 'one-hot__x1_CIELO DE MADRID',\n",
              " 'one-hot__x1_COCHETEUX',\n",
              " 'one-hot__x1_CUADRA A.F.F S.L',\n",
              " 'one-hot__x1_DELTA',\n",
              " 'one-hot__x1_DONALFA',\n",
              " 'one-hot__x1_DUQUE DE ALBURQU...',\n",
              " 'one-hot__x1_ECURIE DES MOUET...',\n",
              " 'one-hot__x1_EL GENTIO',\n",
              " 'one-hot__x1_ELEVAGE LA REVER...',\n",
              " 'one-hot__x1_ENRIQUE FERNANDE...',\n",
              " 'one-hot__x1_EUROPA',\n",
              " 'one-hot__x1_FORRAJES Y CEREA...',\n",
              " 'one-hot__x1_GISPERT',\n",
              " 'one-hot__x1_IGUELDO',\n",
              " 'one-hot__x1_JACAL',\n",
              " 'one-hot__x1_JAVIER MALDONADO...',\n",
              " 'one-hot__x1_LA TOLEDANA',\n",
              " 'one-hot__x1_LAS AGUILAS',\n",
              " 'one-hot__x1_MARTUL',\n",
              " 'one-hot__x1_MEDITERRANEO',\n",
              " 'one-hot__x1_OtherOwner',\n",
              " 'one-hot__x1_PAOLO NERI',\n",
              " 'one-hot__x1_QUINTO REAL',\n",
              " 'one-hot__x1_REAPERTURA',\n",
              " 'one-hot__x1_SALVADOR MARQUEZ',\n",
              " 'one-hot__x1_SANTA BARBARA',\n",
              " 'one-hot__x1_TEN FE',\n",
              " 'one-hot__x1_TIGRES',\n",
              " 'one-hot__x1_TIZIANO',\n",
              " 'one-hot__x1_TRILOGIA',\n",
              " 'one-hot__x1_VALLADOLID',\n",
              " 'one-hot__x1_YEGUADA AGF',\n",
              " 'one-hot__x1_YEGUADA ARANJUEZ',\n",
              " 'one-hot__x1_YEGUADA ROCIO',\n",
              " 'one-hot__x1_ZEZINHO',\n",
              " 'one-hot__x1_ZUL',\n",
              " 'one-hot__x1_ZURRAQUIN',\n",
              " 'one-hot__x2_A.CARRASCO',\n",
              " 'one-hot__x2_A.IMAZ,B.',\n",
              " 'one-hot__x2_A.NUÑEZ',\n",
              " 'one-hot__x2_A.SOTO',\n",
              " 'one-hot__x2_A.TSERETELI',\n",
              " 'one-hot__x2_B.MORENO',\n",
              " 'one-hot__x2_B.RAMA',\n",
              " 'one-hot__x2_B.VALENTI',\n",
              " 'one-hot__x2_C.FERNANDEZ',\n",
              " 'one-hot__x2_D.DIEZ',\n",
              " 'one-hot__x2_E.ARGUINZONES',\n",
              " 'one-hot__x2_F.RODRIGUEZ',\n",
              " 'one-hot__x2_G.ARIZKORRETA',\n",
              " 'one-hot__x2_J.A.RODRIGUEZ',\n",
              " 'one-hot__x2_J.C.CERQUEIRA',\n",
              " 'one-hot__x2_J.C.ROSELL',\n",
              " 'one-hot__x2_J.L.MAROTO',\n",
              " 'one-hot__x2_J.LOPEZ',\n",
              " 'one-hot__x2_J.M.OSORIO',\n",
              " 'one-hot__x2_M&M RACING',\n",
              " 'one-hot__x2_M.A.MARIN',\n",
              " 'one-hot__x2_M.ALONSO R.',\n",
              " 'one-hot__x2_M.ALVAREZ',\n",
              " 'one-hot__x2_M.J.PEREZ',\n",
              " 'one-hot__x2_P.OLAVE',\n",
              " 'one-hot__x2_R.MARTIN V.',\n",
              " 'one-hot__x2_T.MARTINS',\n",
              " 'one-hot__x3_A.MARTINEZ',\n",
              " 'one-hot__x3_B. FAYOS',\n",
              " 'one-hot__x3_B.ESTUPIÑAN',\n",
              " 'one-hot__x3_C. CADEL',\n",
              " 'one-hot__x3_C.HAZEN',\n",
              " 'one-hot__x3_C.PEREZ',\n",
              " 'one-hot__x3_D. FERREIRA',\n",
              " 'one-hot__x3_D.SIKOROVÁ',\n",
              " 'one-hot__x3_F.MARTINEZ',\n",
              " 'one-hot__x3_G.TROLLEY DE PRE...',\n",
              " 'one-hot__x3_J.GELABERT',\n",
              " 'one-hot__x3_J.L. BORREGO',\n",
              " 'one-hot__x3_J.L. MARTINEZ',\n",
              " 'one-hot__x3_L.FONSECA',\n",
              " 'one-hot__x3_N. DE JULIAN',\n",
              " 'one-hot__x3_N. GARCIA',\n",
              " 'one-hot__x3_N.SACCU',\n",
              " 'one-hot__x3_R.N.VALLE',\n",
              " 'one-hot__x3_SRTA. TENA, C.',\n",
              " 'one-hot__x3_STA. BUESA,C.',\n",
              " 'one-hot__x3_V. JANACEK',\n",
              " 'one-hot__x3_V.ALONSO V.',\n",
              " 'one-hot__x3_Y.RODRIGUEZ',\n",
              " 'index',\n",
              " 'Peso',\n",
              " 'Edad',\n",
              " 'Distancia',\n",
              " 'SentidoHipodromo',\n",
              " 'OtherJockey',\n",
              " 'OtherTrainer',\n",
              " 'month',\n",
              " 'Otoño',\n",
              " 'DiasDesdeCarrera',\n",
              " 'DaysSincePreviousRace',\n",
              " 'DestrezaDistancia',\n",
              " 'Problema_Nulo',\n",
              " 'Problema_1',\n",
              " 'Problema_2',\n",
              " 'Problema_3',\n",
              " 'Problema_4',\n",
              " 'Problema_5',\n",
              " 'Problema_6',\n",
              " 'Problema_7',\n",
              " 'Problema_8',\n",
              " 'MediaUltimasActuaciones',\n",
              " 'CantidadActuaciones']"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "m2gBRtgJQMQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4995c0e-054c-4625-a80b-3df8eb4e0854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB (train): 0.9378941164919611\n",
            "XGB (test): 0.0\n",
            "---\n",
            "Cat (train): 0.015779092702169626\n",
            "Cat (test): 0.0\n",
            "---\n",
            "Lineal (train): 0.9028850274850805\n",
            "Lineal (test): 0.0\n",
            "---\n",
            "Ridge (train): 0.9710771736393949\n",
            "Ridge (test): 0.0\n",
            "---\n",
            "LASSO (train): 1.2226978883151942\n",
            "LASSO (test): 0.0\n",
            "---\n",
            "ElasticNet (train): 1.2226978883151942\n",
            "ElasticNet (test): 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "print('XGB (train): ' + str(mean_absolute_error(y_train, xgb_model.predict(X_train))))\n",
        "predicted_yXGB = xgb_model.predict(X_val)\n",
        "print('XGB (test): ' + str(mean_absolute_error(predicted_yXGB, xgb_model.predict(X_val))))\n",
        "\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('Cat (train): ' + str(mean_absolute_error(y_train, model_cat_tun.predict(X_train))))\n",
        "predicted_yCat = model_cat_tun.predict(X_val)\n",
        "print('Cat (test): ' + str(mean_absolute_error(predicted_yCat, model_cat_tun.predict(X_val))))\n",
        "\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('Lineal (train): ' + str(mean_absolute_error(y_train, linear.predict(X_train))))\n",
        "predicted_y1 = linear.predict(X_val)\n",
        "print('Lineal (test): ' + str(mean_absolute_error(predicted_y1, linear.predict(X_val))))\n",
        "\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('Ridge (train): ' + str(mean_absolute_error(y_train, ridge.predict(X_train))))\n",
        "predicted_y2 = ridge.predict(X_val)\n",
        "print('Ridge (test): ' + str(mean_absolute_error(predicted_y2, ridge.predict(X_val))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('LASSO (train): ' + str(mean_absolute_error(y_train, lasso.predict(X_train))))\n",
        "predicted_y3 = lasso.predict(X_val)\n",
        "\n",
        "print('LASSO (test): ' + str(mean_absolute_error(predicted_y3, lasso.predict(X_val))))\n",
        "\n",
        "print('---')\n",
        "\n",
        "print('ElasticNet (train): ' + str(mean_absolute_error(y_train, elasticnet.predict(X_train))))\n",
        "predicted_y4 = elasticnet.predict(X_val)\n",
        "print('ElasticNet (test): ' + str(mean_absolute_error(predicted_y4, elasticnet.predict(X_val))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainOg = train\n",
        "train['Prediccion'] = 0\n",
        "train['Puesto'] = 0\n",
        "train['Carrera'] = 0\n",
        "y_val = pd.concat([train['Carrera'], train['Puesto'],train['NombreCaballo'], train['Prediccion'], train['FechaAux'], train['Hora']],axis=1)"
      ],
      "metadata": {
        "id": "DJ7TQU6sGHXx"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ordenarTest(horarios, index, x):\n",
        "  carrera = 0\n",
        "  aux = -1\n",
        "  for horario in horarios:\n",
        "    if(aux != horario):\n",
        "      carrera += 1\n",
        "      aux = horario\n",
        "    dict = {}\n",
        "    conta = 0\n",
        "    for i in index:\n",
        "      horarioActual = x.at[i, 'Hora']\n",
        "      if(horario == horarioActual ):\n",
        "          x.at[i, 'Carrera'] = carrera\n",
        "          prediccion = x.at[i, 'Prediccion']\n",
        "          dict[i] = prediccion\n",
        "          conta += 1\n",
        "    puesto = 1\n",
        "    for w in sorted(dict, key=dict.get):\n",
        "      x.at[w, ['Puesto']] = puesto\n",
        "      puesto += 1\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "horarios = train['Hora'].tolist()\n",
        "index = train['index'].tolist()"
      ],
      "metadata": {
        "id": "WYHiHuePkTqA"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "XfEgk3lrfyo3"
      },
      "outputs": [],
      "source": [
        "y_val['Prediccion'] = predicted_yXGB\n",
        "xXGB = y_val\n",
        "ordenarTest(horarios, index, xXGB)\n",
        "xXGB.to_csv('outputXgb.csv' , index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "veJOEkFYnJP0"
      },
      "outputs": [],
      "source": [
        "y_val['Puesto'] = predicted_yCat\n",
        "xCat = y_val\n",
        "ordenarTest(horarios, index, xCat)\n",
        "xCat.to_csv('outputCat.csv' , index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "KJsp0jrkX1RG"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_val['Puesto'] = predicted_y1\n",
        "x1 = y_val\n",
        "ordenarTest(horarios, index, x1)\n",
        "x1.to_csv('output1.csv' , index=False)\n",
        "y_val['Puesto'] = predicted_y2\n",
        "x2 = y_val\n",
        "ordenarTest(horarios, index, x2)\n",
        "x2.to_csv('output2.csv', index=False)\n",
        "y_val['Puesto'] = predicted_y3\n",
        "x3 = y_val\n",
        "ordenarTest(horarios, index, x3)\n",
        "x3.to_csv('output3.csv', index=False)\n",
        "y_val['Puesto'] = predicted_y4\n",
        "x4 = y_val\n",
        "ordenarTest(horarios, index, x4)\n",
        "x4.to_csv('output4.csv' , index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "xXGB.head(70)"
      ],
      "metadata": {
        "id": "3xS2OJ8xMUvk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdfdcd19-e39a-49f1-e9a5-58a2cb4a2a8c"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Carrera  Puesto      NombreCaballo  Prediccion   FechaAux  Hora\n",
              "0         1     2.0         OtherHorse    3.031975 2022-11-20  1145\n",
              "1         1     4.0         OtherHorse    3.311717 2022-11-20  1145\n",
              "2         1     5.0         OtherHorse    3.399439 2022-11-20  1145\n",
              "3         1     1.0         OtherHorse    2.709257 2022-11-20  1145\n",
              "4         1     6.0         OtherHorse    3.984837 2022-11-20  1145\n",
              "5         1     3.0         OtherHorse    3.267935 2022-11-20  1145\n",
              "6         2     4.0         OtherHorse    3.109035 2022-11-20  1220\n",
              "7         2     6.0         OtherHorse    3.291421 2022-11-20  1220\n",
              "8         2     1.0         OtherHorse    2.751346 2022-11-20  1220\n",
              "9         2     2.0         OtherHorse    2.932249 2022-11-20  1220\n",
              "10        2     5.0         OtherHorse    3.267869 2022-11-20  1220\n",
              "11        2     9.0         OtherHorse    3.984837 2022-11-20  1220\n",
              "12        2     7.0         OtherHorse    3.314655 2022-11-20  1220\n",
              "13        2     3.0         OtherHorse    2.932249 2022-11-20  1220\n",
              "14        2     8.0         OtherHorse    3.520402 2022-11-20  1220\n",
              "15        3     9.0       LADY RAZALMA    3.114456 2022-11-20  1255\n",
              "16        3     4.0         FAITH ROSE    2.654448 2022-11-20  1255\n",
              "17        3     3.0  WARRIOR'S REVENGE    2.640582 2022-11-20  1255\n",
              "18        3    13.0             SHELBY    3.318692 2022-11-20  1255\n",
              "19        3     8.0         WHITE WINE    3.083817 2022-11-20  1255\n",
              "20        3    16.0     VITA BARELIERE    3.673043 2022-11-20  1255\n",
              "21        3     7.0             XILADO    3.067216 2022-11-20  1255\n",
              "22        3    15.0             WINTON    3.493744 2022-11-20  1255\n",
              "23        3     5.0          AMERICANO    2.859223 2022-11-20  1255\n",
              "24        3     2.0         OtherHorse    2.109317 2022-11-20  1255\n",
              "25        3    12.0         STARSHADOW    3.231319 2022-11-20  1255\n",
              "26        3    11.0         OtherHorse    3.179641 2022-11-20  1255\n",
              "27        3     1.0       SANCTI PETRI    1.955772 2022-11-20  1255\n",
              "28        3     6.0             ROBLON    3.037591 2022-11-20  1255\n",
              "29        3    14.0   WALKING TO GLORY    3.469662 2022-11-20  1255\n",
              "30        3    10.0          FORTUNATO    3.152256 2022-11-20  1255\n",
              "31        4     4.0     AUSTRALIA CAPE    2.097580 2022-11-20  1330\n",
              "32        4     5.0        MAITRE YODA    2.121594 2022-11-20  1330\n",
              "33        4     3.0           ASTURIAS    1.917846 2022-11-20  1330\n",
              "34        4     7.0            SEVERUS    2.604122 2022-11-20  1330\n",
              "35        4     8.0           ROBAYERA    3.077741 2022-11-20  1330\n",
              "36        4     1.0        VIKING CITY    1.639958 2022-11-20  1330\n",
              "37        4    12.0           KANE ORE    3.921406 2022-11-20  1330\n",
              "38        4     9.0              EMBAT    3.147026 2022-11-20  1330\n",
              "39        4    11.0      MONTERREDONDO    3.383354 2022-11-20  1330\n",
              "40        4     2.0         OtherHorse    1.873583 2022-11-20  1330\n",
              "41        4    10.0         OtherHorse    3.201564 2022-11-20  1330\n",
              "42        4     6.0      MEDICEAN BLUE    2.138180 2022-11-20  1330\n",
              "43        5     6.0           EL CANEY    4.026442 2022-11-20  1405\n",
              "44        5     5.0       FINELY TUNED    3.716801 2022-11-20  1405\n",
              "45        5     1.0           THE GAME    1.273724 2022-11-20  1405\n",
              "46        5     2.0             ORBAYO    2.068894 2022-11-20  1405\n",
              "47        5     4.0             ASTRAL    2.656998 2022-11-20  1405\n",
              "48        5     3.0     TRES DE TREBOL    2.068894 2022-11-20  1405\n",
              "49        6     1.0          SOGALINDA    1.604849 2022-11-20  1440\n",
              "50        6    11.0           CHUSQUEZ    3.292472 2022-11-20  1440\n",
              "51        6     2.0            SOFUNNY    1.774644 2022-11-20  1440\n",
              "52        6     4.0       LA MAL AMADA    2.560979 2022-11-20  1440\n",
              "53        6    12.0          ATLANTICO    3.420408 2022-11-20  1440\n",
              "54        6     9.0      CAROLINA WEST    3.106877 2022-11-20  1440\n",
              "55        6     3.0         OtherHorse    2.340823 2022-11-20  1440\n",
              "56        6    10.0            RUMBERA    3.147026 2022-11-20  1440\n",
              "57        6     5.0      SANS ATTENDRE    2.726191 2022-11-20  1440\n",
              "58        6     6.0    HIGHLAND MARKET    2.825633 2022-11-20  1440\n",
              "59        6    13.0          PIU BIRCH    3.704105 2022-11-20  1440\n",
              "60        6     7.0             ARETHA    2.889145 2022-11-20  1440\n",
              "61        6    14.0             IZAMAL    3.842979 2022-11-20  1440\n",
              "62        6    15.0            UPSDAWN    4.428349 2022-11-20  1440\n",
              "63        6     8.0            UPSILON    3.051049 2022-11-20  1440"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5e24d60-e6b7-496c-8833-3be30c44aa76\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Carrera</th>\n",
              "      <th>Puesto</th>\n",
              "      <th>NombreCaballo</th>\n",
              "      <th>Prediccion</th>\n",
              "      <th>FechaAux</th>\n",
              "      <th>Hora</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.031975</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.311717</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.399439</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.709257</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.984837</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.267935</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.109035</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.291421</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.751346</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.932249</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.267869</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>9.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.984837</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.314655</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.932249</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>8.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.520402</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>9.0</td>\n",
              "      <td>LADY RAZALMA</td>\n",
              "      <td>3.114456</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>FAITH ROSE</td>\n",
              "      <td>2.654448</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>WARRIOR'S REVENGE</td>\n",
              "      <td>2.640582</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>SHELBY</td>\n",
              "      <td>3.318692</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>8.0</td>\n",
              "      <td>WHITE WINE</td>\n",
              "      <td>3.083817</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>16.0</td>\n",
              "      <td>VITA BARELIERE</td>\n",
              "      <td>3.673043</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>XILADO</td>\n",
              "      <td>3.067216</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>15.0</td>\n",
              "      <td>WINTON</td>\n",
              "      <td>3.493744</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>5.0</td>\n",
              "      <td>AMERICANO</td>\n",
              "      <td>2.859223</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.109317</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>12.0</td>\n",
              "      <td>STARSHADOW</td>\n",
              "      <td>3.231319</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>3</td>\n",
              "      <td>11.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.179641</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>SANCTI PETRI</td>\n",
              "      <td>1.955772</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>6.0</td>\n",
              "      <td>ROBLON</td>\n",
              "      <td>3.037591</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>14.0</td>\n",
              "      <td>WALKING TO GLORY</td>\n",
              "      <td>3.469662</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3</td>\n",
              "      <td>10.0</td>\n",
              "      <td>FORTUNATO</td>\n",
              "      <td>3.152256</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>AUSTRALIA CAPE</td>\n",
              "      <td>2.097580</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>MAITRE YODA</td>\n",
              "      <td>2.121594</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>ASTURIAS</td>\n",
              "      <td>1.917846</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>SEVERUS</td>\n",
              "      <td>2.604122</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4</td>\n",
              "      <td>8.0</td>\n",
              "      <td>ROBAYERA</td>\n",
              "      <td>3.077741</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>VIKING CITY</td>\n",
              "      <td>1.639958</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>4</td>\n",
              "      <td>12.0</td>\n",
              "      <td>KANE ORE</td>\n",
              "      <td>3.921406</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4</td>\n",
              "      <td>9.0</td>\n",
              "      <td>EMBAT</td>\n",
              "      <td>3.147026</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>4</td>\n",
              "      <td>11.0</td>\n",
              "      <td>MONTERREDONDO</td>\n",
              "      <td>3.383354</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>1.873583</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>3.201564</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>4</td>\n",
              "      <td>6.0</td>\n",
              "      <td>MEDICEAN BLUE</td>\n",
              "      <td>2.138180</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>EL CANEY</td>\n",
              "      <td>4.026442</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>FINELY TUNED</td>\n",
              "      <td>3.716801</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>THE GAME</td>\n",
              "      <td>1.273724</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ORBAYO</td>\n",
              "      <td>2.068894</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>ASTRAL</td>\n",
              "      <td>2.656998</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>TRES DE TREBOL</td>\n",
              "      <td>2.068894</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>6</td>\n",
              "      <td>1.0</td>\n",
              "      <td>SOGALINDA</td>\n",
              "      <td>1.604849</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>6</td>\n",
              "      <td>11.0</td>\n",
              "      <td>CHUSQUEZ</td>\n",
              "      <td>3.292472</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>6</td>\n",
              "      <td>2.0</td>\n",
              "      <td>SOFUNNY</td>\n",
              "      <td>1.774644</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>LA MAL AMADA</td>\n",
              "      <td>2.560979</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>ATLANTICO</td>\n",
              "      <td>3.420408</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>6</td>\n",
              "      <td>9.0</td>\n",
              "      <td>CAROLINA WEST</td>\n",
              "      <td>3.106877</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>OtherHorse</td>\n",
              "      <td>2.340823</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>6</td>\n",
              "      <td>10.0</td>\n",
              "      <td>RUMBERA</td>\n",
              "      <td>3.147026</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>SANS ATTENDRE</td>\n",
              "      <td>2.726191</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>6</td>\n",
              "      <td>6.0</td>\n",
              "      <td>HIGHLAND MARKET</td>\n",
              "      <td>2.825633</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>6</td>\n",
              "      <td>13.0</td>\n",
              "      <td>PIU BIRCH</td>\n",
              "      <td>3.704105</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>6</td>\n",
              "      <td>7.0</td>\n",
              "      <td>ARETHA</td>\n",
              "      <td>2.889145</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>6</td>\n",
              "      <td>14.0</td>\n",
              "      <td>IZAMAL</td>\n",
              "      <td>3.842979</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>6</td>\n",
              "      <td>15.0</td>\n",
              "      <td>UPSDAWN</td>\n",
              "      <td>4.428349</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>6</td>\n",
              "      <td>8.0</td>\n",
              "      <td>UPSILON</td>\n",
              "      <td>3.051049</td>\n",
              "      <td>2022-11-20</td>\n",
              "      <td>1440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5e24d60-e6b7-496c-8833-3be30c44aa76')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5e24d60-e6b7-496c-8833-3be30c44aa76 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5e24d60-e6b7-496c-8833-3be30c44aa76');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmInW-4vZsjC"
      },
      "source": [
        "- **Considero importante realizar una explicacion en cada subida de .csv sobre en que se diferencia respecto al anterior**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cszh0PPWqego",
        "YtUAW4RbsgBI",
        "7cG1h6OssuhQ",
        "Vm1c9qJnu4-u",
        "pO3y7XmtVGKF",
        "mSBaoWTwVMGs",
        "AT3WFAY1VV6m",
        "wSIoYoCBVZvz",
        "VnYcVwMOVcSq",
        "lmxe8YbuVe9c",
        "IXmLbU8sViiz",
        "P0nFs5yJVk-E",
        "K2C6KoQNVnSN",
        "VVGceRFMVpSQ",
        "YMTgV1HiVsvL",
        "cUiYNaafVuSf",
        "E1VOY56rAXmq",
        "zoNlsctKMr42",
        "e3RikP5HVUSW",
        "KJkv475twgAc",
        "aX7A5lFxKi1Z",
        "3hIBAw3uKm1f",
        "OWEb9BGDKp53",
        "W9hxJma87Flr",
        "Jh3WSnxElRwm",
        "TNcJzf5clRwn",
        "Cgzb2rAalRwn",
        "OLeBdZtqlRwp",
        "4dZmuPgIlRws",
        "_92pJYndlRwt",
        "GxCqIWailRwt",
        "M6UpYVOVlRwt",
        "EcaDp1-9lRwt",
        "YNYSed3olRwt",
        "4D0eOBFflRwt",
        "gD5cZkFclRwu",
        "HouSXFB_lRwu",
        "XL0mqMmxlRwu",
        "AA170c9RlRwu"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNFHXNwr8gs4N5KHox/JxY3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}